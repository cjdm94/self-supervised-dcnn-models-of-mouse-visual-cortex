{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import Normalize, Compose, Resize, CenterCrop, ToTensor\n",
    "from torchvision import utils as torch_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    # $pip install --quiet pytorch-lightning>=1.4\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"--quiet\", \"pytorch-lightning>=1.4\"])\n",
    "    import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and check the image data\n",
    "\n",
    "PATH_TO_DATA = '../../data/selection1866'\n",
    "\n",
    "file_1 = loadmat(os.path.join(PATH_TO_DATA, 'img1.mat'))\n",
    "raw_img_1 = file_1['img']\n",
    "\n",
    "plt.imshow(raw_img_1)\n",
    "plt.title(\"Image 1 (trio) | Dims: {}\".format(raw_img_1.shape))\n",
    "plt.show()\n",
    "\n",
    "img_1_tile_1 = raw_img_1[:, :500]\n",
    "\n",
    "# specify colour map greys\n",
    "plt.imshow(img_1_tile_1)\n",
    "plt.title(\"Leftmost tile of image 1 | Dims: {}\".format(img_1_tile_1.shape))\n",
    "plt.show()\n",
    "\n",
    "# Visualise the transformations we will apply\n",
    "transform = Compose([\n",
    "    Resize(224), # Resize shortest edge to 224\n",
    "    CenterCrop((224, 224)), # Crop to (224, 224)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # Normalize\n",
    "])\n",
    "\n",
    "rgb_img = np.stack([img_1_tile_1] * 3, axis=-1) # Convert to RGB\n",
    "tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "tensor = (tensor + 2) / 4.0  # Scale to [0, 1] -> use max \n",
    "tensor = torch.clamp(tensor, 0.0, 1.0)  # Clamp to ensure [0, 1] range\n",
    "processed_img = transform(tensor) # Resize, crop, normalize\n",
    "\n",
    "# use vmin vmax\n",
    "plt.imshow((processed_img * 0.5 + 0.5).permute(1, 2, 0).clamp(0, 1).numpy())\n",
    "plt.title(\"Processed Image | Dims: {}\".format(processed_img.shape))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# TODO: run tile 1 and 2 through model separately + concat feature reps (no crop, pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess images for SimCLR\n",
    "\n",
    "file_list = sorted(f for f in os.listdir(PATH_TO_DATA) if f.endswith('.mat'))\n",
    "\n",
    "# Prepare images for SimCLR; todo: STL10 is 96x96\n",
    "transform = Compose([\n",
    "    Resize(224), # Resize shortest edge to 224 (cut off the rightmost part of the image)\n",
    "    CenterCrop((224, 224)), # Crop to (224, 224)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # !! Normalize expects input is already in the range [0, 1]\n",
    "])\n",
    "\n",
    "img_tensors,labels = [], []\n",
    "for idx, filename in enumerate(file_list):\n",
    "    data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "    \n",
    "    img = data['img'][:, :500] # Take leftmost part of the image\n",
    "    rgb_img = np.stack([img] * 3, axis=-1) # Convert grayscale to RGB for SimCLR\n",
    "    tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "    \n",
    "    # Min-max scale the tensor to [0, 1]\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Clamp to [0, 1] to ensure no outliers due to numerical precision\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)\n",
    "\n",
    "    transformed_tensor = transform(tensor) # Normalize and resize for SimCLR\n",
    "    img_tensors.append(transformed_tensor)\n",
    "    labels.append(idx)\n",
    "\n",
    "dataset = TensorDataset(torch.stack(img_tensors), torch.tensor(labels))\n",
    "\n",
    "images, labels = dataset.tensors\n",
    "print(\"Labels:\", labels[:10])\n",
    "print(\"Processed dataset shape:\", images.shape) # (N, C, 96, 96)\n",
    "print(f\"Min pixel value (processed): {torch.min(images)}\")\n",
    "print(f\"Max pixel value (processed): {torch.max(images)}\")\n",
    "\n",
    "# Show a sample of processed images\n",
    "img_grid = torch_utils.make_grid(images[:12], nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Processed images: sample')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract feature representations of our images from a pretrained SimCLR model\n",
    "\n",
    "MODEL_CHECKPOINT_PATH = \"../../models/tutorial17/SimCLR.ckpt\"\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of workers:\", NUM_WORKERS)\n",
    "\n",
    "class SimCLR(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        \n",
    "        # Base model f(.)\n",
    "        self.convnet = torchvision.models.resnet18(num_classes=4*hidden_dim)  # Output of last linear layer\n",
    "        \n",
    "        # The MLP for g(.) consists of Linear->ReLU->Linear\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            self.convnet.fc, # Linear(ResNet output, 4*hidden_dim)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "# Function to register hooks and capture outputs from intermediate layers\n",
    "def register_hooks(model, layers):\n",
    "    features = {}\n",
    "\n",
    "    def hook(module, input, output, layer_name):\n",
    "        features[layer_name] = output.detach()\n",
    "\n",
    "    for layer_name in layers:\n",
    "        layer = dict([*model.named_modules()])[layer_name]\n",
    "        layer.register_forward_hook(lambda module, input, output, layer_name=layer_name: hook(module, input, output, layer_name))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Run the pretrained SimCLR model on the experiment images, and capture features from final layer and intermediate layers\n",
    "@torch.no_grad()\n",
    "def extract_simclr_features(model, dataset, layers_to_capture):\n",
    "    # Prepare model and register hooks\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity() # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "    \n",
    "    # Register hooks to capture specific intermediate layers\n",
    "    features = register_hooks(network, layers_to_capture)\n",
    "    \n",
    "    # Encode all images\n",
    "    data_loader = DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
    "    feats, labels, intermediate_features = [], [], {layer: [] for layer in layers_to_capture}\n",
    "    \n",
    "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        \n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "        # Collect intermediate layer outputs\n",
    "        for layer in layers_to_capture:\n",
    "            # Final linear layer outputs a 2d tensor; but intermediate layers don't, so we flatten them ready for PCA \n",
    "            layer_output_flattened = features[layer].view(features[layer].size(0), -1) \n",
    "            intermediate_features[layer].append(layer_output_flattened.cpu())\n",
    "    \n",
    "    # Concatenate results for each layer\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    intermediate_features = {layer: torch.cat(intermediate_features[layer], dim=0) for layer in layers_to_capture}\n",
    "    \n",
    "    return TensorDataset(feats, labels), intermediate_features\n",
    "\n",
    "# Load the pretrained SimCLR model\n",
    "model = SimCLR.load_from_checkpoint(MODEL_CHECKPOINT_PATH)\n",
    "model.eval()\n",
    "\n",
    "# Extract SimCLR representations and intermediate features\n",
    "layers_to_capture = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "final_layer, intermediate_features = extract_simclr_features(model, dataset, layers_to_capture)\n",
    "final_layer_feats, labels = final_layer.tensors\n",
    "layer1_feats = intermediate_features['layer1']\n",
    "layer2_feats = intermediate_features['layer2']\n",
    "layer4_feats = intermediate_features['layer4']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the dimensionality of the different representations\n",
    "\n",
    "representations = [\n",
    "    \"Raw Image Data\",\n",
    "    \"SimCLR Layer 1\",\n",
    "    \"SimCLR Layer 2\",\n",
    "    \"SimCLR Layer 4\",\n",
    "    \"SimCLR Final Layer\"\n",
    "]\n",
    "\n",
    "features_per_image = [\n",
    "    images.view(images.size(0), -1).shape[1], # Raw Image Data (224 * 224 * 3)\n",
    "    layer1_feats.shape[1], # SimCLR Layer 1 (64 * 56 * 56)\n",
    "    layer2_feats.shape[1], # SimCLR Layer 2 (128 * 28 * 28)\n",
    "    layer4_feats.shape[1], # SimCLR Layer 4 (512 * 7 * 7)\n",
    "    final_layer_feats.shape[1] # SimCLR Final Layer (512)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(representations, features_per_image, marker='o', label='Dimensionality')\n",
    "# plt.yscale('log')  # Use a log scale for better visualization\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.xlabel(\"Representation\", fontsize=12)\n",
    "plt.ylabel(\"Features per Image\", fontsize=12)\n",
    "plt.title(\"Dimensionality of Features Across Representations\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA\n",
    "\n",
    "num_components = 500\n",
    "\n",
    "def run_pca(data, num_components=num_components, flatten_data=False):\n",
    "    data = data.view(data.size(0), -1) if flatten_data else data\n",
    "\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca.fit(data)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    return np.cumsum(explained_variance), explained_variance\n",
    "\n",
    "print(\"First 10 labels in SimCLR features:\", labels[:10])\n",
    "\n",
    "cumulative_ev_raw, ev_raw = run_pca(images, flatten_data=True)\n",
    "cumulative_ev_final_layer, ev_final_layer = run_pca(final_layer_feats.numpy())\n",
    "cumulative_ev_layer1, ev_layer1 = run_pca(layer1_feats)\n",
    "cumulative_ev_layer2, ev_layer2 = run_pca(layer2_feats)\n",
    "cumulative_ev_layer4, ev_layer4 = run_pca(layer4_feats)\n",
    "\n",
    "# Plot cumulative explained var vs. # principal components\n",
    "plot_components = range(1, num_components + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(plot_components, cumulative_ev_raw, label=\"Raw Images\", marker='x')\n",
    "plt.plot(plot_components, cumulative_ev_final_layer, label=\"SimCLR Final Layer\", marker='x')\n",
    "plt.plot(plot_components, cumulative_ev_layer1, label=\"SimCLR Layer 1\", marker='x')\n",
    "plt.plot(plot_components, cumulative_ev_layer2, label=\"SimCLR Layer 2\", marker='x')\n",
    "plt.plot(plot_components, cumulative_ev_layer4, label=\"SimCLR Layer 4\", marker='x')\n",
    "\n",
    "plt.xlabel(\"# Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance vs. Number of Components\")\n",
    "plt.legend()\n",
    "plt.text(\n",
    "    0.5, -0.15,  # X and Y coordinates\n",
    "    \"Note: The output of the base encoder's final linear layer is the recommended representation for downstream tasks (Chen et al., 2020).\",\n",
    "    fontsize=10,\n",
    "    color=\"gray\",\n",
    "    ha=\"center\",\n",
    "    va=\"top\",\n",
    "    transform=plt.gca().transAxes\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot explained variance ratio (log) against number of principal components (log)\n",
    "\n",
    "print(f\"Min value: {ev_final_layer.min()}, Max value: {ev_final_layer.max()}\")\n",
    "\n",
    "plot_components = range(1, num_components + 1)  # Do not apply np.log10 here\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(plot_components, ev_raw, label=\"Raw Images\", marker=\"x\", linestyle=\"--\", linewidth=2)\n",
    "plt.plot(plot_components, ev_final_layer, label=\"SimCLR Final Layer\", marker=\"x\", linestyle=\"-\", linewidth=2)\n",
    "\n",
    "# Use log-log scaling for the axes\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"# Principal Components\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"PCA: Explained Variance Ratio vs. Number of Components\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend(loc=\"best\", fontsize=\"small\", frameon=False)\n",
    "plt.text(\n",
    "    0.5, -0.15,  # X and Y coordinates\n",
    "    \"Note: The output of the base encoder's final linear layer is the recommended representation for downstream tasks (Chen et al., 2020).\",\n",
    "    fontsize=10,\n",
    "    color=\"gray\",\n",
    "    ha=\"center\",\n",
    "    va=\"top\",\n",
    "    transform=plt.gca().transAxes\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross validate PCA results for SimCLR final layer\n",
    "\n",
    "# Run PCA on \"training\" subset of the representations\n",
    "simclr_train, simclr_test = torch.utils.data.random_split(final_layer_feats, [1508, 358])\n",
    "pca_simclr = PCA(n_components=num_components)\n",
    "pca_simclr.fit(simclr_train)\n",
    "explained_var_simclr_train = pca_simclr.explained_variance_ratio_\n",
    "\n",
    "# Fit the PCA model on the test set \n",
    "simclr_test_transformed = pca_simclr.transform(simclr_test)\n",
    "explained_var_simclr_test = np.var(simclr_test_transformed, axis=0) / np.sum(np.var(simclr_test, axis=0))\n",
    "\n",
    "# Compute cumulative explained variances\n",
    "cumulative_variance_train = np.cumsum(explained_var_simclr_train)\n",
    "cumulative_variance_test = np.cumsum(explained_var_simclr_test)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance_train) + 1), cumulative_variance_train, label=\"Training Set\", marker='o')\n",
    "plt.plot(range(1, len(cumulative_variance_test) + 1), cumulative_variance_test, label=\"Test Set\", marker='x')\n",
    "plt.xlabel(\"# Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"SimCLR final layer: Cross-Validation of PCA Explained Variance\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross validate PCA results for raw image data\n",
    "\n",
    "# Run PCA on \"training\" subset of the representations\n",
    "flattened_images = images.view(images.size(0), -1)\n",
    "raw_images_train, raw_images_test = torch.utils.data.random_split(flattened_images, [1508, 358])\n",
    "pca_raw_images = PCA(n_components=num_components)\n",
    "pca_raw_images.fit(raw_images_train)\n",
    "explained_var_raw_images_train = pca_raw_images.explained_variance_ratio_\n",
    "\n",
    "# Fit the PCA model on the test set \n",
    "raw_images_test_transformed = pca_raw_images.transform(raw_images_test)\n",
    "explained_var_raw_images_test = np.var(raw_images_test_transformed, axis=0) / np.sum(np.var(raw_images_test, axis=0))\n",
    "\n",
    "# Compute cumulative explained variances\n",
    "cumulative_variance_train = np.cumsum(explained_var_raw_images_train)\n",
    "cumulative_variance_test = np.cumsum(explained_var_raw_images_test)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance_train) + 1), cumulative_variance_train, label=\"Training Set\", marker='o')\n",
    "plt.plot(range(1, len(cumulative_variance_test) + 1), cumulative_variance_test, label=\"Test Set\", marker='x')\n",
    "plt.xlabel(\"# Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Raw Images: Cross-Validation of PCA Explained Variance\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Project an image into PCA space and project it back\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_image_projection(image, pca, num_components, original_shape):\n",
    "    \"\"\"\n",
    "    Project an image into PCA space and reconstruct it back into pixel space.\n",
    "\n",
    "    Args:\n",
    "        image: The original image as a flattened vector (1D array).\n",
    "        pca: The fitted PCA object.\n",
    "        num_components: Number of PCA components to use for reconstruction.\n",
    "        original_shape: Tuple of the original shape (C, H, W).\n",
    "\n",
    "    Returns:\n",
    "        Reconstructed image in the original shape (C, H, W).\n",
    "    \"\"\"\n",
    "    # Step 1: Project the image into PCA space\n",
    "    image_pca = pca.transform(image.reshape(1, -1))  # Transform to PCA space\n",
    "    image_pca[:, num_components:] = 0  # Keep only the top `num_components`\n",
    "\n",
    "    # Step 2: Reconstruct the image back into pixel space\n",
    "    reconstructed_image = pca.inverse_transform(image_pca)\n",
    "\n",
    "    # Step 3: Reshape the reconstructed image to its original shape\n",
    "    return reconstructed_image.reshape(original_shape)\n",
    "\n",
    "\n",
    "# Step 1: Flatten the dataset\n",
    "flattened_images = images.view(images.shape[0], -1)  # Shape: [1866, 150528]\n",
    "\n",
    "# Step 2: Fit PCA on the flattened dataset\n",
    "pca = PCA(n_components=100)  # Keep top 100 components\n",
    "pca.fit(flattened_images.numpy())  # Convert to NumPy for PCA\n",
    "\n",
    "# Step 3: Select an example image for projection\n",
    "example_image = images[0].view(-1).numpy()  # Flatten the first image\n",
    "original_shape = (3, 224, 224)  # Original shape of the image\n",
    "\n",
    "# Step 4: Reconstruct the image using PCA\n",
    "num_components = 10  # Adjust this to see the effect of different numbers of components\n",
    "reconstructed_image = pca_image_projection(example_image, pca, num_components, original_shape)\n",
    "\n",
    "# Normalize the reconstructed image to [0, 1]\n",
    "reconstructed_image = (reconstructed_image - reconstructed_image.min()) / (reconstructed_image.max() - reconstructed_image.min())\n",
    "reconstructed_image = np.clip(reconstructed_image, 0, 1)  # Clamp to ensure valid range\n",
    "\n",
    "# Step 5: Undo normalization for the original image\n",
    "original_image = (images[0] * 0.5 + 0.5).permute(1, 2, 0).numpy()  # Undo normalization\n",
    "\n",
    "# Step 6: Visualize the original and reconstructed images\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image)  # Display the unnormalized original image\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Reconstructed Image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed_image.transpose(1, 2, 0))  # Convert to (H, W, C) for display\n",
    "plt.title(f\"Reconstructed with {num_components} PCs\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
