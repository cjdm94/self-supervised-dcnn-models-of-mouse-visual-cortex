{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import Normalize, Compose, Resize, CenterCrop, ToTensor\n",
    "from torchvision import utils as torch_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    # $pip install --quiet pytorch-lightning>=1.4\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"--quiet\", \"pytorch-lightning>=1.4\"])\n",
    "    import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and check the image data\n",
    "\n",
    "PATH_TO_DATA = '../../data/selection1866'\n",
    "\n",
    "file_1 = loadmat(os.path.join(PATH_TO_DATA, 'img1.mat'))\n",
    "raw_img_1 = file_1['img']\n",
    "\n",
    "plt.imshow(raw_img_1)\n",
    "plt.title(\"Image 1 (trio) | Dims: {}\".format(raw_img_1.shape))\n",
    "plt.show()\n",
    "\n",
    "img_1_tile_1 = raw_img_1[:, :500]\n",
    "\n",
    "plt.imshow(img_1_tile_1)\n",
    "plt.title(\"Leftmost tile of image 1 | Dims: {}\".format(img_1_tile_1.shape))\n",
    "plt.show()\n",
    "\n",
    "# Visualise the transformations we will apply\n",
    "transform = Compose([\n",
    "    Resize(224), # Resize shortest edge to 224\n",
    "    CenterCrop((224, 224)), # Crop to (224, 224)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # Normalize\n",
    "])\n",
    "\n",
    "rgb_img = np.stack([img_1_tile_1] * 3, axis=-1) # Convert to RGB\n",
    "tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "tensor = (tensor + 2) / 4.0  # Scale to [0, 1]\n",
    "tensor = torch.clamp(tensor, 0.0, 1.0)  # Clamp to ensure [0, 1] range\n",
    "processed_img = transform(tensor) # Resize, crop, normalize\n",
    "\n",
    "plt.imshow((processed_img * 0.5 + 0.5).permute(1, 2, 0).clamp(0, 1).numpy())\n",
    "plt.title(\"Processed Image | Dims: {}\".format(processed_img.shape))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess images for SimCLR\n",
    "\n",
    "file_list = sorted(f for f in os.listdir(PATH_TO_DATA) if f.endswith('.mat'))\n",
    "\n",
    "# Prepare images for SimCLR; todo: STL10 is 96x96\n",
    "transform = Compose([\n",
    "    Resize(224), # Resize shortest edge to 224 (cut off the rightmost part of the image)\n",
    "    CenterCrop((224, 224)), # Crop to (224, 224)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # !! Normalize expects input is already in the range [0, 1]\n",
    "])\n",
    "\n",
    "img_tensors,labels = [], []\n",
    "for idx, filename in enumerate(file_list):\n",
    "    data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "    \n",
    "    img = data['img'][:, :500] # Take leftmost part of the image\n",
    "    rgb_img = np.stack([img] * 3, axis=-1) # Convert grayscale to RGB for SimCLR\n",
    "    \n",
    "    tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "    tensor = (tensor + 2) / 4.0  # Scale to [0, 1]\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)  # Clamp to ensure [0, 1] range\n",
    "    transformed_tensor = transform(tensor) # Normalize and resize for SimCLR\n",
    "    \n",
    "    img_tensors.append(transformed_tensor)\n",
    "    labels.append(idx)\n",
    "\n",
    "dataset = TensorDataset(torch.stack(img_tensors), torch.tensor(labels))\n",
    "\n",
    "images, labels = dataset.tensors\n",
    "print(\"Labels:\", labels[:10])\n",
    "print(\"Processed dataset shape:\", images.shape) # (N, C, 96, 96)\n",
    "print(f\"Min pixel value (processed): {torch.min(images)}\")\n",
    "print(f\"Max pixel value (processed): {torch.max(images)}\")\n",
    "\n",
    "# Show a sample of processed images\n",
    "img_grid = torch_utils.make_grid(images[:12], nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Processed images: sample')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract feature representations of our images from a pretrained SimCLR model\n",
    "\n",
    "MODEL_CHECKPOINT_PATH = \"../../models/tutorial17/SimCLR.ckpt\"\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of workers:\", NUM_WORKERS)\n",
    "\n",
    "class SimCLR(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        \n",
    "        # Base model f(.)\n",
    "        self.convnet = torchvision.models.resnet18(num_classes=4*hidden_dim)  # Output of last linear layer\n",
    "        \n",
    "        # The MLP for g(.) consists of Linear->ReLU->Linear\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            self.convnet.fc, # Linear(ResNet output, 4*hidden_dim)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "# Function to register hooks and capture outputs from intermediate layers\n",
    "def register_hooks(model, layers):\n",
    "    features = {}\n",
    "\n",
    "    def hook(module, input, output, layer_name):\n",
    "        features[layer_name] = output.detach()\n",
    "\n",
    "    for layer_name in layers:\n",
    "        layer = dict([*model.named_modules()])[layer_name]\n",
    "        layer.register_forward_hook(lambda module, input, output, layer_name=layer_name: hook(module, input, output, layer_name))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Run the pretrained SimCLR model on the experiment images, and capture features from final layer and intermediate layers\n",
    "@torch.no_grad()\n",
    "def extract_simclr_features(model, dataset, layers_to_capture):\n",
    "    # Prepare model and register hooks\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity() # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "    \n",
    "    # Register hooks to capture specific intermediate layers\n",
    "    features = register_hooks(network, layers_to_capture)\n",
    "    \n",
    "    # Encode all images\n",
    "    data_loader = DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
    "    feats, labels, intermediate_features = [], [], {layer: [] for layer in layers_to_capture}\n",
    "    \n",
    "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        \n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "        # Collect intermediate layer outputs\n",
    "        for layer in layers_to_capture:\n",
    "            # Final linear layer outputs a 2d tensor; but intermediate layers don't, so we flatten them ready for PCA \n",
    "            layer_output_flattened = features[layer].view(features[layer].size(0), -1) \n",
    "            intermediate_features[layer].append(layer_output_flattened.cpu())\n",
    "    \n",
    "    # Concatenate results for each layer\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    intermediate_features = {layer: torch.cat(intermediate_features[layer], dim=0) for layer in layers_to_capture}\n",
    "    \n",
    "    return TensorDataset(feats, labels), intermediate_features\n",
    "\n",
    "# Load the pretrained SimCLR model\n",
    "model = SimCLR.load_from_checkpoint(MODEL_CHECKPOINT_PATH)\n",
    "model.eval()\n",
    "\n",
    "# Extract SimCLR representations and intermediate features\n",
    "layers_to_capture = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "final_layer, intermediate_features = extract_simclr_features(model, dataset, layers_to_capture)\n",
    "final_layer_feats, labels = final_layer.tensors\n",
    "layer1_feats = intermediate_features['layer1']\n",
    "layer2_feats = intermediate_features['layer2']\n",
    "layer4_feats = intermediate_features['layer4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualise SimCLR feature representations in pixel space\n",
    "\n",
    "# Single image tensor (already extracted)\n",
    "img_1 = images[0]\n",
    "\n",
    "# Define the layers you want to capture\n",
    "layers_to_capture = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "\n",
    "# Function to register hooks for specified layers\n",
    "def register_hooks(model, layers):\n",
    "    features = {}\n",
    "\n",
    "    def hook(module, input, output, layer_name):\n",
    "        features[layer_name] = output.detach()\n",
    "\n",
    "    for layer_name in layers:\n",
    "        layer = dict([*model.named_modules()])[layer_name]\n",
    "        layer.register_forward_hook(lambda module, input, output, layer_name=layer_name: hook(module, input, output, layer_name))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Ensure img_1 is a tensor in the correct format\n",
    "# Assuming img_1 is already a PyTorch tensor with shape (C, H, W) or (1, C, H, W)\n",
    "if img_1.ndim == 3:  # If shape is (C, H, W), add batch dimension\n",
    "    img_tensor = img_1.unsqueeze(0)\n",
    "elif img_1.ndim == 4:  # If shape is already (1, C, H, W), no changes are needed\n",
    "    img_tensor = img_1\n",
    "else:\n",
    "    raise ValueError(\"img_1 must have shape (C, H, W) or (1, C, H, W)\")\n",
    "\n",
    "# Ensure the tensor is normalized for SimCLR\n",
    "# Normalize from [0, 1] to [-1, 1] if necessary\n",
    "if img_tensor.max() > 1 or img_tensor.min() < -1:\n",
    "    img_tensor = (img_tensor * 2) - 1  # Scale [0, 1] to [-1, 1]\n",
    "\n",
    "# Load the SimCLR model, remove the projection head, and copy its encoder\n",
    "model.eval()\n",
    "network = deepcopy(model.convnet)\n",
    "network.fc = torch.nn.Identity()  # Remove projection head\n",
    "network.eval()\n",
    "network.to(device)\n",
    "\n",
    "# Register hooks and capture features\n",
    "features = register_hooks(network, layers_to_capture)\n",
    "\n",
    "# Pass the image through the network\n",
    "img_tensor = img_tensor.to(device)\n",
    "feats = network(img_tensor)  # Forward pass\n",
    "\n",
    "# Visualize all feature maps layer-by-layer, row-by-row\n",
    "def plot_feature_maps(features, max_maps_per_layer=16):\n",
    "    \"\"\"\n",
    "    Plot feature maps layer-by-layer.\n",
    "    Each layer's feature maps appear in a separate figure.\n",
    "    \"\"\"\n",
    "    for layer_name, layer_output in features.items():\n",
    "        layer_output = layer_output.squeeze(0).cpu()  # Remove batch dimension\n",
    "        num_maps = layer_output.shape[0]  # Number of feature maps for the current layer\n",
    "        \n",
    "        # Limit the number of maps displayed per layer\n",
    "        maps_to_plot = min(num_maps, max_maps_per_layer)\n",
    "\n",
    "        # Create a new figure for each layer\n",
    "        plt.figure(figsize=(maps_to_plot * 2, 4))\n",
    "        for i in range(maps_to_plot):\n",
    "            plt.subplot(1, maps_to_plot, i + 1)  # 1 row, multiple columns\n",
    "            plt.imshow(layer_output[i].numpy(), cmap='viridis')\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Map {i}\")\n",
    "        plt.suptitle(f\"Feature Maps for {layer_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with the captured features\n",
    "plot_feature_maps(features)\n",
    "\n",
    "# Show the original image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray' if img_tensor.shape[1] == 1 else None)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualise feature representations from SimCLR layers\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "def total_variation_loss(image):\n",
    "    \"\"\"\n",
    "    Total variation loss to smooth the optimized image.\n",
    "    \"\"\"\n",
    "    tv_loss = torch.sum(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :])) + \\\n",
    "              torch.sum(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:]))\n",
    "    return tv_loss\n",
    "\n",
    "\n",
    "def visualize_random_filters(model, layers, num_filters=10, input_size=(96, 96), iterations=200, lr=0.005):\n",
    "    \"\"\"\n",
    "    Visualize patterns that maximize activation for a random selection of filters in specified layers.\n",
    "\n",
    "    Args:\n",
    "        model: The SimCLR model (PyTorch).\n",
    "        layers: List of layer names to visualize (e.g., ['layer1', 'layer2', 'layer3', 'layer4']).\n",
    "        num_filters: Number of random filters to visualize per layer.\n",
    "        input_size: Size of the input image (default: 96x96).\n",
    "        iterations: Number of optimization steps (default: 200).\n",
    "        lr: Learning rate for optimization (default: 0.005).\n",
    "    \"\"\"\n",
    "    # Prepare the model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = torch.nn.Identity()  # Remove the projection head\n",
    "    network.eval().to(device)\n",
    "\n",
    "    # Iterate over each layer\n",
    "    for layer_name in layers:\n",
    "        # Hook to capture the output of the target layer\n",
    "        activations = {}\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            activations[\"layer_output\"] = output\n",
    "\n",
    "        target_layer = dict([*network.named_modules()])[layer_name]\n",
    "        target_layer.register_forward_hook(hook)\n",
    "\n",
    "        # Determine the number of filters in the layer\n",
    "        _ = network(torch.randn(1, 3, *input_size, device=device))  # Forward pass to populate hook\n",
    "        total_filters = activations[\"layer_output\"].shape[1]\n",
    "\n",
    "        # Select random filters\n",
    "        random_filters = random.sample(range(total_filters), num_filters)\n",
    "        print(f\"Visualizing {num_filters} filters from {layer_name} (Randomly chosen: {random_filters})\")\n",
    "\n",
    "        # Create a figure to display all filter visualizations\n",
    "        fig, axes = plt.subplots(1, num_filters, figsize=(15, 3))\n",
    "        fig.suptitle(f\"{layer_name} Filter Visualizations\", fontsize=16)\n",
    "\n",
    "        # Optimize and visualize each filter\n",
    "        for idx, filter_index in enumerate(random_filters):\n",
    "            # Start with a random noise image\n",
    "            input_img = torch.randn(1, 3, *input_size, requires_grad=True, device=device)\n",
    "\n",
    "            # Optimizer to modify the input image\n",
    "            optimizer = torch.optim.Adam([input_img], lr=lr)\n",
    "\n",
    "            for _ in range(iterations):\n",
    "                optimizer.zero_grad()\n",
    "                _ = network(input_img)  # Forward pass\n",
    "                layer_output = activations[\"layer_output\"]\n",
    "\n",
    "                # Maximize activation and add regularization\n",
    "                loss = -layer_output[0, filter_index].mean() + 0.01 * total_variation_loss(input_img)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Normalize the resulting image for visualization\n",
    "            input_img = input_img.detach().cpu().squeeze()\n",
    "            input_img = (input_img - input_img.min()) / (input_img.max() - input_img.min())\n",
    "            \n",
    "            # Display the optimized image\n",
    "            axes[idx].imshow(ToPILImage()(input_img))\n",
    "            axes[idx].axis(\"off\")\n",
    "            axes[idx].set_title(f\"Filter {filter_index}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "layers_to_visualize = [\"layer1\", \"layer2\", \"layer3\", \"layer4\", \"fc\"]  # Add 'fc' for the final layer\n",
    "visualize_random_filters(model, layers=layers_to_visualize, num_filters=10, input_size=(96, 96), iterations=500, lr=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
