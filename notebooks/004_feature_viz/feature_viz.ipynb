{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import Normalize, Compose, Resize, CenterCrop, ToTensor\n",
    "from torchvision import utils as torch_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    # $pip install --quiet pytorch-lightning>=1.4\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"--quiet\", \"pytorch-lightning>=1.4\"])\n",
    "    import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and check the image data\n",
    "\n",
    "PATH_TO_DATA = '../../data/selection1866'\n",
    "\n",
    "file_1 = loadmat(os.path.join(PATH_TO_DATA, 'img1.mat'))\n",
    "raw_img_1 = file_1['img']\n",
    "\n",
    "plt.imshow(raw_img_1)\n",
    "plt.title(\"Image 1 (trio) | Dims: {}\".format(raw_img_1.shape))\n",
    "plt.show()\n",
    "\n",
    "img_1_tile_1 = raw_img_1[:, :500]\n",
    "\n",
    "plt.imshow(img_1_tile_1)\n",
    "plt.title(\"Leftmost tile of image 1 | Dims: {}\".format(img_1_tile_1.shape))\n",
    "plt.show()\n",
    "\n",
    "# Visualise the transformations we will apply\n",
    "transform = Compose([\n",
    "    Resize(96), # Resize shortest edge to 96\n",
    "    CenterCrop((96, 96)), # Crop to (224, 224)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # Normalize\n",
    "])\n",
    "\n",
    "rgb_img = np.stack([img_1_tile_1] * 3, axis=-1) # Convert to RGB\n",
    "tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "tensor = (tensor + 2) / 4.0  # Scale to [0, 1]\n",
    "tensor = torch.clamp(tensor, 0.0, 1.0)  # Clamp to ensure [0, 1] range\n",
    "processed_img = transform(tensor) # Resize, crop, normalize\n",
    "\n",
    "plt.imshow((processed_img * 0.5 + 0.5).permute(1, 2, 0).clamp(0, 1).numpy())\n",
    "plt.title(\"Processed Image | Dims: {}\".format(processed_img.shape))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess images for SimCLR\n",
    "\n",
    "file_list = sorted(f for f in os.listdir(PATH_TO_DATA) if f.endswith('.mat'))\n",
    "\n",
    "# Prepare images for SimCLR; todo: STL10 is 96x96\n",
    "transform = Compose([\n",
    "    Resize(224), # Resize shortest edge to 224 (cut off the rightmost part of the image)\n",
    "    CenterCrop((224, 224)), # Crop to (224, 224)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # !! Normalize expects input is already in the range [0, 1]\n",
    "])\n",
    "\n",
    "img_tensors,labels = [], []\n",
    "for idx, filename in enumerate(file_list):\n",
    "    data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "    \n",
    "    img = data['img'][:, :500] # Take leftmost part of the image\n",
    "    rgb_img = np.stack([img] * 3, axis=-1) # Convert grayscale to RGB for SimCLR\n",
    "    tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "    \n",
    "    # Min-max scale the tensor to [0, 1]\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Clamp to [0, 1] to ensure no outliers due to numerical precision\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)\n",
    "\n",
    "    transformed_tensor = transform(tensor) # Normalize and resize for SimCLR\n",
    "    img_tensors.append(transformed_tensor)\n",
    "    labels.append(idx)\n",
    "\n",
    "image_dataset = TensorDataset(torch.stack(img_tensors), torch.tensor(labels))\n",
    "\n",
    "dataset = TensorDataset(torch.stack(img_tensors), torch.tensor(labels))\n",
    "\n",
    "images, labels = dataset.tensors\n",
    "print(\"Labels:\", labels[:10])\n",
    "print(\"Processed dataset shape:\", images.shape) # (N, C, 96, 96)\n",
    "print(f\"Min pixel value (processed): {torch.min(images)}\")\n",
    "print(f\"Max pixel value (processed): {torch.max(images)}\")\n",
    "\n",
    "# Show a sample of processed images\n",
    "img_grid = torch_utils.make_grid(images[:12], nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Processed images: sample')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract feature representations of our images from a pretrained SimCLR model\n",
    "\n",
    "MODEL_CHECKPOINT_PATH = \"../../models/tutorial17/SimCLR.ckpt\"\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of workers:\", NUM_WORKERS)\n",
    "\n",
    "class SimCLR(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        \n",
    "        # Base model f(.)\n",
    "        self.convnet = torchvision.models.resnet18(num_classes=4*hidden_dim)  # Output of last linear layer\n",
    "        \n",
    "        # The MLP for g(.) consists of Linear->ReLU->Linear\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            self.convnet.fc, # Linear(ResNet output, 4*hidden_dim)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "# Function to register hooks and capture outputs from intermediate layers\n",
    "def register_hooks(model, layers):\n",
    "    features = {}\n",
    "\n",
    "    def hook(module, input, output, layer_name):\n",
    "        features[layer_name] = output.detach()\n",
    "\n",
    "    for layer_name in layers:\n",
    "        layer = dict([*model.named_modules()])[layer_name]\n",
    "        layer.register_forward_hook(lambda module, input, output, layer_name=layer_name: hook(module, input, output, layer_name))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Run the pretrained SimCLR model on the experiment images, and capture features from final layer and intermediate layers\n",
    "@torch.no_grad()\n",
    "def extract_simclr_features(model, dataset, layers_to_capture):\n",
    "    # Prepare model and register hooks\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity() # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "    \n",
    "    # Register hooks to capture specific intermediate layers\n",
    "    features = register_hooks(network, layers_to_capture)\n",
    "    \n",
    "    # Encode all images\n",
    "    data_loader = DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
    "    feats, labels, intermediate_features = [], [], {layer: [] for layer in layers_to_capture}\n",
    "    \n",
    "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        \n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "        \n",
    "        # Collect intermediate layer outputs\n",
    "        for layer in layers_to_capture:\n",
    "            # Final linear layer outputs a 2d tensor; but intermediate layers don't, so we flatten them ready for PCA \n",
    "            layer_output_flattened = features[layer].view(features[layer].size(0), -1) \n",
    "            intermediate_features[layer].append(layer_output_flattened.cpu())\n",
    "    \n",
    "    # Concatenate results for each layer\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    intermediate_features = {layer: torch.cat(intermediate_features[layer], dim=0) for layer in layers_to_capture}\n",
    "    \n",
    "    return TensorDataset(feats, labels), intermediate_features\n",
    "\n",
    "# Load the pretrained SimCLR model\n",
    "model = SimCLR.load_from_checkpoint(MODEL_CHECKPOINT_PATH)\n",
    "model.eval()\n",
    "\n",
    "# Extract SimCLR representations and intermediate features\n",
    "layers_to_capture = ['layer1', 'layer2', 'layer3', 'layer4', 'fc']\n",
    "final_layer, intermediate_features = extract_simclr_features(model, dataset, layers_to_capture)\n",
    "final_layer_feats, labels = final_layer.tensors\n",
    "layer1_feats = intermediate_features['layer1']\n",
    "layer2_feats = intermediate_features['layer2']\n",
    "layer4_feats = intermediate_features['layer4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualise SimCLR feature representations in pixel space\n",
    "\n",
    "# Single image tensor (already extracted)\n",
    "img_1 = images[0]\n",
    "\n",
    "# Define the layers you want to capture\n",
    "layers_to_capture = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "\n",
    "# Function to register hooks for specified layers\n",
    "def register_hooks(model, layers):\n",
    "    features = {}\n",
    "\n",
    "    def hook(module, input, output, layer_name):\n",
    "        features[layer_name] = output.detach()\n",
    "\n",
    "    for layer_name in layers:\n",
    "        layer = dict([*model.named_modules()])[layer_name]\n",
    "        layer.register_forward_hook(lambda module, input, output, layer_name=layer_name: hook(module, input, output, layer_name))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Ensure img_1 is a tensor in the correct format\n",
    "# Assuming img_1 is already a PyTorch tensor with shape (C, H, W) or (1, C, H, W)\n",
    "if img_1.ndim == 3:  # If shape is (C, H, W), add batch dimension\n",
    "    img_tensor = img_1.unsqueeze(0)\n",
    "elif img_1.ndim == 4:  # If shape is already (1, C, H, W), no changes are needed\n",
    "    img_tensor = img_1\n",
    "else:\n",
    "    raise ValueError(\"img_1 must have shape (C, H, W) or (1, C, H, W)\")\n",
    "\n",
    "# Ensure the tensor is normalized for SimCLR\n",
    "# Normalize from [0, 1] to [-1, 1] if necessary\n",
    "if img_tensor.max() > 1 or img_tensor.min() < -1:\n",
    "    img_tensor = (img_tensor * 2) - 1  # Scale [0, 1] to [-1, 1]\n",
    "\n",
    "# Load the SimCLR model, remove the projection head, and copy its encoder\n",
    "model.eval()\n",
    "network = deepcopy(model.convnet)\n",
    "network.fc = torch.nn.Identity()  # Remove projection head\n",
    "network.eval()\n",
    "network.to(device)\n",
    "\n",
    "# Register hooks and capture features\n",
    "features = register_hooks(network, layers_to_capture)\n",
    "\n",
    "# Pass the image through the network\n",
    "img_tensor = img_tensor.to(device)\n",
    "feats = network(img_tensor)  # Forward pass\n",
    "\n",
    "# Visualize all feature maps layer-by-layer, row-by-row\n",
    "def plot_feature_maps(features, max_maps_per_layer=16):\n",
    "    \"\"\"\n",
    "    Plot feature maps layer-by-layer.\n",
    "    Each layer's feature maps appear in a separate figure.\n",
    "    \"\"\"\n",
    "    for layer_name, layer_output in features.items():\n",
    "        layer_output = layer_output.squeeze(0).cpu()  # Remove batch dimension\n",
    "        num_maps = layer_output.shape[0]  # Number of feature maps for the current layer\n",
    "        \n",
    "        # Limit the number of maps displayed per layer\n",
    "        maps_to_plot = min(num_maps, max_maps_per_layer)\n",
    "\n",
    "        # Create a new figure for each layer\n",
    "        plt.figure(figsize=(maps_to_plot * 2, 4))\n",
    "        for i in range(maps_to_plot):\n",
    "            plt.subplot(1, maps_to_plot, i + 1)  # 1 row, multiple columns\n",
    "            plt.imshow(layer_output[i].numpy(), cmap='viridis')\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Map {i}\")\n",
    "        plt.suptitle(f\"Feature Maps for {layer_name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with the captured features\n",
    "plot_feature_maps(features)\n",
    "\n",
    "# Show the original image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy(), cmap='gray' if img_tensor.shape[1] == 1 else None)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualise feature representations from SimCLR layers\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "def total_variation_loss(image):\n",
    "    \"\"\"\n",
    "    Total variation loss to smooth the optimized image.\n",
    "    \"\"\"\n",
    "    tv_loss = torch.sum(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :])) + \\\n",
    "              torch.sum(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:]))\n",
    "    return tv_loss\n",
    "\n",
    "\n",
    "def visualize_random_filters(model, layers, num_filters=10, input_size=(96, 96), iterations=200, lr=0.005):\n",
    "    \"\"\"\n",
    "    Visualize patterns that maximize activation for a random selection of filters in specified layers.\n",
    "\n",
    "    Args:\n",
    "        model: The SimCLR model (PyTorch).\n",
    "        layers: List of layer names to visualize (e.g., ['layer1', 'layer2', 'layer3', 'layer4']).\n",
    "        num_filters: Number of random filters to visualize per layer.\n",
    "        input_size: Size of the input image (default: 96x96).\n",
    "        iterations: Number of optimization steps (default: 200).\n",
    "        lr: Learning rate for optimization (default: 0.005).\n",
    "    \"\"\"\n",
    "    # Prepare the model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = torch.nn.Identity()  # Remove the projection head\n",
    "    network.eval().to(device)\n",
    "\n",
    "    # Iterate over each layer\n",
    "    for layer_name in layers:\n",
    "        # Hook to capture the output of the target layer\n",
    "        activations = {}\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            activations[\"layer_output\"] = output\n",
    "\n",
    "        target_layer = dict([*network.named_modules()])[layer_name]\n",
    "        target_layer.register_forward_hook(hook)\n",
    "\n",
    "        # Determine the number of filters in the layer\n",
    "        _ = network(torch.randn(1, 3, *input_size, device=device))  # Forward pass to populate hook\n",
    "        total_filters = activations[\"layer_output\"].shape[1]\n",
    "\n",
    "        # Select random filters\n",
    "        # random_filters = random.sample(range(total_filters), num_filters)\n",
    "        random_filters = [106, 380, 26, 498, 373, 438, 65, 208, 333, 142]\n",
    "        print(f\"Visualizing {num_filters} filters from {layer_name} (Randomly chosen: {random_filters})\")\n",
    "\n",
    "        # Create a figure to display all filter visualizations\n",
    "        rows = 5 \n",
    "        cols = 10\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 7))\n",
    "        fig.suptitle(f\"{layer_name} Filter Visualizations\", fontsize=16)\n",
    "\n",
    "        # Optimize and visualize each filter\n",
    "        for idx, filter_index in enumerate(random_filters):\n",
    "            # Start with a random noise image\n",
    "            input_img = torch.randn(1, 3, *input_size, requires_grad=True, device=device)\n",
    "\n",
    "            # Optimizer to modify the input image\n",
    "            optimizer = torch.optim.Adam([input_img], lr=lr)\n",
    "\n",
    "            for _ in range(iterations):\n",
    "                optimizer.zero_grad()\n",
    "                _ = network(input_img)  # Forward pass\n",
    "                layer_output = activations[\"layer_output\"]\n",
    "\n",
    "                # Maximize activation and add regularization\n",
    "                loss = -layer_output[0, filter_index].mean() + 0.01 * total_variation_loss(input_img)\n",
    "                loss.backward() # backprop, computes how much each tensor contributed to final lsos\n",
    "                optimizer.step()\n",
    "\n",
    "            # Normalize the resulting image for visualization\n",
    "            input_img = input_img.detach().cpu().squeeze()\n",
    "            input_img = (input_img - input_img.min()) / (input_img.max() - input_img.min())\n",
    "            \n",
    "            # Display the optimized image\n",
    "            # axes[idx].imshow(ToPILImage()(input_img),  cmap=\"gray\")\n",
    "            # axes[idx].axis(\"off\")\n",
    "            # axes[idx].set_title(f\"Filter {filter_index}\")\n",
    "            row, col = divmod(idx, cols)\n",
    "            axes[row, col].imshow(ToPILImage()(input_img), cmap=\"gray\")\n",
    "            axes[row, col].axis(\"off\")\n",
    "            axes[row, col].set_title(f\"Filter {filter_index}\", fontsize=8)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "layers_to_visualize = [\"fc\"]  # Add 'fc' for the final layer\n",
    "visualize_random_filters(model, layers=layers_to_visualize, num_filters=10, input_size=(96, 96), iterations=500, lr=0.01)\n",
    "\n",
    "# Layer3\n",
    "# Filters sorted by activation variability (top 10):\n",
    "# Filter 150: Variance = 28.695124\n",
    "# Filter 116: Variance = 22.668385\n",
    "# Filter 236: Variance = 12.087730\n",
    "# Filter 134: Variance = 12.006798\n",
    "# Filter 244: Variance = 10.250396\n",
    "# Filter 239: Variance = 9.221184\n",
    "# Filter 110: Variance = 7.636278\n",
    "# Filter 145: Variance = 7.632918\n",
    "# Filter 151: Variance = 4.537350\n",
    "# Filter 195: Variance = 4.303740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage, Compose, Resize, CenterCrop, Normalize\n",
    "from copy import deepcopy\n",
    "\n",
    "# Preprocessing transformation as used during training\n",
    "transform_visualization = Compose([\n",
    "    Resize(224),\n",
    "    CenterCrop((224, 224)),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def preprocess_for_display(tensor):\n",
    "    \"\"\"\n",
    "    Undo normalization for display, ensuring values are in [0,1].\n",
    "    \"\"\"\n",
    "    device = tensor.device  \n",
    "    mean = torch.tensor([0.5, 0.5, 0.5], device=device).view(3, 1, 1)\n",
    "    std = torch.tensor([0.5, 0.5, 0.5], device=device).view(3, 1, 1)\n",
    "    tensor = tensor * std + mean\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)\n",
    "    return tensor\n",
    "\n",
    "def total_variation_loss(image):\n",
    "    \"\"\"Total variation loss to smooth the optimized image.\"\"\"\n",
    "    tv_loss = torch.sum(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :])) + \\\n",
    "              torch.sum(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:]))\n",
    "    return tv_loss\n",
    "\n",
    "def visualize_filter_with_real_images(model, layer_name, filter_index, dataset, input_size=(224, 224), iterations=300, lr=0.01):\n",
    "    \"\"\"\n",
    "    For a given filter in a specified layer, this function:\n",
    "      - Computes activations over all images in the dataset.\n",
    "      - Computes and prints filter variability.\n",
    "      - Retrieves the top 50 and bottom 50 activating images.\n",
    "      - Synthesizes a feature via gradient ascent.\n",
    "      - Displays a grid: synthetic feature (first row), top 50 images (next 5 rows, 10 per row), and bottom 50 images (final 5 rows).\n",
    "      \n",
    "    Args:\n",
    "        model: Trained SimCLR model.\n",
    "        layer_name: The target layer (e.g., \"layer3\").\n",
    "        filter_index: Index of the filter (or final dimension) to visualize.\n",
    "        dataset: A PyTorch tensor of shape [N, C, H, W] with preprocessed images.\n",
    "        input_size: Input image size for synthetic feature generation.\n",
    "        iterations: Number of gradient ascent iterations.\n",
    "        lr: Learning rate.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = torch.nn.Identity()  # Remove projection head\n",
    "    network.eval().to(device)\n",
    "    \n",
    "    # Hook to capture activations\n",
    "    activations = {}\n",
    "    def hook(module, input, output):\n",
    "        activations[\"layer_output\"] = output.clone().detach()\n",
    "    \n",
    "    target_layer = dict([*network.named_modules()])[layer_name]\n",
    "    handle = target_layer.register_forward_hook(hook)\n",
    "    \n",
    "    # Move dataset to device and compute activations\n",
    "    dataset = dataset.to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = network(dataset)\n",
    "        layer_output = activations[\"layer_output\"]  # Shape: [N, num_filters, H, W]\n",
    "    \n",
    "    handle.remove()  # Remove hook\n",
    "    \n",
    "    # Compute variability of each filter\n",
    "    def get_filter_variability(layer_output):\n",
    "        N, num_filters, H, W = layer_output.shape\n",
    "        variability_dict = {}\n",
    "        for f in range(num_filters):\n",
    "            pooled = layer_output[:, f].max(dim=1)[0].max(dim=1)[0]  # [N]\n",
    "            variability_dict[f] = pooled.var().item()\n",
    "        return variability_dict\n",
    "    \n",
    "    variability_dict = get_filter_variability(layer_output)\n",
    "    sorted_filters = sorted(variability_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Top 10 filters sorted by activation variability:\")\n",
    "    for f, var in sorted_filters[:10]:\n",
    "        print(f\"Filter {f}: Variance = {var:.6f}\")\n",
    "    \n",
    "    # For the chosen filter, pool activations over spatial dimensions\n",
    "    pooled_activations = layer_output[:, filter_index].max(dim=1)[0].max(dim=1)[0]  # [N]\n",
    "    pooled_activations = (pooled_activations - pooled_activations.min()) / (\n",
    "        pooled_activations.max() - pooled_activations.min() + 1e-8\n",
    "    )\n",
    "    print(f\"Pooled Activations Stats for Filter {filter_index}:\")\n",
    "    print(f\"  Min: {pooled_activations.min().item():.6f}, Max: {pooled_activations.max().item():.6f}, Mean: {pooled_activations.mean().item():.6f}\")\n",
    "    \n",
    "    # Rank images: now taking 50 instead of 10\n",
    "    top_indices = torch.argsort(pooled_activations, descending=True)[:50]\n",
    "    bottom_indices = torch.argsort(pooled_activations)[:50]\n",
    "    \n",
    "    print(\"Top image indices:\", top_indices.cpu().numpy())\n",
    "    print(\"Bottom image indices:\", bottom_indices.cpu().numpy())\n",
    "    \n",
    "    # Retrieve top and bottom images (undo normalization for display)\n",
    "    top_images = torch.stack([preprocess_for_display(dataset[i].to(device)) for i in top_indices]).cpu()\n",
    "    bottom_images = torch.stack([preprocess_for_display(dataset[i].to(device)) for i in bottom_indices]).cpu()\n",
    "    \n",
    "    # Synthetic feature visualization via gradient ascent\n",
    "    input_img = torch.randn(1, 3, *input_size, requires_grad=True, device=device)\n",
    "    optimizer = torch.optim.Adam([input_img], lr=lr)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        _ = network(input_img)\n",
    "        layer_out = activations[\"layer_output\"]\n",
    "        loss = -layer_out[0, filter_index].mean() + 0.01 * total_variation_loss(input_img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    synthetic_img = preprocess_for_display(input_img.detach().cpu().squeeze())\n",
    "    \n",
    "    # Visualization grid: We want 1 row for synthetic, 5 rows for top 50, 5 rows for bottom 50 -> total 11 rows, 10 columns.\n",
    "    fig, axes = plt.subplots(11, 10, figsize=(20, 22))\n",
    "    fig.suptitle(f\"Filter {filter_index} in {layer_name}: Synthetic + Top/Bottom Activating Images\", fontsize=16)\n",
    "    \n",
    "    # First row: display synthetic feature in the first cell, rest blank\n",
    "    axes[0, 0].imshow(ToPILImage()(synthetic_img), cmap=\"gray\")\n",
    "    axes[0, 0].set_title(\"Synthetic Feature\", fontsize=8)\n",
    "    axes[0, 0].axis(\"off\")\n",
    "    for j in range(1, 10):\n",
    "        axes[0, j].axis(\"off\")\n",
    "    \n",
    "    # Next 5 rows: display top 50 images (10 per row)\n",
    "    for i in range(50):\n",
    "        row = 1 + i // 10\n",
    "        col = i % 10\n",
    "        axes[row, col].imshow(ToPILImage()(top_images[i]), cmap=\"gray\")\n",
    "        axes[row, col].set_title(f\"Top {i+1}\", fontsize=8)\n",
    "        axes[row, col].axis(\"off\")\n",
    "    \n",
    "    # Last 5 rows: display bottom 50 images (10 per row)\n",
    "    for i in range(50):\n",
    "        row = 6 + i // 10  # rows 6 to 10 (0-indexed)\n",
    "        col = i % 10\n",
    "        axes[row, col].imshow(ToPILImage()(bottom_images[i]), cmap=\"gray\")\n",
    "        axes[row, col].set_title(f\"Low {i+1}\", fontsize=8)\n",
    "        axes[row, col].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "layer_to_visualize = \"layer3\"\n",
    "filter_to_visualize = 150  # Choose a filter/dimension with high variability\n",
    "visualize_filter_with_real_images(model, layer_to_visualize, filter_to_visualize, dataset=images)\n",
    "\n",
    "\n",
    "# Layer3\n",
    "# Filters sorted by activation variability (top 10):\n",
    "# Filter 150: Variance = 28.695124\n",
    "# Filter 116: Variance = 22.668385\n",
    "# Filter 236: Variance = 12.087730\n",
    "# Filter 134: Variance = 12.006798\n",
    "# Filter 244: Variance = 10.250396\n",
    "# Filter 239: Variance = 9.221184\n",
    "# Filter 110: Variance = 7.636278\n",
    "# Filter 145: Variance = 7.632918\n",
    "# Filter 151: Variance = 4.537350\n",
    "# Filter 195: Variance = 4.303740\n",
    "\n",
    "# Layer4\n",
    "# Filters sorted by activation variability (top 10):\n",
    "# Filter 65: Variance = 3.064611\n",
    "# Filter 228: Variance = 3.017383\n",
    "# Filter 117: Variance = 2.004943\n",
    "# Filter 503: Variance = 1.593511\n",
    "# Filter 146: Variance = 1.398939\n",
    "# Filter 37: Variance = 1.290163\n",
    "# Filter 272: Variance = 1.241190\n",
    "# Filter 6: Variance = 1.238872\n",
    "# Filter 210: Variance = 1.196525\n",
    "# Filter 504: Variance = 1.120707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualise features from final layer\n",
    "\n",
    "# Top final layer dimensions by average activation:\n",
    "# Dimension 106: Average Activation = 1.319144\n",
    "# Dimension 498: Average Activation = 1.194227\n",
    "# Dimension 333: Average Activation = 1.170060\n",
    "# Dimension 380: Average Activation = 1.162012\n",
    "# Dimension 438: Average Activation = 1.102969\n",
    "# Dimension 130: Average Activation = 1.085204\n",
    "# Dimension 26: Average Activation = 1.064653\n",
    "# Dimension 315: Average Activation = 1.018300\n",
    "# Dimension 119: Average Activation = 1.000116\n",
    "# Dimension 84: Average Activation = 0.959936\n",
    "\n",
    "avg_activations = final_layer_feats.mean(dim=0)  # [D] average activation for each dimension\n",
    "\n",
    "# Sort dimensions in descending order by average activation\n",
    "top_k = 10\n",
    "top_indices = torch.argsort(avg_activations, descending=True)[:top_k]\n",
    "\n",
    "# Top final layer dimensions by activation variability:\n",
    "# Dimension 106: Variance = 0.623146\n",
    "# Dimension 380: Variance = 0.511653\n",
    "# Dimension 26: Variance = 0.483480\n",
    "# Dimension 498: Variance = 0.483273\n",
    "# Dimension 373: Variance = 0.468444\n",
    "# Dimension 438: Variance = 0.416003\n",
    "# Dimension 65: Variance = 0.368167\n",
    "# Dimension 208: Variance = 0.364436\n",
    "# Dimension 333: Variance = 0.362497\n",
    "# Dimension 142: Variance = 0.349626\n",
    "\n",
    "# final_layer_feats: tensor of shape [N, D]\n",
    "variability = final_layer_feats.var(dim=0)  # Variance of each dimension across images\n",
    "\n",
    "top_k = 10  # Number of dimensions to display\n",
    "top_variability_indices = torch.argsort(variability, descending=True)[:top_k]\n",
    "\n",
    "print(\"Top final layer dimensions by activation variability:\")\n",
    "for idx in top_variability_indices:\n",
    "    print(f\"Dimension {idx.item()}: Variance = {variability[idx].item():.6f}\")\n",
    "\n",
    "\n",
    "print(\"Top final layer dimensions by average activation:\")\n",
    "for i in top_indices:\n",
    "    print(f\"Dimension {i.item()}: Average Activation = {avg_activations[i].item():.6f}\")\n",
    "\n",
    "\n",
    "def visualize_final_layer_dimension(\n",
    "    model, layer_name, filter_index, dataset,\n",
    "    input_size=(224, 224), iterations=300, lr=0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize how a specific dimension (filter_index) in the final layer\n",
    "    responds to your dataset and produce a synthetic feature.\n",
    "\n",
    "    Args:\n",
    "        model: Trained SimCLR model.\n",
    "        layer_name: The name of the final layer (e.g., 'fc' or 'fc.0').\n",
    "        filter_index: The dimension to visualize (0 <= filter_index < final_layer_dim).\n",
    "        dataset: A PyTorch tensor of shape [N, C, H, W] (preprocessed images).\n",
    "        input_size: Size for the synthetic image.\n",
    "        iterations: Number of gradient ascent iterations.\n",
    "        lr: Learning rate for optimization.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    # Copy the model and remove the projection head\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = torch.nn.Identity()\n",
    "    network.eval().to(device)\n",
    "\n",
    "    # Hook to capture final layer activations\n",
    "    activations = {}\n",
    "    def hook(module, input, output):\n",
    "        # final layer shape: [N, D], no spatial dims\n",
    "        activations[\"final_output\"] = output.clone().detach()\n",
    "\n",
    "    target_layer = dict([*network.named_modules()])[layer_name]\n",
    "    handle = target_layer.register_forward_hook(hook)\n",
    "\n",
    "    # Move dataset to device and run forward pass\n",
    "    dataset = dataset.to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = network(dataset)\n",
    "        final_output = activations[\"final_output\"]  # shape: [N, D]\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    # For the chosen dimension, each image has a single scalar\n",
    "    # So we can directly use final_output[:, filter_index]\n",
    "    dim_activations = final_output[:, filter_index]\n",
    "\n",
    "    # Normalize activations to [0,1] for sorting\n",
    "    dim_activations = (dim_activations - dim_activations.min()) / (\n",
    "        dim_activations.max() - dim_activations.min() + 1e-8\n",
    "    )\n",
    "\n",
    "    # Rank images\n",
    "    top_indices = torch.argsort(dim_activations, descending=True)[:10]\n",
    "    bottom_indices = torch.argsort(dim_activations)[:10]\n",
    "\n",
    "    print(f\"Final Layer Dimension {filter_index} Stats:\")\n",
    "    print(f\"  Min: {dim_activations.min().item():.6f}, Max: {dim_activations.max().item():.6f}, Mean: {dim_activations.mean().item():.6f}\")\n",
    "    print(\"Top image indices:\", top_indices.cpu().numpy())\n",
    "    print(\"Bottom image indices:\", bottom_indices.cpu().numpy())\n",
    "\n",
    "    # Prepare images for display\n",
    "    top_images = torch.stack([preprocess_for_display(dataset[i]) for i in top_indices]).cpu()\n",
    "    bottom_images = torch.stack([preprocess_for_display(dataset[i]) for i in bottom_indices]).cpu()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Synthetic feature visualization\n",
    "    # -------------------------------\n",
    "    # We'll do gradient ascent on the chosen dimension\n",
    "    input_img = torch.randn(1, 3, *input_size, requires_grad=True, device=device)\n",
    "    optimizer = torch.optim.Adam([input_img], lr=lr)\n",
    "\n",
    "    # Re-register hook to capture final layer for the synthetic image\n",
    "    handle = target_layer.register_forward_hook(hook)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        _ = network(input_img)\n",
    "        synthetic_output = activations[\"final_output\"]  # shape: [1, D]\n",
    "        # Maximize dimension filter_index\n",
    "        loss = -synthetic_output[0, filter_index] + 0.01 * total_variation_loss(input_img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    handle.remove()\n",
    "    synthetic_img = preprocess_for_display(input_img.detach().cpu().squeeze())\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(3, 10, figsize=(20, 10))\n",
    "    fig.suptitle(f\"Final Layer Dimension {filter_index}: Synthetic + Top/Bottom Activating Images\", fontsize=14)\n",
    "\n",
    "    # Synthetic feature in row 0, col 0\n",
    "    axes[0, 0].imshow(ToPILImage()(synthetic_img), cmap=\"gray\")\n",
    "    axes[0, 0].set_title(\"Synthetic Feature\", fontsize=8)\n",
    "    axes[0, 0].axis(\"off\")\n",
    "    # Optionally turn off the rest of row 0\n",
    "    for j in range(1, 10):\n",
    "        axes[0, j].axis(\"off\")\n",
    "\n",
    "    # Top 10 images\n",
    "    for i in range(10):\n",
    "        axes[1, i].imshow(ToPILImage()(top_images[i]), cmap=\"gray\")\n",
    "        axes[1, i].set_title(f\"Top {i+1}\", fontsize=8)\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "    # Bottom 10 images\n",
    "    for i in range(10):\n",
    "        axes[2, i].imshow(ToPILImage()(bottom_images[i]), cmap=\"gray\")\n",
    "        axes[2, i].set_title(f\"Low {i+1}\", fontsize=8)\n",
    "        axes[2, i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "#  - layer_name could be \"fc\" (if your final layer is named fc).\n",
    "#  - filter_index is any dimension in [0, D-1].\n",
    "layer_to_visualize = \"fc\"  # or something like \"fc.0\" if it's a submodule\n",
    "filter_to_visualize = 333    # Choose a dimension in the final layer\n",
    "visualize_final_layer_dimension(model, layer_to_visualize, filter_to_visualize, dataset=images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
