{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use a standard-library implementation as a validator\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "diabetes_data = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "\n",
    "# 'target' = diabetes progression 1 year from now\n",
    "diabetes_data['target'] = diabetes.target\n",
    "\n",
    "diabetes_data.head()\n",
    "\n",
    "X = diabetes_data.drop('target', axis=1)\n",
    "y = diabetes_data['target']\n",
    "X.shape, y.shape\n",
    "\n",
    "\n",
    "# we want to train the model to map first 353 samples (80%) to corresponding targets, then predict the target values for the remaining 89 samples (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train, sample_weight=None) # fit finds the best-fitting line that minimizes the sum of squared residuals\n",
    "predicted = model.predict(X_test) # predict the target values for the remaining 89 samples\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, predicted) # compares the model's predictions to the mean of the target values\n",
    "mse = mean_squared_error(y_test, predicted) # measures the average of the squares of the errors\n",
    "evs = explained_variance_score(y_test, predicted) # measures the extent to which a mathematical model accounts for the variation of a given data set\n",
    "mae = mean_absolute_error(y_test, predicted) # measures the average of the absolute errors\n",
    "intercept = model.intercept_ # the point where the line crosses the y-axis\n",
    "\n",
    "# Print all\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"Explained Variance Score: {evs}\")\n",
    "\n",
    "# Plot the predicted target values (x-axis) against the actual values (y-axis)\n",
    "# If the model is a good fit, the points should be close to the line y=x, indicating a strong linear relationship\n",
    "plt.scatter(y_test, predicted, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement LR from scratch\n",
    "### https://www.youtube.com/watch?v=VmbA0pi2cRQ&t=1s&ab_channel=NeuralNine\n",
    "### https://www.youtube.com/watch?v=sDv4f4s2SB8&ab_channel=StatQuestwithJoshStarmer\n",
    "###Â https://www.youtube.com/watch?v=I876Sb5xrws&ab_channel=StarfishMaths\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5]) # input, dependent variable\n",
    "y = np.array([3, 6, 9, 12, 15]) # target, independent variable\n",
    "\n",
    "# With linear regression our goal is to find a line that best fits the data\n",
    "# This means we can use the line to predict the target independent variable for a given value of X\n",
    "# The equation of the line is given by y = wX + b, where w is the weight (slope) and b is the bias (intercept)\n",
    "\n",
    "# Let's just do one iteration first\n",
    "w = 0.1\n",
    "b = 0.0\n",
    "learning_rate = 0.01 # learning rate (come back to this)\n",
    "iterations = 100\n",
    "n = len(X) # number of samples\n",
    "\n",
    "y_predicted = w * X + b\n",
    "print(f'Predicted: {y_predicted}') # [0.1 0.2 0.3 0.4 0.5]\n",
    "print(f'Actual: {y}') # [ 3  6  9 12 15]\n",
    "\n",
    "# Clearly this line predicts the target values poorly - they are way off\n",
    "# So, we need to adjust our line! Which means we need to adjust our weight and bias (slope and intercept)\n",
    "plt.scatter(X, y, color='blue', label='Actual (Target Values)')\n",
    "plt.plot(X, y_predicted, color='red', label='Initial Predicted Line')\n",
    "plt.xlabel('Input (X)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Linear Regression - Initial Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the mean squared error between y_predicted and y\n",
    "# This is our cost function - we want to minimize this\n",
    "# mse = [ (3-0.1)^2 + (6-0.2)^2 + (9-0.3)^2 + (12-0.4)^2 + (15-0.5)^2 ] / 5\n",
    "residuals = y - y_predicted\n",
    "mse = np.mean(residuals ** 2)\n",
    "\n",
    "print(f'Predicted: {y_predicted}') # [0.1 0.2 0.3 0.4 0.5]\n",
    "print(f'Actual: {y}') # [ 3  6  9 12 15]\n",
    "print(f'Residuals: {residuals}') # [ 2.9  5.8  8.7 11.6 14.5]\n",
    "print(f'Squared residuals: {residuals**2}') # [  8.41  33.64  75.69 134.56 210.25]\n",
    "print(f'Mean Squared Error: {mse}') # 92.509\n",
    "\n",
    "# OK, we have our cost function, but how do we minimize it?\n",
    "# We can use gradient descent to iteratively move towards the minimum\n",
    "\n",
    "# (here is a more Pythonic version from ChatGPT - can use it to validate the \"calculus-y\" code below;\n",
    "# but generally in these implementations, I want to avoid numpy magic and code the math myself)\n",
    "w_grad_check = (2 / X.shape[0]) * np.dot(X.T, (y_predicted - y))\n",
    "b_grad_check = (2 / X.shape[0]) * np.sum(y_predicted - y)\n",
    "w_check = w - (learning_rate * w_grad_check)\n",
    "b_check = b - (learning_rate * b_grad_check)\n",
    "\n",
    "print(f'Weight: {w_check}')\n",
    "print(f'Bias: {b_check}')\n",
    "print(f'Weight Gradient: {w_grad_check}')\n",
    "print(f'Bias Gradient: {b_grad_check}')\n",
    "\n",
    "# Let's do gradient descent but for only one parameter (b) for now\n",
    "# If we were to plot our cost function (mean squared residuals) against b (intercept), we would see a quadratic curve\n",
    "# We want to descend \"down\" this curve to the global minima, where the cost is lowest\n",
    "# To do this, we need to, for each intercept value, compute the derivative (slope) of the curve at that point\n",
    "# Then we can subtract this derivative from the current intercept b to get the new intercept\n",
    "# So we want the derivative of the cost function with respect to b\n",
    "# MSE = (1/n) * sum((y - y_predicted)^2) = (1/n) * sum(residuals^2)\n",
    "# Let's first change the calculation of MSE so we can reflect here the calculus\n",
    "MSE = (1/n) * np.sum((y-(w*X+b))**2)\n",
    "print(f'Mean Squared Error - Calculus-y version: {mse}') # 92.509\n",
    "\n",
    "# Compute partial derivatives of MSE (cost function) with respect to b\n",
    "outer_derivative_b = 2*(y-(w*X+b)) # derivative of `(y - wX - b)^2`\n",
    "inner_derivative_b = -1 # derivative of `y - wX - b` (wrt b)\n",
    "gradient_b_components = outer_derivative_b * inner_derivative_b # chain rule\n",
    "gradient_b = np.mean(gradient_b_components)\n",
    "\n",
    "# Now we can do the same with respect to w\n",
    "outer_derivative_w = 2*(y-(w*X+b))\n",
    "inner_derivative_w = -X # # derivative of `y - Xw - b` (wrt w)\n",
    "gradient_w_components = outer_derivative_w * inner_derivative_w # chain rule\n",
    "gradient_w = np.mean(gradient_w_components)\n",
    "\n",
    "# Now we can update our values of w and b\n",
    "w = w - (learning_rate * gradient_w)\n",
    "b = b - (learning_rate * gradient_b)\n",
    "\n",
    "print(f'My Weight: {w}')\n",
    "print(f'My Bias: {b}')\n",
    "print(f'My Weight Gradient: {gradient_w}')\n",
    "print(f'My Bias Gradient: {gradient_b}')\n",
    "\n",
    "# Now we can put this all together in a loop and descend iteratively towards the minimum\n",
    "w = 0.1\n",
    "b = 0.0\n",
    "learning_rate = 0.01\n",
    "iterations = 100\n",
    "n = len(X)\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Calculate the mean squared error between y_predicted and y\n",
    "    MSE = (1/n) * np.sum((y-(w*X+b))**2)\n",
    "\n",
    "    # Compute partial derivatives of MSE (cost function) with respect to b\n",
    "    outer_derivative_b = 2*(y-(w*X+b)) # derivative of `(y - wX - b)^2`\n",
    "    inner_derivative_b = -1 # derivative of `y - wX - b` (wrt b)\n",
    "    gradient_b_components = outer_derivative_b * inner_derivative_b # chain rule\n",
    "    gradient_b = np.mean(gradient_b_components)\n",
    "\n",
    "    # Now we can do the same with respect to w\n",
    "    outer_derivative_w = 2*(y-(w*X+b))\n",
    "    inner_derivative_w = -X # # derivative of `y - Xw - b` (wrt w)\n",
    "    gradient_w_components = outer_derivative_w * inner_derivative_w # chain rule\n",
    "    gradient_w = np.mean(gradient_w_components)\n",
    "\n",
    "    w = w - (learning_rate * gradient_w)\n",
    "    b = b - (learning_rate * gradient_b)\n",
    "\n",
    "# Print the final values of w and b\n",
    "print(f'Final Weight: {w}')\n",
    "print(f'Final Bias: {b}')\n",
    "\n",
    "# Plot the final line\n",
    "y_predicted = w * X + b\n",
    "plt.scatter(X, y, color='blue', label='Actual (Target Values)')\n",
    "plt.plot(X, y_predicted, color='red', label='Final Predicted Line')\n",
    "plt.xlabel('Input (X)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Linear Regression - Final Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
