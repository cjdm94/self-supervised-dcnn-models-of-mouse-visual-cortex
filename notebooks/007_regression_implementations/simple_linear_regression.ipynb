{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use a standard-library implementation as a validator\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "diabetes_data = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "\n",
    "# 'target' = diabetes progression 1 year from now\n",
    "diabetes_data['target'] = diabetes.target\n",
    "\n",
    "diabetes_data.head()\n",
    "\n",
    "X = diabetes_data.drop('target', axis=1)\n",
    "y = diabetes_data['target']\n",
    "X.shape, y.shape\n",
    "\n",
    "\n",
    "# we want to train the model to map first 353 samples (80%) to corresponding targets, then predict the target values for the remaining 89 samples (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train, sample_weight=None) # fit finds the best-fitting line that minimizes the sum of squared residuals\n",
    "predicted = model.predict(X_test) # predict the target values for the remaining 89 samples\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, predicted) # compares the model's predictions to the mean of the target values\n",
    "mse = mean_squared_error(y_test, predicted) # measures the average of the squares of the errors\n",
    "evs = explained_variance_score(y_test, predicted) # measures the extent to which a mathematical model accounts for the variation of a given data set\n",
    "mae = mean_absolute_error(y_test, predicted) # measures the average of the absolute errors\n",
    "intercept = model.intercept_ # the point where the line crosses the y-axis\n",
    "\n",
    "# Print all\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"Explained Variance Score: {evs}\")\n",
    "\n",
    "# Plot the predicted target values (x-axis) against the actual values (y-axis)\n",
    "# If the model is a good fit, the points should be close to the line y=x, indicating a strong linear relationship\n",
    "plt.scatter(y_test, predicted, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement LR from scratch\n",
    "### https://www.youtube.com/watch?v=VmbA0pi2cRQ&t=1s&ab_channel=NeuralNine\n",
    "### https://www.youtube.com/watch?v=sDv4f4s2SB8&ab_channel=StatQuestwithJoshStarmer\n",
    "### https://www.youtube.com/watch?v=I876Sb5xrws&ab_channel=StarfishMaths\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5]) # input, dependent variable\n",
    "y = np.array([3, 6, 9, 12, 15]) # target, independent variable\n",
    "\n",
    "# With linear regression our goal is to find a line that best fits the data\n",
    "# This means we can use the line to predict the target independent variable for a given value of X\n",
    "# The equation of the line is given by y = wX + b, where w is the weight (slope) and b is the bias (intercept)\n",
    "\n",
    "# Let's just do one iteration first\n",
    "w = 0.1\n",
    "b = 0.0\n",
    "learning_rate = 0.01 # learning rate (come back to this)\n",
    "iterations = 100\n",
    "n = len(X) # number of samples\n",
    "\n",
    "y_predicted = w * X + b\n",
    "print(f'Predicted: {y_predicted}') # [0.1 0.2 0.3 0.4 0.5]\n",
    "print(f'Actual: {y}') # [ 3  6  9 12 15]\n",
    "\n",
    "# Clearly this line predicts the target values poorly - they are way off\n",
    "# So, we need to adjust our line! Which means we need to adjust our weight and bias (slope and intercept)\n",
    "plt.scatter(X, y, color='blue', label='Actual (Target Values)')\n",
    "plt.plot(X, y_predicted, color='red', label='Initial Predicted Line')\n",
    "plt.xlabel('Input (X)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Linear Regression - Initial Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the mean squared error between y_predicted and y\n",
    "# This is our cost function - we want to minimize this\n",
    "# mse = [ (3-0.1)^2 + (6-0.2)^2 + (9-0.3)^2 + (12-0.4)^2 + (15-0.5)^2 ] / 5\n",
    "residuals = y - y_predicted\n",
    "mse = np.mean(residuals ** 2)\n",
    "\n",
    "print(f'Predicted: {y_predicted}') # [0.1 0.2 0.3 0.4 0.5]\n",
    "print(f'Actual: {y}') # [ 3  6  9 12 15]\n",
    "print(f'Residuals: {residuals}') # [ 2.9  5.8  8.7 11.6 14.5]\n",
    "print(f'Squared residuals: {residuals**2}') # [  8.41  33.64  75.69 134.56 210.25]\n",
    "print(f'Mean Squared Error: {mse}') # 92.509\n",
    "\n",
    "# OK, we have our cost function, but how do we minimize it?\n",
    "# We can use gradient descent to iteratively move towards the minimum\n",
    "\n",
    "# (here is a more Pythonic version from ChatGPT - can use it to validate the \"calculus-y\" code below;\n",
    "# but generally in these implementations, I want to avoid numpy magic and code the math myself)\n",
    "w_grad_check = (2 / X.shape[0]) * np.dot(X.T, (y_predicted - y))\n",
    "b_grad_check = (2 / X.shape[0]) * np.sum(y_predicted - y)\n",
    "w_check = w - (learning_rate * w_grad_check)\n",
    "b_check = b - (learning_rate * b_grad_check)\n",
    "\n",
    "print(f'Weight: {w_check}')\n",
    "print(f'Bias: {b_check}')\n",
    "print(f'Weight Gradient: {w_grad_check}')\n",
    "print(f'Bias Gradient: {b_grad_check}')\n",
    "\n",
    "# Let's do gradient descent but for only one parameter (b) for now\n",
    "# If we were to plot our cost function (mean squared residuals) against b (intercept), we would see a quadratic curve\n",
    "# We want to descend \"down\" this curve to the global minima, where the cost is lowest\n",
    "# To do this, we need to, for each intercept value, compute the derivative (slope) of the curve at that point\n",
    "# Then we can subtract this derivative from the current intercept b to get the new intercept\n",
    "# So we want the derivative of the cost function with respect to b\n",
    "# MSE = (1/n) * sum((y - y_predicted)^2) = (1/n) * sum(residuals^2)\n",
    "# Let's first change the calculation of MSE so we can reflect here the calculus\n",
    "MSE = (1/n) * np.sum((y-(w*X+b))**2)\n",
    "print(f'Mean Squared Error - Calculus-y version: {mse}') # 92.509\n",
    "\n",
    "# Compute partial derivatives of MSE (cost function) with respect to b\n",
    "outer_derivative_b = 2*(y-(w*X+b)) # derivative of `(y - wX - b)^2`\n",
    "inner_derivative_b = -1 # derivative of `y - wX - b` (wrt b)\n",
    "gradient_b_components = outer_derivative_b * inner_derivative_b # chain rule\n",
    "gradient_b = np.mean(gradient_b_components)\n",
    "\n",
    "# Now we can do the same with respect to w\n",
    "outer_derivative_w = 2*(y-(w*X+b))\n",
    "inner_derivative_w = -X # # derivative of `y - Xw - b` (wrt w)\n",
    "gradient_w_components = outer_derivative_w * inner_derivative_w # chain rule\n",
    "gradient_w = np.mean(gradient_w_components)\n",
    "\n",
    "# Now we can update our values of w and b\n",
    "w = w - (learning_rate * gradient_w)\n",
    "b = b - (learning_rate * gradient_b)\n",
    "\n",
    "print(f'My Weight: {w}')\n",
    "print(f'My Bias: {b}')\n",
    "print(f'My Weight Gradient: {gradient_w}')\n",
    "print(f'My Bias Gradient: {gradient_b}')\n",
    "\n",
    "# Now we can put this all together in a loop and descend iteratively towards the minimum\n",
    "w = 0.1\n",
    "b = 0.0\n",
    "learning_rate = 0.01\n",
    "iterations = 100\n",
    "n = len(X)\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Calculate the mean squared error between y_predicted and y\n",
    "    MSE = (1/n) * np.sum((y-(w*X+b))**2)\n",
    "\n",
    "    # Compute partial derivatives of MSE (cost function) with respect to b\n",
    "    outer_derivative_b = 2*(y-(w*X+b)) # derivative of `(y - wX - b)^2`\n",
    "    inner_derivative_b = -1 # derivative of `y - wX - b` (wrt b)\n",
    "    gradient_b_components = outer_derivative_b * inner_derivative_b # chain rule\n",
    "    gradient_b = np.mean(gradient_b_components)\n",
    "\n",
    "    # Now we can do the same with respect to w\n",
    "    outer_derivative_w = 2*(y-(w*X+b))\n",
    "    inner_derivative_w = -X # # derivative of `y - Xw - b` (wrt w)\n",
    "    gradient_w_components = outer_derivative_w * inner_derivative_w # chain rule\n",
    "    gradient_w = np.mean(gradient_w_components)\n",
    "\n",
    "    w = w - (learning_rate * gradient_w)\n",
    "    b = b - (learning_rate * gradient_b)\n",
    "\n",
    "# Print the final values of w and b\n",
    "print(f'Final Weight: {w}')\n",
    "print(f'Final Bias: {b}')\n",
    "\n",
    "# Plot the final line\n",
    "y_predicted = w * X + b\n",
    "plt.scatter(X, y, color='blue', label='Actual (Target Values)')\n",
    "plt.plot(X, y_predicted, color='red', label='Final Predicted Line')\n",
    "plt.xlabel('Input (X)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Linear Regression - Final Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementing my linear regression as a class; validate against sklearn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class MyLinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, iterations=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "\n",
    "        # initialize with zero weights\n",
    "        self.w = 0.0\n",
    "        self.b = 0.0\n",
    "\n",
    "        self.cost_history = []\n",
    "        self.w_history = []\n",
    "        self.b_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.iterations):\n",
    "            # Calculate the mean squared error between y_predicted and y\n",
    "            MSE = (1/len(X)) * np.sum((y-(self.w*X+self.b))**2)\n",
    "            print(f'Mean Squared Error: {MSE}') if i % 100 == 0 else None\n",
    "\n",
    "            # Compute partial derivatives of MSE (cost function) with respect to b\n",
    "            outer_derivative_b = 2*(y-(self.w*X+self.b)) # derivative of `(y - wX - b)^2`\n",
    "            inner_derivative_b = -1 # derivative of `y - wX - b` (wrt b)\n",
    "            gradient_b_components = outer_derivative_b * inner_derivative_b # chain rule\n",
    "            gradient_b = np.mean(gradient_b_components)\n",
    "\n",
    "            # Now we can do the same with respect to w\n",
    "            outer_derivative_w = 2*(y-(self.w*X+self.b))\n",
    "            inner_derivative_w = -X # # derivative of `y - Xw - b` (wrt w)\n",
    "            gradient_w_components = outer_derivative_w * inner_derivative_w # chain rule\n",
    "            gradient_w = np.mean(gradient_w_components)\n",
    "\n",
    "            self.w = self.w - (self.learning_rate * gradient_w)\n",
    "            self.b = self.b - (self.learning_rate * gradient_b)\n",
    "\n",
    "            # Just for visualising the descent down the cost function curve\n",
    "            self.cost_history.append(MSE)\n",
    "            self.w_history.append(self.w)\n",
    "            self.b_history.append(self.b)\n",
    "\n",
    "            # Just for visualising the movement of the line\n",
    "            if i % 100 == 0:\n",
    "                y_pred = (self.w * X) + self.b\n",
    "                plt.scatter(X, y, color='blue')  # Data points\n",
    "                plt.plot(X, y_pred, color='red')  # Line of best fit\n",
    "                plt.title(f'Iteration {i}')\n",
    "                plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y = (self.w * X) + self.b\n",
    "        return Y\n",
    "    \n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5]) # input, dependent variable\n",
    "y = np.array([3, 6, 9, 12, 15]) # target, independent variable\n",
    "\n",
    "my_model = MyLinearRegression(learning_rate=0.01, iterations=1000)\n",
    "my_model.fit(X, y)\n",
    "my_prediction = my_model.predict(np.array([2.5, 3.5, 4.5, 5.5]))\n",
    "\n",
    "print(f'Prediction: {my_prediction}') # 7.508027010030832\n",
    "print(f'Final weight: {my_model.w}') # 2.9927705260079667\n",
    "print(f'Final bias: {my_model.b}') # 0.026100695010915737\n",
    "\n",
    "validator = LinearRegression(fit_intercept=True)\n",
    "validator.fit(X.reshape(-1, 1), y)\n",
    "validator_prediction = validator.predict(np.array([2.5, 3.5, 4.5, 5.5]).reshape(-1, 1))\n",
    "\n",
    "print(f'Validator Prediction: {validator_prediction}')\n",
    "print(f'Validator final weight: {validator.coef_}')\n",
    "print(f'Validation final bias: {validator.intercept_:.20f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the descent down the cost function curve\n",
    "# Prepare grid for 3D cost function surface\n",
    "ws = np.linspace(-1, 4, 50)  # Range of weights\n",
    "bs = np.linspace(-2, 2, 50)  # Range of biases\n",
    "W, B = np.meshgrid(ws, bs)  # Create a grid of weights and biases\n",
    "Costs = np.zeros_like(W)\n",
    "\n",
    "# Compute cost for each (w, b) pair\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        w = W[i, j]\n",
    "        b = B[i, j]\n",
    "        y_pred = w * X + b\n",
    "        Costs[i, j] = np.mean((y - y_pred) ** 2)\n",
    "\n",
    "# Plot 3D surface\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot_surface(W, B, Costs, cmap='viridis', alpha=0.8)\n",
    "ax.set_xlabel(\"Weight (w)\")\n",
    "ax.set_ylabel(\"Bias (b)\")\n",
    "ax.set_zlabel(\"Cost (MSE)\")\n",
    "ax.set_title(\"Cost Function Surface\")\n",
    "\n",
    "# Overlay gradient descent trajectory\n",
    "ax.plot(my_model.w_history, my_model.b_history, my_model.cost_history, color=\"red\", marker=\"o\", label=\"Gradient Descent Path\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# (sklearn's LinearRegression does not use gradient descent, but rather the normal equation)\n",
    "# but the SGDRegressor does use (stochastic) gradient descent\n",
    "sgd_regressor = SGDRegressor(\n",
    "    max_iter=1000,\n",
    "    tol=1e-3,\n",
    "    random_state=42,\n",
    "    shuffle=False,\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01\n",
    ")\n",
    "sgd_regressor.fit(X.reshape(-1, 1), y)\n",
    "sgd_regressor.predict(np.array([[2.5]]))\n",
    "\n",
    "print(\"Prediction:\", sgd_regressor.predict(np.array([2.5, 3.5, 4.5, 5.5]).reshape(-1, 1)))\n",
    "print(\"Weights:\", sgd_regressor.coef_)\n",
    "print(\"Bias:\", sgd_regressor.intercept_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
