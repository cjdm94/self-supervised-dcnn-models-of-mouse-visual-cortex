{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1. Load data\n",
    "\n",
    "# imresps.npy is of shape (1573, 2, 15363), where 1573 is number of images, 2 repeats each, and 15363 neurons recorded\n",
    "#Â stimids.npy has the image id (matching the image dataset ~selection1866~) for each stimulus number, \n",
    "# so of you want to see what image was presented on imresps[502] you would check stim_ids[502]\n",
    "\n",
    "PATH_TO_DATA = '../../data/neural'\n",
    "\n",
    "imresps = np.load(path.join(PATH_TO_DATA, 'imresps.npy'))\n",
    "stimids = np.load(path.join(PATH_TO_DATA, 'stimids.npy'))\n",
    "\n",
    "print(imresps.shape) # (1573, 2, 15363)\n",
    "print(stimids.shape) # (1573,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_signal_related_variance(resp_a, resp_b, mean_center=True):\n",
    "    \"\"\"\n",
    "    compute the fraction of signal-related variance for each neuron,\n",
    "    as per Stringer et al Nature 2019. Cross-validated by splitting\n",
    "    responses into two halves. Note, this only is \"correct\" if resp_a\n",
    "    and resp_b are *not* averages of many trials.\n",
    "\n",
    "    Args:\n",
    "        resp_a (ndarray): n_stimuli, n_cells\n",
    "        resp_b (ndarray): n_stimuli, n_cells\n",
    "\n",
    "    Returns:\n",
    "        fraction_of_stimulus_variance: 0-1, 0 is non-stimulus-caring, 1 is only-stimulus-caring neurons\n",
    "        stim_to_noise_ratio: ratio of the stim-related variance to all other variance\n",
    "    \"\"\"\n",
    "    if len(resp_a.shape) > 2:\n",
    "        # if the stimulus is multi-dimensional, flatten across all stimuli\n",
    "        resp_a = resp_a.reshape(-1, resp_a.shape[-1])\n",
    "        resp_b = resp_b.reshape(-1, resp_b.shape[-1])\n",
    "    ns, nc = resp_a.shape\n",
    "    if mean_center:\n",
    "        # mean-center the activity of each cell\n",
    "        resp_a = resp_a - resp_a.mean(axis=0)\n",
    "        resp_b = resp_b - resp_b.mean(axis=0)\n",
    "    \n",
    "    # compute the cross-trial stimulus covariance of each cell\n",
    "    # dot-product each cell's (n_stim, ) vector from one half\n",
    "    # with its own (n_stim, ) vector on the other half\n",
    "\n",
    "    covariance = (resp_a * resp_b).sum(axis=0) / ns\n",
    "\n",
    "    # compute the variance of each cell across both halves\n",
    "    resp_a_variance = (resp_a**2).sum(axis=0) / ns\n",
    "    resp_b_variance = (resp_b**2).sum(axis=0) / ns\n",
    "    total_variance = (resp_a_variance + resp_b_variance) / 2\n",
    "\n",
    "    if np.any(total_variance < 1e-12):\n",
    "        print(f\"Warning: Near-zero total variance for neurons: {np.where(total_variance < 1e-12)[0]}\")\n",
    "\n",
    "    # compute the fraction of the total variance that is\n",
    "    # captured in the covariance\n",
    "    fraction_of_stimulus_variance = covariance / total_variance\n",
    "\n",
    "    # if you want, you can compute SNR as well:\n",
    "    stim_to_noise_ratio = fraction_of_stimulus_variance / (\n",
    "        1 - fraction_of_stimulus_variance\n",
    "    )\n",
    "\n",
    "    return fraction_of_stimulus_variance, stim_to_noise_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1. Compute the null distribution of SRV values for all neurons\n",
    "\n",
    "# TODO: double check INDEXING (images, cells)\n",
    "\n",
    "# imresps shape = (1573, 2, 15363)\n",
    "# responses in imresps shape = (2, 15363)\n",
    "num_stimuli = imresps.shape[0] # 1573\n",
    "num_repeats = imresps.shape[1] # 2\n",
    "num_neurons = imresps.shape[2] # 15363\n",
    "n_shuffles = 100\n",
    "\n",
    "null_srv_all_neurons = [] # shape (n_shuffles, num_neurons)\n",
    "\n",
    "for _ in range(n_shuffles):\n",
    "    # Shuffle stimulus indices *twice* to create two independent splits!\n",
    "    shuffled_indices_A = np.random.permutation(num_stimuli)\n",
    "    shuffled_indices_B = np.random.permutation(num_stimuli)\n",
    "\n",
    "    # Now for the splits, we can just use fixed repeat indices, \n",
    "    # because for each split, at index N the responses correspond to different stimuli\n",
    "    # e.g. split_A = [ stim_100_repeat_1, stim_2_repeat_1, stim_19_repeat_1, ... ]\n",
    "    # e.g. split_B = [ stim_543_repeat_2, stim_345_repeat_2, stim_3_repeat_2, ... ]\n",
    "    split_A = imresps[shuffled_indices_A, 0, :]\n",
    "    split_B = imresps[shuffled_indices_B, 1, :]\n",
    "\n",
    "    # Compute SRV for the shuffled data\n",
    "    fraction_of_stimulus_variance, _ = compute_signal_related_variance(split_A, split_B)\n",
    "    null_srv_all_neurons.append(fraction_of_stimulus_variance)\n",
    "\n",
    "null_srv_all_neurons = np.array(null_srv_all_neurons)\n",
    "null_srv_all_neurons.shape # (100, 15363)\n",
    "\n",
    "print(null_srv_all_neurons[0])\n",
    "print(null_srv_all_neurons[33])\n",
    "\n",
    "# e.g. if neuron_index = 0, it will plot the SRV value for neuron 0 across all shuffles\n",
    "neuron_index = 0\n",
    "plt.hist([srv[neuron_index] for srv in null_srv_all_neurons], bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Shuffles\")\n",
    "plt.title(f\"Null Distribution of SRV for Neuron {neuron_index}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2. Compute the real SRV for each neuron\n",
    "\n",
    "# TODO: Question for Ali: why can't we just split like this?\n",
    "# split_A_real = imresps[:, 0, :] # First repeat for each stimulus\n",
    "# split_B_real = imresps[:, 1, :] # Second repeat for each stimulus\n",
    "\n",
    "split_A, split_B = [], []\n",
    "for responses in imresps: # responses shape: (2, n_neurons)\n",
    "    indices = np.random.permutation(2) # Randomly shuffle [0, 1]\n",
    "    split_A.append(responses[indices[0]]) # Assign one repeat to split_A\n",
    "    split_B.append(responses[indices[1]]) # Assign the other to split_B\n",
    "\n",
    "split_A = np.array(split_A)  # Shape: (n_stimuli, n_neurons)\n",
    "split_B = np.array(split_B)  # Shape: (n_stimuli, n_neurons)\n",
    "\n",
    "# Compute SRV for real data\n",
    "real_srv_all_neurons, stim_to_noise_ratio = compute_signal_related_variance(split_A, split_B)\n",
    "\n",
    "print(real_srv_all_neurons)\n",
    "print(stim_to_noise_ratio)\n",
    "\n",
    "print(\"Real SRV shape:\", real_srv_all_neurons.shape) # Should be (15363,)\n",
    "\n",
    "plt.hist(real_srv_all_neurons, bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Shuffles\")\n",
    "plt.title(f\"Null Distribution of SRV for Neuron {neuron_index}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3. Filter neurons whose real SRV is in the top 90th percentile of its null distribution\n",
    "\n",
    "# This gives the 90th-percentile SRV value of the null distribution for each neuron\n",
    "# In other words the threshold for each neuron to be considered reliable\n",
    "# e.g. if neuron 0 has a null distribution of SRVs across 10 shuffles \n",
    "# [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], the threshold would be 0.9\n",
    "top_99th_percentile_null = np.percentile(null_srv_all_neurons, 99, axis=0)\n",
    "print(top_99th_percentile_null) # [0.03651716 0.03126347 0.03325775 ... 0.02738261 0.03546677 0.0333109 ]\n",
    "\n",
    "# Get indices of reliable neurons\n",
    "reliable_neuron_indices = np.where(real_srv_all_neurons >= top_99th_percentile_null)[0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of reliable neurons: {len(reliable_neuron_indices)}\") # 5654\n",
    "print(f\"Indices of reliable neurons: {reliable_neuron_indices}\") # [   14    29    48 ... 15357 15358 15360]\n",
    "\n",
    "plt.hist(real_srv_all_neurons, bins=100, color='red', alpha=0.7)\n",
    "plt.hist(real_srv_all_neurons[reliable_neuron_indices], bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Shuffles\")\n",
    "plt.title(\"All Neurons: SRV all vs. SRV reliable\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(real_srv_all_neurons[reliable_neuron_indices], bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Neurons\")\n",
    "plt.title(\"SRV Distribution for Reliable Neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1. Load and preprocess images\n",
    "\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import Normalize, Compose, Resize, CenterCrop\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import utils as torch_utils\n",
    " \n",
    "PATH_TO_DATA = '../../data/selection1866'\n",
    "\n",
    "file_list = sorted(f for f in os.listdir(PATH_TO_DATA) if f.endswith('.mat'))\n",
    "stim_ids = stimids.astype(int)\n",
    "\n",
    "print(stim_ids)\n",
    "print(stimids)\n",
    "\n",
    "# TODO: run tile 1 and 2 through model separately + concat feature reps (no crop, pad)\n",
    "transform = Compose([\n",
    "    Resize(96), # Resize shortest edge to 96 (cut off the rightmost part of the image)\n",
    "    CenterCrop((96, 96)), # Crop to (96, 96)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # !! Normalize expects input is already in the range [0, 1]\n",
    "])\n",
    "\n",
    "img_tensors, labels = [], []\n",
    "\n",
    "print('List:', file_list)\n",
    "\n",
    "# we have 1866 images here, but the neural response data only uses 1573 of them\n",
    "# because some ~300 images didn't have two repeats, so were disposed\n",
    "# therefore we filter the full set here so that we only use the relevant 1573\n",
    "for stim_id in stim_ids:\n",
    "    filename = 'img' + str(stim_id) + '.mat'\n",
    "    data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "\n",
    "    img = data['img'][:, :500] # Take leftmost part of the image\n",
    "    rgb_img = np.stack([img] * 3, axis=-1) # Convert grayscale to RGB for SimCLR\n",
    "    tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "    \n",
    "    # Min-max scale the tensor to [0, 1]\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Clamp to [0, 1] to ensure no outliers due to numerical precision\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)\n",
    "\n",
    "    transformed_tensor = transform(tensor) # Normalize and resize for SimCLR\n",
    "    img_tensors.append(transformed_tensor)\n",
    "    labels.append(stim_id)\n",
    "\n",
    "image_dataset = TensorDataset(torch.stack(img_tensors), torch.tensor(labels))\n",
    "\n",
    "images, labels = image_dataset.tensors\n",
    "print(\"Processed image labels (stim id):\", labels[:30])\n",
    "print(\"Stim IDs from neural data:\", stim_ids[:30])\n",
    "print(\"Processed dataset shape:\", images.shape) # (N, C, 96, 96)\n",
    "print(f\"Min pixel value (processed): {torch.min(images)}\")\n",
    "print(f\"Max pixel value (processed): {torch.max(images)}\")\n",
    "\n",
    "# Show a sample of processed images\n",
    "img_grid = torch_utils.make_grid(images[:12], nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Processed images: sample')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'img20.mat'\n",
    "data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "img = data['img'][:, :500]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img, cmap='gray')  # Adjust cmap as needed ('viridis', 'jet', etc.)\n",
    "plt.colorbar(label=\"Pixel Intensity\")\n",
    "plt.title(\"Rendered Image\")\n",
    "plt.axis(\"off\")  # Hide axis for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2. Run images through a pretrained SimCLR model and extract features\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict\n",
    "from torch.utils.data import Dataset\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "from collections import defaultdict\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Base ResNet18 backbone (pretrained=False, because we load custom weights later, from the SimCLR checkpoint file)\n",
    "        self.convnet = torchvision.models.resnet18(pretrained=False)\n",
    "        \n",
    "        # This is the projection head, only needed during training. For downstream tasks it is disposed of\n",
    "        # and the final linear layer output is used (Chen et al., 2020) \n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.intermediate_layers_to_capture =[]\n",
    "        self.intermediate_layer_features = {}\n",
    "        self.num_workers = os.cpu_count()\n",
    "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    def load_pretrained(self):\n",
    "        \"\"\"\n",
    "        Load pretrained SimCLR weights\n",
    "        \"\"\"\n",
    "        base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial17/\"\n",
    "        models_dir = \"../../models\"\n",
    "        pretrained_simclr_filename = \"SimCLR.ckpt\"\n",
    "        pretrained_simclr_path = os.path.join(models_dir, pretrained_simclr_filename)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Check whether the pretrained model file already exists locally. If not, try downloading it\n",
    "        file_url = base_url + pretrained_simclr_filename\n",
    "        if not os.path.isfile(pretrained_simclr_path):\n",
    "            print(f\"Downloading pretrained SimCLR model {file_url}...\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(file_url, pretrained_simclr_path)\n",
    "            except HTTPError as e:\n",
    "                print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
    "\n",
    "        print(f\"Already downloaded pretrained model: {file_url}\")\n",
    "\n",
    "        # Load pretrained model\n",
    "        checkpoint = torch.load(pretrained_simclr_path, map_location=self.device)\n",
    "        self.load_state_dict(checkpoint['state_dict'])\n",
    "        self.to(self.device)\n",
    "        self.eval()\n",
    "    \n",
    "    def set_intermediate_layers_to_capture(self, layers):\n",
    "        \"\"\"\n",
    "        Register hooks to capture features from intermediate layers\n",
    "        \"\"\"\n",
    "        # Just check the layers specified are actually in the convnet\n",
    "        top_level_block_layers = [name for name, _ in self.convnet.named_children()]\n",
    "        if not all(layer in top_level_block_layers for layer in layers):\n",
    "            print('You have specified convnet layers that are not top-level blocks - make sure your layer names are valid')\n",
    "        \n",
    "        self.intermediate_layers_to_capture = layers\n",
    "        intermediate_layer_features = {}\n",
    "\n",
    "        def get_hook(layer_name):\n",
    "            def hook(module, input, output):\n",
    "                intermediate_layer_features[layer_name] = output.detach()\n",
    "                # print(f\"Hook stored {layer_name} | Mean activation: {output.mean().item():.6f}\")\n",
    "            return hook\n",
    "        \n",
    "        # def get_hook(layer_name):\n",
    "        #     def hook(module, input, output):\n",
    "        #         self.intermediate_layer_features[layer_name] = output.detach()\n",
    "        #     return hook\n",
    "\n",
    "        for layer_name in layers:\n",
    "            layer = dict([*self.convnet.named_modules()])[layer_name]\n",
    "            layer.register_forward_hook(get_hook(layer_name))\n",
    "\n",
    "        self.intermediate_layer_features = intermediate_layer_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, dataset: Dataset) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run the pretrained SimCLR model on the image data, and capture features from final layer and intermediate layers.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): A PyTorch Dataset containing input images and labels. The image data should have shape (N, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: A dictionary containing:\n",
    "                - Intermediate layer features as tensors.\n",
    "                - Final layer features under 'final_layer'.\n",
    "                - Labels under 'labels'.\n",
    "            Features from a given layer has shape (N, F) where N is num images, F is number of features - flattened version of (C, H, W).\n",
    "        \"\"\"\n",
    "        self.convnet.fc = nn.Identity()  # Removing projection head g(.)\n",
    "        self.eval()\n",
    "        self.to(self.device)\n",
    "        \n",
    "        # Encode all images\n",
    "        data_loader = DataLoader(dataset, batch_size=64, num_workers=self.num_workers, shuffle=False, drop_last=False)\n",
    "        feats, labels, intermediate_features = [], [], {layer: [] for layer in self.intermediate_layers_to_capture}\n",
    "\n",
    "        print(\"â Starting feature extraction...\")\n",
    "        print(f\"Total batches: {len(data_loader)}\")\n",
    "\n",
    "        for batch_idx, (batch_imgs, batch_labels) in enumerate(tqdm(data_loader)):\n",
    "            batch_imgs = batch_imgs.to(self.device)\n",
    "            batch_feats = self.convnet(batch_imgs)\n",
    "            \n",
    "            feats.append(batch_feats.detach().cpu())\n",
    "            labels.append(batch_labels)\n",
    "\n",
    "            # Debugging log to check batch processing\n",
    "            # print(f\"ð¡ Processing batch {batch_idx + 1}/{len(data_loader)} | Batch shape: {batch_imgs.shape}\")\n",
    "            \n",
    "            # Collect intermediate layer outputs\n",
    "            for layer in self.intermediate_layers_to_capture:\n",
    "                # Final linear layer outputs a 2d tensor; but intermediate layers don't, so we flatten them (ready for PCA etc.)\n",
    "                layer_output_flattened = self.intermediate_layer_features[layer].view(self.intermediate_layer_features[layer].size(0), -1) \n",
    "                \n",
    "                # Check if features are stored correctly (first batch vs. later batches)\n",
    "                # if batch_idx == 0:\n",
    "                #     print(f\"ð¹ {layer} (First Batch) | Shape: {layer_output_flattened.shape}\")\n",
    "\n",
    "                intermediate_features[layer].append(layer_output_flattened.cpu())\n",
    "        \n",
    "        for layer in intermediate_features:\n",
    "            print(f\"ð {layer} | Stored {len(intermediate_features[layer])} batches before concatenation\")\n",
    "\n",
    "        # Concatenate results for each layer\n",
    "        feats = torch.cat(feats, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        intermediate_features = {layer: torch.cat(intermediate_features[layer], dim=0) for layer in self.intermediate_layers_to_capture}\n",
    "\n",
    "        # Debugging log after concatenation\n",
    "        print(\"â Feature extraction complete. Final feature shapes:\")\n",
    "        print(f\"Final layer: {feats.shape}\")\n",
    "        for layer, feature in intermediate_features.items():\n",
    "            print(f\"{layer}: {feature.shape}\")  # Check final stored shape\n",
    "\n",
    "        return {**intermediate_features, 'final_layer': feats, 'labels': labels}\n",
    "\n",
    "intermediate_layers = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "\n",
    "sim_clr = SimCLR()\n",
    "sim_clr.load_pretrained()\n",
    "sim_clr.set_intermediate_layers_to_capture(intermediate_layers)\n",
    "feats = sim_clr.extract_features(image_dataset)\n",
    "\n",
    "# Print captured feature shapes\n",
    "# for layer in intermediate_layers:\n",
    "#     if layer in feats:\n",
    "#         print(f\"{layer}: {feats[layer].shape}\")\n",
    "#     else:\n",
    "#         print(f\"{layer}: Not captured\")\n",
    "\n",
    "# for layer in intermediate_layers:\n",
    "#     if layer in feats:\n",
    "#         print(f\"{layer} first feature sample:\", feats[layer][0, :10].tolist())  # Print first 10 values\n",
    "\n",
    "for layer in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    if layer in feats:\n",
    "        variance = np.var(feats[layer].numpy())\n",
    "        print(f\"{layer} variance: {variance:.6f}\")\n",
    "\n",
    "layer1_feats = feats['layer1'] # Shape: torch.Size([1573, 200704]) (n_images, n_features)\n",
    "layer2_feats = feats['layer2']\n",
    "layer3_feats = feats['layer3']\n",
    "layer4_feats = feats['layer4']\n",
    "final_layer_feats = feats['final_layer'] # Shape: torch.Size([1573, 512])\n",
    "\n",
    "print('layer1 shape', layer1_feats.shape)\n",
    "print('final layer shape', final_layer_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute feature-wise variance for each layer\n",
    "feature_variances_layer1 = np.var(layer1_feats.numpy(), axis=0)\n",
    "feature_variances_layer2 = np.var(layer2_feats.numpy(), axis=0)\n",
    "feature_variances_layer3 = np.var(layer3_feats.numpy(), axis=0)\n",
    "feature_variances_layer4 = np.var(layer4_feats.numpy(), axis=0)\n",
    "\n",
    "# Plot histograms of feature variance distributions for each layer\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(feature_variances_layer1, bins=50, alpha=0.5, label=\"Layer 1\")\n",
    "plt.hist(feature_variances_layer2, bins=50, alpha=0.5, label=\"Layer 2\")\n",
    "plt.hist(feature_variances_layer3, bins=50, alpha=0.5, label=\"Layer 3\")\n",
    "plt.hist(feature_variances_layer4, bins=50, alpha=0.5, label=\"Layer 4\")\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel(\"Feature Variance\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Feature Variance Distribution Across Layers\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3. Apply PCA to SimCLR representations\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_components = 500\n",
    "\n",
    "# TODO note: cross-validation, and subsampling of each layer, is done in notebooks/003_experiment_images/experiment_images.ipynb\n",
    "def run_pca(data, num_components=num_components):\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca.fit(data)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    return np.cumsum(explained_variance), explained_variance\n",
    "\n",
    "print(\"First 10 labels in SimCLR features:\", labels[:10])\n",
    "\n",
    "# Our original images are grayscale, but SimCLR expects 3-channel RGB input.\n",
    "# To meet this requirement, we duplicated the grayscale values across all three RGB channels.\n",
    "# However, for PCA, we only need a single channel, so we extract just the first channel (Red).\n",
    "flattened_images = images[:, 0, :, :].view(images.shape[0], -1) # shape: [1573, 50176] (1573 images, 224x224 pixels)\n",
    "\n",
    "cumulative_ev_raw, ev_raw = run_pca(flattened_images)\n",
    "cumulative_ev_layer1, ev_layer1 = run_pca(layer1_feats)\n",
    "cumulative_ev_layer2, ev_layer2 = run_pca(layer2_feats)\n",
    "cumulative_ev_layer3, ev_layer3 = run_pca(layer3_feats)\n",
    "cumulative_ev_layer4, ev_layer4 = run_pca(layer4_feats)\n",
    "cumulative_ev_final_layer, ev_final_layer = run_pca(final_layer_feats, 500)\n",
    "\n",
    "# Plot cumulative explained var vs. # principal components\n",
    "plot_components = range(1, num_components + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(plot_components, cumulative_ev_raw, label=\"Raw Images\", marker='o')\n",
    "plt.plot(plot_components, cumulative_ev_layer1, label=\"SimCLR Layer 1\", marker='x')\n",
    "plt.plot(plot_components, cumulative_ev_layer2, label=\"SimCLR Layer 2\", marker='x')\n",
    "plt.plot(plot_components, cumulative_ev_layer3, label=\"SimCLR Layer 3\", marker='x')\n",
    "plt.plot(plot_components, cumulative_ev_layer4, label=\"SimCLR Layer 4\", marker='x')\n",
    "plt.plot(plot_components, cumulative_ev_final_layer, label=\"SimCLR Layer Final Layer\", marker='x')\n",
    "# plt.plot(plot_components, cumulative_ev_final_layer, label=\"SimCLR Final Layer\", marker='x')\n",
    "plt.xlabel(\"# Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance vs. Number of Components\")\n",
    "plt.legend()\n",
    "plt.text(\n",
    "    0.5, -0.15,  # X and Y coordinates\n",
    "    \"Note: The output of the base encoder's final linear layer is the recommended representation for downstream tasks (Chen et al., 2020).\",\n",
    "    fontsize=10,\n",
    "    color=\"gray\",\n",
    "    ha=\"center\",\n",
    "    va=\"top\",\n",
    "    transform=plt.gca().transAxes\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot explained variance ratio (log) against number of principal components (log)\n",
    "\n",
    "print(f\"Min value: {cumulative_ev_final_layer.min()}, Max value: {cumulative_ev_final_layer.max()}\")\n",
    "\n",
    "plot_components = range(1, num_components + 1) # Do not apply np.log10 here\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(plot_components, ev_raw, label=\"Raw Images\", marker=\"x\", linestyle=\"--\", linewidth=2, color='blue')\n",
    "plt.plot(plot_components, ev_final_layer, label=\"SimCLR Final Layer\", marker=\"x\", linestyle=\"-\", linewidth=2, color='brown')\n",
    "plt.plot(plot_components, ev_layer1, label=\"SimCLR Layer 1\", marker=\"x\", linestyle=\"-\", linewidth=2, color='orange')\n",
    "# plt.plot(plot_components, ev_layer2, label=\"SimCLR Layer 2\", marker=\"x\", linestyle=\"-\", linewidth=2)\n",
    "# plt.plot(plot_components, ev_layer3, label=\"SimCLR Layer 3\", marker=\"x\", linestyle=\"-\", linewidth=2)\n",
    "\n",
    "# Use log-log scaling for the axes\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"# Principal Components\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.title(\"PCA: Explained Variance Ratio vs. Number of Components\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend(loc=\"best\", fontsize=\"small\", frameon=False)\n",
    "plt.text(\n",
    "    0.5, -0.15,  # X and Y coordinates\n",
    "    \"Note: The output of the base encoder's final linear layer is the recommended representation for downstream tasks (Chen et al., 2020).\",\n",
    "    fontsize=10,\n",
    "    color=\"gray\",\n",
    "    ha=\"center\",\n",
    "    va=\"top\",\n",
    "    transform=plt.gca().transAxes\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.4. Choose principal components that explain a % of variance e.g. 90%.\n",
    "\n",
    "# Ensure the number of features is not vastly greater than the number of neurons, to reduce overfitting.\n",
    "# We have 5654 reliable neurons\n",
    "def get_num_components_for_variance(cumulative_variance, target_variance=0.9):\n",
    "    index_of_pc_reaching_target_var = np.argmax(cumulative_variance >= target_variance)\n",
    "    \n",
    "    if(index_of_pc_reaching_target_var == 0):\n",
    "        print(f\"Warning: The computed PCs do not cumulatively explain {target_variance * 100}% variance\")\n",
    "    \n",
    "    return index_of_pc_reaching_target_var + 1\n",
    "\n",
    "# Get the number of PCs for 90% variance\n",
    "num_pcs_raw = get_num_components_for_variance(cumulative_ev_raw, target_variance=0.75)\n",
    "num_pcs_layer1 = get_num_components_for_variance(cumulative_ev_layer1, target_variance=0.75)\n",
    "num_pcs_layer2 = get_num_components_for_variance(cumulative_ev_layer2, target_variance=0.75)\n",
    "num_pcs_layer3 = get_num_components_for_variance(cumulative_ev_layer3, target_variance=0.75)\n",
    "num_pcs_layer4 = get_num_components_for_variance(cumulative_ev_layer4, target_variance=0.75)\n",
    "num_pcs_final_layer = get_num_components_for_variance(cumulative_ev_final_layer, target_variance=0.75)\n",
    "\n",
    "print(f\"Number of PCs explaining 90% variance (Raw Images): {num_pcs_raw}\")\n",
    "print(f\"Number of PCs explaining 90% variance (SimCLR Layer 1): {num_pcs_layer1}\")\n",
    "print(f\"Number of PCs explaining 90% variance (SimCLR Layer 2): {num_pcs_layer2}\")\n",
    "print(f\"Number of PCs explaining 90% variance (SimCLR Layer 3): {num_pcs_layer3}\")\n",
    "print(f\"Number of PCs explaining 90% variance (SimCLR Layer 4): {num_pcs_layer4}\")\n",
    "print(f\"Number of PCs explaining 90% variance (SimCLR Final Layer): {num_pcs_final_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute number of PCs needed for 90% variance\n",
    "def compute_pcs_for_variance(features, variance_threshold=0.90):\n",
    "    pca = PCA()\n",
    "    pca.fit(features)  # Fit PCA to the layer features\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)  # Compute cumulative variance explained\n",
    "    num_pcs = np.searchsorted(cumulative_variance, variance_threshold) + 1  # Find first PC count above threshold\n",
    "    return num_pcs\n",
    "\n",
    "# Compute number of PCs needed for 90% variance for each layer\n",
    "num_pcs_raw_images = compute_pcs_for_variance(flattened_images)\n",
    "num_pcs_layer1 = compute_pcs_for_variance(layer1_feats)\n",
    "num_pcs_layer2 = compute_pcs_for_variance(layer2_feats)\n",
    "num_pcs_layer3 = compute_pcs_for_variance(layer3_feats)\n",
    "num_pcs_layer4 = compute_pcs_for_variance(layer4_feats)\n",
    "num_pcs_final_layer = compute_pcs_for_variance(final_layer_feats)\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of PCs needed to explain 90% variance:\")\n",
    "print(f\"Raw images: {num_pcs_raw_images}\")\n",
    "print(f\"Layer 1: {num_pcs_layer1}\")\n",
    "print(f\"Layer 2: {num_pcs_layer2}\")\n",
    "print(f\"Layer 3: {num_pcs_layer3}\")\n",
    "print(f\"Layer 4: {num_pcs_layer4}\")\n",
    "print(f\"Final layer: {num_pcs_final_layer}\")\n",
    "\n",
    "# Store the number of PCs needed for each layer in a dictionary\n",
    "num_pcs_dict = {\n",
    "    \"Raw Images\": num_pcs_raw_images,\n",
    "    \"Layer 1\": num_pcs_layer1,\n",
    "    \"Layer 2\": num_pcs_layer2,\n",
    "    \"Layer 3\": num_pcs_layer3,\n",
    "    \"Layer 4\": num_pcs_layer4,\n",
    "    \"Final Layer\": num_pcs_final_layer,\n",
    "}\n",
    "\n",
    "print(num_pcs_dict)\n",
    "\n",
    "# Plot as a bar graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(num_pcs_dict.keys(), num_pcs_dict.values(), color=['blue', 'orange', 'green', 'red', 'purple', 'brown'])\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"SimCLR Layer (Including Raw Images)\")\n",
    "plt.ylabel(\"Number of PCs Needed for 90% Variance\")\n",
    "plt.title(\"Number of Principal Components Needed per Layer\")\n",
    "plt.xticks(rotation=30)  # Rotate x-axis labels for readability\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Â 3.4. Project the data into PCA space\n",
    "\n",
    "# Our original images are grayscale, but SimCLR expects 3-channel RGB input.\n",
    "# To meet this requirement, we duplicated the grayscale values across all three RGB channels.\n",
    "# However, for PCA, we only need a single channel, so we extract just the first channel (Red).\n",
    "images.shape # shape: [1573, 3, 224, 224]\n",
    "flattened_images = images[:, 0, :, :].view(images.shape[0], -1) # shape: [1573, 50176] (1573 images, 224x224 pixels)\n",
    "\n",
    "# Use same number of principal components for each input\n",
    "num_pcs = min(num_pcs_raw, num_pcs_layer2, num_pcs_final_layer)\n",
    "\n",
    "## 3.4.1. For raw images\n",
    "pca_raw = PCA(n_components=num_pcs)\n",
    "pca_raw.fit(flattened_images)\n",
    "raw_image_pcs = pca_raw.transform(flattened_images) # Shape: (1573, N) -> 1573 images, N PCs\n",
    "\n",
    "pca_layer1 = PCA(n_components=num_pcs)\n",
    "pca_layer1.fit(layer1_feats)\n",
    "layer1_pcs = pca_layer1.transform(layer1_feats)\n",
    "\n",
    "## 3.4.2. For layer-2 activations\n",
    "pca_layer2 = PCA(n_components=num_pcs)\n",
    "pca_layer2.fit(layer2_feats)\n",
    "layer2_pcs = pca_layer2.transform(layer2_feats) # Shape: (1573, 1163) -> 1573 images, 1163 PCs\n",
    "\n",
    "pca_layer3 = PCA(n_components=num_pcs)\n",
    "pca_layer3.fit(layer3_feats)\n",
    "layer3_pcs = pca_layer3.transform(layer3_feats)\n",
    "\n",
    "pca_layer4 = PCA(n_components=num_pcs)\n",
    "pca_layer4.fit(layer4_feats)\n",
    "layer4_pcs = pca_layer4.transform(layer4_feats)\n",
    "layer4_pcs = pca_layer4.transform(layer4_feats)\n",
    "\n",
    "## 3.4.3. For final layer activations\n",
    "pca_final_layer = PCA(n_components=num_pcs)\n",
    "pca_final_layer.fit(final_layer_feats)\n",
    "final_layer_pcs = pca_final_layer.transform(final_layer_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1. Regression per neuron\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Gather the neural responses for the reliable neurons\n",
    "# we take the average across repeats for each neuron\n",
    "neural_responses = imresps[:, :, reliable_neuron_indices] # Shape: (1573, 2, 5654)\n",
    "neural_responses_mean = neural_responses.mean(axis=1) # Shape: (1573, 5654) -> 1573 images, 5654 neurons\n",
    "\n",
    "# Select a specific reliable neuron by index (because we will have a regression model per neuron)\n",
    "reliable_neuron_index = reliable_neuron_indices[0]\n",
    "mean_response_single_neuron = neural_responses_mean[:, reliable_neuron_index]  # Shape: (1573,)\n",
    "\n",
    "## 4.1.1. For raw images\n",
    "\n",
    "print('Raw image PCs shape', raw_image_pcs.shape) # (1573, 526)\n",
    "print('Mean response single neuron shape', mean_response_single_neuron.shape) # (1573,)\n",
    "\n",
    "# Fit a single linear regression model (handles all neurons at once)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    raw_image_pcs, neural_responses_mean, test_size=0.2, random_state=42\n",
    ")\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, Y_train)\n",
    "Y_pred = reg.predict(X_test)\n",
    "raw_image_r2_scores = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "\n",
    "for neuron_index, score in zip(reliable_neuron_indices[:10], raw_image_r2_scores):\n",
    "    print(f\"Raw Images: R^2 Score for Reliable Neuron {neuron_index}: {score}\")\n",
    "\n",
    "## 4.1.2. For SimCLR layer 2 features\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    layer2_pcs, neural_responses_mean, test_size=0.2, random_state=42\n",
    ")\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, Y_train)\n",
    "Y_pred = reg.predict(X_test)\n",
    "layer_2_r2_scores = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "\n",
    "for neuron_index, score in zip(reliable_neuron_indices[:10], layer_2_r2_scores):\n",
    "    print(f\"Layer 2: R^2 Score for Reliable Neuron {neuron_index}: {score}\")\n",
    "\n",
    "## 4.1.3. For SimCLR final layer features\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    final_layer_pcs, neural_responses_mean, test_size=0.2, random_state=42\n",
    ")\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, Y_train)\n",
    "Y_pred = reg.predict(X_test)\n",
    "final_layer_r2_scores = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "\n",
    "for neuron_index, score in zip(reliable_neuron_indices[:10], final_layer_r2_scores):\n",
    "    print(f\"Final Layer: R^2 Score for Reliable Neuron {neuron_index}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot overlaid histogram of distribution of regression scores across reliable neurons, for different layers\n",
    "\n",
    "# Fit a single linear regression model (handles all neurons at once)\n",
    "def linear_regression(image_representations, neural_responses):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        image_representations, neural_responses, test_size=0.2, random_state=42\n",
    "    )\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train, Y_train)\n",
    "    Y_pred = reg.predict(X_test)\n",
    "    r2_scores = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "    return r2_scores\n",
    "\n",
    "raw_image_pcs_scores = linear_regression(raw_image_pcs, neural_responses_mean)\n",
    "layer1_pcs_scores = linear_regression(layer1_pcs, neural_responses_mean)\n",
    "layer2_pcs_scores = linear_regression(layer2_pcs, neural_responses_mean)\n",
    "layer3_pcs_scores = linear_regression(layer3_pcs, neural_responses_mean)\n",
    "layer4_pcs_scores = linear_regression(layer4_pcs, neural_responses_mean)\n",
    "final_layer_pcs_scores = linear_regression(final_layer_pcs, neural_responses_mean)\n",
    "\n",
    "plt.hist(raw_image_pcs_scores, bins=50, alpha=0.5, label='Raw Images', color='red')\n",
    "# plt.hist(layer1_pcs_scores, bins=50, alpha=0.5, label='SimCLR Layer 1', color='pink')\n",
    "# plt.hist(layer2_pcs_scores, bins=50, alpha=0.5, label='SimCLR Layer 2', color='yellow')\n",
    "# plt.hist(layer3_pcs_scores, bins=50, alpha=0.5, label='SimCLR Layer 3', color='orange')\n",
    "# plt.hist(layer4_pcs_scores, bins=50, alpha=0.5, label='SimCLR Layer 4', color='blue')\n",
    "# plt.hist(final_layer_pcs_scores, bins=50, alpha=0.5, label='SimCLR Final Layer', color='green')\n",
    "\n",
    "plt.xlabel(\"RÂ² Score\")\n",
    "plt.ylabel(\"Number of Neurons\")\n",
    "plt.title(\"Distribution of Regression Scores Across Reliable Neurons\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1. Ridge regression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# TODO: tune reg param for Ridge\n",
    "# TODO: max val score by iterating alpha and test on test set\n",
    "\n",
    "## 5.1.1. For raw images\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_image_pcs, neural_responses_mean, test_size=0.2, random_state=42)\n",
    "reg = Ridge(alpha=1.0)\n",
    "reg.fit(X_train, y_train)\n",
    "Y_pred = reg.predict(X_test)\n",
    "raw_images_r2_scores = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "\n",
    "for neuron_index, score in zip(reliable_neuron_indices[:10], final_layer_r2_scores):\n",
    "    print(f\"Raw Images (Ridge): R^2 Score for Reliable Neuron {neuron_index}: {score}\")\n",
    "\n",
    "## 5.1.2. For SimCLR layer 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(layer2_pcs, neural_responses_mean, test_size=0.2, random_state=42)\n",
    "reg = Ridge(alpha=1.0)\n",
    "reg.fit(X_train, y_train)\n",
    "Y_pred = reg.predict(X_test)\n",
    "layer_2_r2_scores = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "\n",
    "for neuron_index, score in zip(reliable_neuron_indices[:10], final_layer_r2_scores):\n",
    "    print(f\"Layer 2 (Ridge): R^2 Score for Reliable Neuron {neuron_index}: {score}\")\n",
    "\n",
    "## 5.1.3. For SimCLR final layer\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_layer_pcs, neural_responses_mean, test_size=0.2, random_state=42)\n",
    "reg = Ridge(alpha=1.0)\n",
    "reg.fit(X_train, y_train)\n",
    "Y_pred = reg.predict(X_test)\n",
    "final_layer_r2_scores = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "\n",
    "for neuron_index, score in zip(reliable_neuron_indices[:10], final_layer_r2_scores):\n",
    "    print(f\"Final Layer (Ridge): R^2 Score for Reliable Neuron {neuron_index}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of PCs to visualize\n",
    "num_pcs = 10\n",
    "\n",
    "# Create a grid for visualization\n",
    "fig, axes = plt.subplots(1, num_pcs, figsize=(12, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    pc_image = pca_raw.components_[i].reshape(images.shape[2], images.shape[3])  # Reshape to image dimensions\n",
    "    ax.imshow(pc_image, cmap=\"gray\")\n",
    "    ax.set_title(f\"PC {i+1}\")\n",
    "    ax.axis(\"off\")  # Hide axis for cleaner visualization\n",
    "\n",
    "# Adjust layout\n",
    "plt.suptitle(\"First 5 Principal Components as Images\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "## Visualise the principal components (project the image into PC space)\n",
    "print('PC shape:', pca_raw.components_.shape) # (32, 50176)\n",
    "print('images shape:', images.shape) # (1573, 3, 224, 224)\n",
    "\n",
    "# Visualize the first 5 PCs as images\n",
    "for i in range(5):\n",
    "    pc_image = pca_raw.components_[i].reshape(images.shape[2], images.shape[3])  # Reshape to image dimensions\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(pc_image, cmap=\"gray\")\n",
    "    plt.title(f\"Principal Component {i+1}\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(pca_layer2.components_[:5], cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"First 5 Principal Components - Layer 2 Features\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"PC Index\")\n",
    "plt.show()\n",
    "\n",
    "# raw_image_pcs[i, j] â The score of the i-th image for the j-th principal component\n",
    "# e.g. how much image i is represented by a PC \n",
    "# e.g. if a PC captures \"vertical edges\", an image with strong vertical edges will have a high score for that component\n",
    "print(raw_image_pcs[0, 0])\n",
    "\n",
    "# Compute correlation between each PC and neural responses\n",
    "pc_index = 0  # Change this to test different PCs\n",
    "correlations = np.corrcoef(raw_image_pcs[:, pc_index], neural_responses_mean, rowvar=False)[0, 1:]\n",
    "\n",
    "# Plot histogram of correlations\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(correlations, bins=30, kde=True)\n",
    "plt.xlabel(f\"Raw Images: Correlation between PC {pc_index+1} and neural responses\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(f\"Histogram of PC {pc_index+1} correlation with neurons\")\n",
    "plt.show()\n",
    "\n",
    "# Identify top 5 neurons most correlated with the first PC\n",
    "top_neurons = np.argsort(np.abs(correlations))[-5:]\n",
    "\n",
    "# Scatter plot for each top neuron\n",
    "for neuron_id in top_neurons:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(raw_image_pcs[:, 0], neural_responses_mean[:, neuron_id], alpha=0.5)\n",
    "    slope, intercept, r_value, _, _ = stats.linregress(raw_image_pcs[:, 0], neural_responses_mean[:, neuron_id])\n",
    "    plt.plot(raw_image_pcs[:, 0], slope * raw_image_pcs[:, 0] + intercept, color=\"red\", label=f\"R={r_value:.2f}\")\n",
    "    plt.xlabel(\"PC 1 Score\")\n",
    "    plt.ylabel(f\"Neuron {neuron_id} Response\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Neuron {neuron_id}: PC 1 vs Neural Response\")\n",
    "    plt.show()\n",
    "\n",
    "pc_index = 0  # Change this to test different PCs\n",
    "correlations = np.corrcoef(layer2_pcs[:, pc_index], neural_responses_mean, rowvar=False)[0, 1:]\n",
    "\n",
    "# Plot histogram of correlations\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(correlations, bins=30, kde=True)\n",
    "plt.xlabel(f\"Layer 2: Correlation between PC {pc_index+1} and neural responses\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(f\"Histogram of PC {pc_index+1} correlation with neurons\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for each top neuron\n",
    "for neuron_id in top_neurons:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(layer2_pcs[:, 0], neural_responses_mean[:, neuron_id], alpha=0.5)\n",
    "    slope, intercept, r_value, _, _ = stats.linregress(layer2_pcs[:, 0], neural_responses_mean[:, neuron_id])\n",
    "    plt.plot(layer2_pcs[:, 0], slope * layer2_pcs[:, 0] + intercept, color=\"red\", label=f\"R={r_value:.2f}\")\n",
    "    plt.xlabel(\"PC 1 Score\")\n",
    "    plt.ylabel(f\"Neuron {neuron_id} Response\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Neuron {neuron_id}: PC 1 vs Neural Response\")\n",
    "    plt.show()\n",
    "\n",
    "pc_index = 0  # Change this to test different PCs\n",
    "correlations = np.corrcoef(final_layer_feats[:, pc_index], neural_responses_mean, rowvar=False)[0, 1:]\n",
    "\n",
    "# Plot histogram of correlations\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(correlations, bins=30, kde=True)\n",
    "plt.xlabel(f\"Final Layer: Correlation between PC {pc_index+1} and neural responses\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(f\"Histogram of PC {pc_index+1} correlation with neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation between each PC and neural responses\n",
    "pc_index = 10  # Change this to test different PCs\n",
    "\n",
    "# Raw images\n",
    "raw_images_correlations = np.corrcoef(raw_image_pcs[:, pc_index], neural_responses_mean, rowvar=False)[0, 1:]\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(raw_images_correlations, bins=30, kde=True)\n",
    "plt.xlabel(f\"Correlation between PC {pc_index+1} and neural responses\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(f\"Raw Images: PC {pc_index+1} correlation with neurons\")\n",
    "plt.show()\n",
    "\n",
    "# Layer 1\n",
    "layer_1_correlations = np.corrcoef(layer1_pcs[:, pc_index], neural_responses_mean, rowvar=False)[0, 1:]\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(layer_1_correlations, bins=30, kde=True)\n",
    "plt.xlabel(f\"Correlation between PC {pc_index+1} and neural responses\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(f\"Layer 1: PC {pc_index+1} correlation with neurons\")\n",
    "plt.show()\n",
    "\n",
    "# Layer 2\n",
    "layer_2_correlations = np.corrcoef(layer2_pcs[:, pc_index], neural_responses_mean, rowvar=False)[0, 1:]\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(layer_2_correlations, bins=30, kde=True)\n",
    "plt.xlabel(f\"Correlation between PC {pc_index+1} and neural responses\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(f\"Layer 2: PC {pc_index+1} correlation with neurons\")\n",
    "plt.show()\n",
    "\n",
    "# Layer 3\n",
    "layer_3_correlations = np.corrcoef(layer3_pcs[:, pc_index], neural_responses_mean, rowvar=False)[0, 1:]\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(layer_3_correlations, bins=30, kde=True)\n",
    "plt.xlabel(f\"Correlation between PC {pc_index+1} and neural responses\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(f\"Layer 3: PC {pc_index+1} correlation with neurons\")\n",
    "plt.show()\n",
    "\n",
    "# Layer 4\n",
    "layer_4_correlations = np.corrcoef(layer4_pcs[:, pc_index], neural_responses_mean, rowvar=False)[0, 1:]\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(layer_4_correlations, bins=30, kde=True)\n",
    "plt.xlabel(f\"Correlation between PC {pc_index+1} and neural responses\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(f\"Layer 4: PC {pc_index+1} correlation with neurons\")\n",
    "plt.show()\n",
    "\n",
    "# Final Layer\n",
    "final_layer_correlations = np.corrcoef(final_layer_pcs[:, pc_index], neural_responses_mean, rowvar=False)[0, 1:]\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(final_layer_correlations, bins=30, kde=True)\n",
    "plt.xlabel(f\"Correlation between PC {pc_index+1} and neural responses\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(f\"Final Layer: PC {pc_index+1} correlation with neurons\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(raw_images_correlations, bins=30, kde=True, label=\"Raw Images\", color=\"blue\", alpha=0.5)\n",
    "sns.histplot(layer_2_correlations, bins=30, kde=True, label=\"Layer 2\", color=\"orange\", alpha=0.5)\n",
    "sns.histplot(final_layer_correlations, bins=30, kde=True, label=\"Final Layer\", color=\"red\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Correlation with Neural Responses\")\n",
    "plt.ylabel(\"Number of Neurons\")\n",
    "plt.title(f\"Comparison of PC {3+1} Correlations Across Representations\")\n",
    "plt.show()\n",
    "\n",
    "pc_indices = range(10)  # First 10 PCs\n",
    "avg_correlations = [np.mean(np.abs(np.corrcoef(raw_image_pcs[:, i], neural_responses_mean, rowvar=False)[0, 1:])) for i in pc_indices]\n",
    "\n",
    "plt.plot(pc_indices, avg_correlations, marker='o', linestyle='-')\n",
    "plt.xlabel(\"PC Index\")\n",
    "plt.ylabel(\"Average |Correlation| with Neural Responses\")\n",
    "plt.title(\"How Neural Response Correlation Varies Across PCs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Create random noise features with the same shape as SimCLR features\n",
    "random_features = np.random.randn(*layer4_feats.shape)\n",
    "\n",
    "# Train regression on SimCLR features\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(layer4_feats, neural_responses_mean, test_size=0.2, random_state=42, shuffle=False)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, Y_train)\n",
    "r2_simclr = reg.score(X_test, Y_test)\n",
    "\n",
    "# Train regression on random noise features\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(random_features, neural_responses_mean, test_size=0.2, random_state=42)\n",
    "reg.fit(X_train, Y_train)\n",
    "r2_random = reg.score(X_test, Y_test)\n",
    "\n",
    "print(f\"RÂ² using SimCLR: {r2_simclr:.4f}\")\n",
    "print(f\"RÂ² using Random Noise: {r2_random:.4f}\")\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Compute correlation between SimCLR features and neural responses\n",
    "correlations = []\n",
    "for neuron_idx in range(neural_responses_mean.shape[1]):  # Loop over neurons\n",
    "    corr, _ = pearsonr(layer4_feats[:, 0], neural_responses_mean[:, neuron_idx])  # Correlate 1st PC\n",
    "    correlations.append(corr)\n",
    "\n",
    "# Plot histogram of correlations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(correlations, bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Pearson Correlation\")\n",
    "plt.ylabel(\"Number of Neurons\")\n",
    "plt.title(\"Correlation of SimCLR Features with Neural Responses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regression using 10 PCs of raw images, and 500 most reliable neurons\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_representation = flattened_images # [flattened_images, layer1_feats, layer2_feats, layer3_feats, layer4_feats, final_layer_feats]\n",
    "num_pcs = 10\n",
    "num_neurons = 500\n",
    "reduce_dimensionality = False\n",
    "\n",
    "# ===================================\n",
    "# Filter only the top Y neurons (SRV)\n",
    "# ===================================\n",
    "reliable_srv_scores = real_srv_all_neurons[reliable_neuron_indices]\n",
    "sorted_indices = np.argsort(reliable_srv_scores)[::-1]\n",
    "most_reliable_neurons = reliable_neuron_indices[sorted_indices[:num_neurons]]\n",
    "highest_srv_scores = real_srv_all_neurons[most_reliable_neurons]\n",
    "neural_responses = imresps[:, :, most_reliable_neurons]\n",
    "neural_responses_mean = neural_responses.mean(axis=1)\n",
    "\n",
    "assert most_reliable_neurons.shape[0] == num_neurons, \"Mismatch in neuron selection!\"\n",
    "print(\"Dimensionality of neural responses:\", neural_responses_mean.shape)\n",
    "print(\"Top 500 reliable neuron indices:\", most_reliable_neurons[:10])\n",
    "print(\"Corresponding SRV scores:\", highest_srv_scores[:10])\n",
    "print(\"Top 500 neural responses shape:\", neural_responses.shape) # (1573, 2, 500)\n",
    "print(\"Averaged top 500 neural responses shape:\", neural_responses_mean.shape) # (1573, 500)\n",
    "\n",
    "# ======================\n",
    "# Get test-train split\n",
    "# ======================\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(image_representation, neural_responses_mean, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# ===========================================================\n",
    "# Apply PCA (only on training set).\n",
    "# Then transform validation and test sets using trained PCA\n",
    "# ===========================================================\n",
    "if reduce_dimensionality is True:\n",
    "    print(\"[INFO] Applying PCA to image representation (only on training set)!\")\n",
    "    \n",
    "    pca = PCA(n_components=num_pcs)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_val = pca.transform(X_val)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    print(f\"[INFO] PCA reduced image representation to {num_pcs} dimensions!\")\n",
    "else:\n",
    "    print(f\"[INFO] No PCA applied\")\n",
    "\n",
    "print(f\"Raw image representation shape: {image_representation.shape}\")\n",
    "print(f\"Image representation shape: Train={X_train.shape}, Val={X_val.shape}, Test={X_test.shape}\")\n",
    "\n",
    "# ===================================================\n",
    "# Run ridge regressions, tune regularisation param\n",
    "# ===================================================\n",
    "alphas = np.logspace(-1, 5, num=7)  # [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "train_scores, test_scores, val_scores = [], [], []\n",
    "best_alpha = None\n",
    "best_model = None\n",
    "best_val_r2 = float('-inf')\n",
    "\n",
    "for alpha in alphas:\n",
    "    reg = Ridge(alpha=alpha)\n",
    "    reg.fit(X_train, Y_train)\n",
    "\n",
    "    train_r2 = r2_score(Y_train, reg.predict(X_train))\n",
    "    test_r2 = r2_score(Y_test, reg.predict(X_test))\n",
    "    val_r2 = r2_score(Y_val, reg.predict(X_val))\n",
    "\n",
    "    print(f\"Alpha: {alpha}, Train RÂ²: {train_r2:.4f}, Val RÂ²: {val_r2:.4f}, Test RÂ²: {test_r2:.4f}\")\n",
    "\n",
    "    train_scores.append(train_r2)\n",
    "    test_scores.append(test_r2)\n",
    "    val_scores.append(val_r2)\n",
    "\n",
    "    if val_r2 > best_val_r2:\n",
    "        best_val_r2 = val_r2\n",
    "        best_alpha = alpha\n",
    "        best_model = reg\n",
    "\n",
    "if best_model is None:\n",
    "    print(\"[WARNING]: No best model was found. Check your data or hyperparameters.\")\n",
    "    best_model = Ridge(alpha=best_alpha).fit(X_train, Y_train)\n",
    "\n",
    "print(f\"\\nBest Î±: {best_alpha} with Test RÂ²: {best_val_r2:.4f}\")\n",
    "\n",
    "# ===================================\n",
    "# Visualisation\n",
    "# ===================================\n",
    "plt.plot(alphas, train_scores, marker='o', label='Train RÂ²')\n",
    "plt.plot(alphas, val_scores, marker='s', label='Validation RÂ²')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Ridge Regularization (Î±)\")\n",
    "plt.ylabel(\"RÂ² Score\")\n",
    "plt.title(\"Effect of Ridge Regularization on Model Performance\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "Y_pred = best_model.predict(X_test)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(Y_test.flatten(), Y_pred.flatten(), alpha=0.5)\n",
    "plt.plot([-2, 2], [-2, 2], 'r--', label=\"Ideal Fit\")\n",
    "plt.xlabel(\"True Neural Response\")\n",
    "plt.ylabel(\"Predicted Neural Response\")\n",
    "plt.title(f\"Predictions vs. Actual Neural Responses (Î±={best_alpha})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Â Plot regression scores for a particular layer, for varying number of PCs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image_representation = flattened_images # [flattened_images, layer1_feats, layer2_feats, layer3_feats, layer4_feats, final_layer_feats]\n",
    "alpha = 100000\n",
    "pcs_90pc_var = 372 # {'Raw Images': 372, 'Layer 1': 1016, 'Layer 2': 909, 'Layer 3': 701, 'Layer 4': 519, 'Final Layer': 119}\n",
    "num_neurons = 500\n",
    "\n",
    "# ===================================\n",
    "# Filter only the top Y neurons (SRV)\n",
    "# ===================================\n",
    "reliable_srv_scores = real_srv_all_neurons[reliable_neuron_indices]\n",
    "sorted_indices = np.argsort(reliable_srv_scores)[::-1]\n",
    "most_reliable_neurons = reliable_neuron_indices[sorted_indices[:num_neurons]]\n",
    "highest_srv_scores = real_srv_all_neurons[most_reliable_neurons]\n",
    "neural_responses = imresps[:, :, most_reliable_neurons]\n",
    "neural_responses_mean = neural_responses.mean(axis=1)\n",
    "\n",
    "assert most_reliable_neurons.shape[0] == num_neurons, \"Mismatch in neuron selection!\"\n",
    "print(\"Dimensionality of neural responses:\", neural_responses_mean.shape)\n",
    "print(\"Top 500 reliable neuron indices:\", most_reliable_neurons[:10])\n",
    "print(\"Corresponding SRV scores:\", highest_srv_scores[:10])\n",
    "print(\"Top 500 neural responses shape:\", neural_responses.shape) # (1573, 2, 500)\n",
    "print(\"Averaged top 500 neural responses shape:\", neural_responses_mean.shape) # (1573, 500)\n",
    "\n",
    "# ======================\n",
    "# Get test-train split\n",
    "# ======================\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(image_representation, neural_responses_mean, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "pcs_values = [10, 50, 100, 200, 500]\n",
    "train_r2_scores = []\n",
    "val_r2_scores = []\n",
    "test_r2_scores = []\n",
    "\n",
    "for num_pcs in pcs_values:\n",
    "    pca = PCA(n_components=num_pcs)\n",
    "    \n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    reg = Ridge(alpha=alpha)\n",
    "    reg.fit(X_train_pca, Y_train)\n",
    "\n",
    "    train_r2 = r2_score(Y_train, reg.predict(X_train_pca))\n",
    "    val_r2 = r2_score(Y_val, reg.predict(X_val_pca))\n",
    "    test_r2 = r2_score(Y_test, reg.predict(X_test_pca))\n",
    "\n",
    "    train_r2_scores.append(train_r2)\n",
    "    val_r2_scores.append(val_r2)\n",
    "    test_r2_scores.append(test_r2)\n",
    "\n",
    "    print(f\"PCs: {num_pcs}, Train RÂ²: {train_r2:.4f}, Val RÂ²: {val_r2:.4f}, Test RÂ²: {test_r2:.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(pcs_values, train_r2_scores, marker='o', linestyle='--', label='Train RÂ²')\n",
    "plt.plot(pcs_values, val_r2_scores, marker='s', linestyle='-', label='Validation RÂ²')\n",
    "plt.plot(pcs_values, test_r2_scores, marker='^', linestyle='-', label='Test RÂ²')\n",
    "\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"RÂ² Score\")\n",
    "plt.axvline(x=pcs_90pc_var, color='red', linestyle='--', label=f\"90% Variance ({pcs_90pc_var} PCs)\")\n",
    "plt.title(\"Effect of PCA on Ridge Regression Performance\")\n",
    "plt.legend()\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')  # Reference line at RÂ² = 0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regression performance across image representations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the feature types\n",
    "feature_types = [\"Raw Pixels\", \"SimCLR Layer 1\", \"SimCLR Layer 2\", \"SimCLR Layer 3\", \"SimCLR Layer 4\", \"Final SimCLR Layer\"]\n",
    "train_r2_scores, val_r2_scores, test_r2_scores = [], [], []\n",
    "\n",
    "for feature in feature_types:\n",
    "    # Select the feature representation\n",
    "    if feature == \"Raw Pixels\":\n",
    "        X_feature = flattened_images\n",
    "    elif feature == \"SimCLR Layer 1\":\n",
    "        X_feature = layer1_feats\n",
    "    elif feature == \"SimCLR Layer 2\":\n",
    "        X_feature = layer2_feats\n",
    "    elif feature == \"SimCLR Layer 3\":\n",
    "        X_feature = layer3_feats\n",
    "    elif feature == \"SimCLR Layer 4\":\n",
    "        X_feature = layer4_feats\n",
    "    elif feature == \"Final SimCLR Layer\":\n",
    "        X_feature = final_layer_feats\n",
    "\n",
    "    # Split into train, val, test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_feature, neural_responses_mean, test_size=0.2, random_state=42, shuffle=False)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "    # Apply PCA if needed\n",
    "    num_pcs = 100  # Adjust based on previous PCA analysis\n",
    "    pca = PCA(n_components=num_pcs)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Train Ridge Regression\n",
    "    reg = Ridge(alpha=100000)\n",
    "    reg.fit(X_train_pca, Y_train)\n",
    "\n",
    "    # Compute RÂ² scores\n",
    "    train_r2 = r2_score(Y_train, reg.predict(X_train_pca))\n",
    "    val_r2 = r2_score(Y_val, reg.predict(X_val_pca))\n",
    "    test_r2 = r2_score(Y_test, reg.predict(X_test_pca))\n",
    "\n",
    "    train_r2_scores.append(train_r2)\n",
    "    val_r2_scores.append(val_r2)\n",
    "    test_r2_scores.append(test_r2)\n",
    "\n",
    "    print(f\"{feature}: Train RÂ²: {train_r2:.4f}, Val RÂ²: {val_r2:.4f}, Test RÂ²: {test_r2:.4f}\")\n",
    "\n",
    "# Plot bar chart\n",
    "x = np.arange(len(feature_types))\n",
    "width = 0.3  # Width of bars\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - width, train_r2_scores, width=width, label='Train RÂ²', color='blue', alpha=0.7)\n",
    "plt.bar(x, val_r2_scores, width=width, label='Validation RÂ²', color='orange', alpha=0.7)\n",
    "plt.bar(x + width, test_r2_scores, width=width, label='Test RÂ²', color='green', alpha=0.7)\n",
    "\n",
    "plt.xticks(x, feature_types, rotation=45, ha='right')\n",
    "plt.ylabel(\"RÂ² Score\")\n",
    "plt.title(\"Comparison of Ridge Regression Performance Across Feature Types\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_feature = layer3_feats \n",
    "Y_target = neural_responses_mean\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_feature, Y_target, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "num_pcs = 10\n",
    "pca_x = PCA(n_components=num_pcs)\n",
    "X_train_pca = pca_x.fit_transform(X_train)\n",
    "X_val_pca = pca_x.transform(X_val)\n",
    "X_test_pca = pca_x.transform(X_test)\n",
    "\n",
    "Y_train_single = Y_train[:, 0]\n",
    "Y_val_single = Y_val[:, 0]\n",
    "Y_test_single = Y_test[:, 0]\n",
    "\n",
    "svr = SVR(kernel='rbf', C=10, epsilon=0.1) # C and epsilon can be tuned\n",
    "svr.fit(X_train_pca, Y_train_single)\n",
    "\n",
    "train_r2_single = r2_score(Y_train_single, svr.predict(X_train_pca))\n",
    "val_r2_single = r2_score(Y_val_single, svr.predict(X_val_pca))\n",
    "test_r2_single = r2_score(Y_test_single, svr.predict(X_test_pca))\n",
    "\n",
    "print(f\" SVR (Single Neuron) Results:\")\n",
    "print(f\"Train RÂ²: {train_r2_single:.4f}, Val RÂ²: {val_r2_single:.4f}, Test RÂ²: {test_r2_single:.4f}\")\n",
    "\n",
    "pca_y = PCA(n_components=1)\n",
    "Y_train_pca = pca_y.fit_transform(Y_train)\n",
    "Y_val_pca = pca_y.transform(Y_val)\n",
    "Y_test_pca = pca_y.transform(Y_test)\n",
    "svr.fit(X_train_pca, Y_train_pca.ravel())\n",
    "train_r2_pc = r2_score(Y_train_pca, svr.predict(X_train_pca))\n",
    "val_r2_pc = r2_score(Y_val_pca, svr.predict(X_val_pca))\n",
    "test_r2_pc = r2_score(Y_test_pca, svr.predict(X_test_pca))\n",
    "\n",
    "print(f\"\\n SVR (First PC of Neural Responses) Results:\")\n",
    "print(f\"Train RÂ²: {train_r2_pc:.4f}, Val RÂ²: {val_r2_pc:.4f}, Test RÂ²: {test_r2_pc:.4f}\")\n",
    "\n",
    "models = [\"Ridge (Î±=100K)\", \"SVR (Neuron 1)\", \"SVR (First PC)\"]\n",
    "test_r2_scores = [0.0240, test_r2_single, test_r2_pc]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(models, test_r2_scores, color=['blue', 'green', 'purple'])\n",
    "plt.ylabel(\"Test RÂ² Score\")\n",
    "plt.title(\"Regression Performance: Ridge vs. SVR on SimCLR Layer 3\")\n",
    "plt.ylim(-0.05, max(test_r2_scores) + 0.01)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
