{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I have been using features from layer1, layer2, layer3, layer4, final layer\n",
    "### Let's try with the individual conv layers e.g. layer2.0.conv1\n",
    "### Cadena et al. use sublayers for VGG-19 https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006897#sec002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### 1.1. Load data\n",
    "\n",
    "# imresps.npy is of shape (1573, 2, 15363), where 1573 is number of images, 2 repeats each, and 15363 neurons recorded\n",
    "#Â stimids.npy has the image id (matching the image dataset ~selection1866~) for each stimulus number, \n",
    "# so of you want to see what image was presented on imresps[502] you would check stim_ids[502]\n",
    "\n",
    "PATH_TO_DATA = '../../data/neural'\n",
    "\n",
    "imresps = np.load(path.join(PATH_TO_DATA, 'imresps.npy'))\n",
    "stimids = np.load(path.join(PATH_TO_DATA, 'stimids.npy'))\n",
    "\n",
    "print(imresps.shape) # (1573, 2, 15363)\n",
    "print(stimids.shape) # (1573,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1. Compute the null distribution of SRV values for all neurons\n",
    "\n",
    "def compute_signal_related_variance(resp_a, resp_b, mean_center=True):\n",
    "    \"\"\"\n",
    "    compute the fraction of signal-related variance for each neuron,\n",
    "    as per Stringer et al Nature 2019. Cross-validated by splitting\n",
    "    responses into two halves. Note, this only is \"correct\" if resp_a\n",
    "    and resp_b are *not* averages of many trials.\n",
    "\n",
    "    Args:\n",
    "        resp_a (ndarray): n_stimuli, n_cells\n",
    "        resp_b (ndarray): n_stimuli, n_cells\n",
    "\n",
    "    Returns:\n",
    "        fraction_of_stimulus_variance: 0-1, 0 is non-stimulus-caring, 1 is only-stimulus-caring neurons\n",
    "        stim_to_noise_ratio: ratio of the stim-related variance to all other variance\n",
    "    \"\"\"\n",
    "    if len(resp_a.shape) > 2:\n",
    "        # if the stimulus is multi-dimensional, flatten across all stimuli\n",
    "        resp_a = resp_a.reshape(-1, resp_a.shape[-1])\n",
    "        resp_b = resp_b.reshape(-1, resp_b.shape[-1])\n",
    "    ns, nc = resp_a.shape\n",
    "    if mean_center:\n",
    "        # mean-center the activity of each cell\n",
    "        resp_a = resp_a - resp_a.mean(axis=0)\n",
    "        resp_b = resp_b - resp_b.mean(axis=0)\n",
    "    \n",
    "    # compute the cross-trial stimulus covariance of each cell\n",
    "    # dot-product each cell's (n_stim, ) vector from one half\n",
    "    # with its own (n_stim, ) vector on the other half\n",
    "\n",
    "    covariance = (resp_a * resp_b).sum(axis=0) / ns\n",
    "\n",
    "    # compute the variance of each cell across both halves\n",
    "    resp_a_variance = (resp_a**2).sum(axis=0) / ns\n",
    "    resp_b_variance = (resp_b**2).sum(axis=0) / ns\n",
    "    total_variance = (resp_a_variance + resp_b_variance) / 2\n",
    "\n",
    "    if np.any(total_variance < 1e-12):\n",
    "        print(f\"Warning: Near-zero total variance for neurons: {np.where(total_variance < 1e-12)[0]}\")\n",
    "\n",
    "    # compute the fraction of the total variance that is\n",
    "    # captured in the covariance\n",
    "    fraction_of_stimulus_variance = covariance / total_variance\n",
    "\n",
    "    # if you want, you can compute SNR as well:\n",
    "    stim_to_noise_ratio = fraction_of_stimulus_variance / (\n",
    "        1 - fraction_of_stimulus_variance\n",
    "    )\n",
    "\n",
    "    return fraction_of_stimulus_variance, stim_to_noise_ratio\n",
    "\n",
    "# TODO: double check INDEXING (images, cells)\n",
    "\n",
    "# imresps shape = (1573, 2, 15363)\n",
    "# responses in imresps shape = (2, 15363)\n",
    "num_stimuli = imresps.shape[0] # 1573\n",
    "num_repeats = imresps.shape[1] # 2\n",
    "num_neurons = imresps.shape[2] # 15363\n",
    "n_shuffles = 100\n",
    "\n",
    "null_srv_all_neurons = [] # shape (n_shuffles, num_neurons)\n",
    "\n",
    "for _ in range(n_shuffles):\n",
    "    # Shuffle stimulus indices *twice* to create two independent splits!\n",
    "    shuffled_indices_A = np.random.permutation(num_stimuli)\n",
    "    shuffled_indices_B = np.random.permutation(num_stimuli)\n",
    "\n",
    "    # Now for the splits, we can just use fixed repeat indices, \n",
    "    # because for each split, at index N the responses correspond to different stimuli\n",
    "    # e.g. split_A = [ stim_100_repeat_1, stim_2_repeat_1, stim_19_repeat_1, ... ]\n",
    "    # e.g. split_B = [ stim_543_repeat_2, stim_345_repeat_2, stim_3_repeat_2, ... ]\n",
    "    split_A = imresps[shuffled_indices_A, 0, :]\n",
    "    split_B = imresps[shuffled_indices_B, 1, :]\n",
    "\n",
    "    # Compute SRV for the shuffled data\n",
    "    fraction_of_stimulus_variance, _ = compute_signal_related_variance(split_A, split_B)\n",
    "    null_srv_all_neurons.append(fraction_of_stimulus_variance)\n",
    "\n",
    "null_srv_all_neurons = np.array(null_srv_all_neurons)\n",
    "null_srv_all_neurons.shape # (100, 15363)\n",
    "\n",
    "print(null_srv_all_neurons[0])\n",
    "print(null_srv_all_neurons[33])\n",
    "\n",
    "# e.g. if neuron_index = 0, it will plot the SRV value for neuron 0 across all shuffles\n",
    "neuron_index = 0\n",
    "plt.hist([srv[neuron_index] for srv in null_srv_all_neurons], bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Shuffles\")\n",
    "plt.title(f\"Null Distribution of SRV for Neuron {neuron_index}\")\n",
    "plt.show()\n",
    "\n",
    "### 2.2. Compute the real SRV for each neuron\n",
    "\n",
    "# TODO: Question for Ali: why can't we just split like this?\n",
    "# split_A_real = imresps[:, 0, :] # First repeat for each stimulus\n",
    "# split_B_real = imresps[:, 1, :] # Second repeat for each stimulus\n",
    "\n",
    "split_A, split_B = [], []\n",
    "for responses in imresps: # responses shape: (2, n_neurons)\n",
    "    indices = np.random.permutation(2) # Randomly shuffle [0, 1]\n",
    "    split_A.append(responses[indices[0]]) # Assign one repeat to split_A\n",
    "    split_B.append(responses[indices[1]]) # Assign the other to split_B\n",
    "\n",
    "split_A = np.array(split_A)  # Shape: (n_stimuli, n_neurons)\n",
    "split_B = np.array(split_B)  # Shape: (n_stimuli, n_neurons)\n",
    "\n",
    "# Compute SRV for real data\n",
    "real_srv_all_neurons, stim_to_noise_ratio = compute_signal_related_variance(split_A, split_B)\n",
    "\n",
    "print(real_srv_all_neurons)\n",
    "print(stim_to_noise_ratio)\n",
    "\n",
    "print(\"Real SRV shape:\", real_srv_all_neurons.shape) # Should be (15363,)\n",
    "\n",
    "plt.hist(real_srv_all_neurons, bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Shuffles\")\n",
    "plt.title(f\"Null Distribution of SRV for Neuron {neuron_index}\")\n",
    "plt.show()\n",
    "\n",
    "### 2.3. Filter neurons whose real SRV is in the top 90th percentile of its null distribution\n",
    "\n",
    "# This gives the 90th-percentile SRV value of the null distribution for each neuron\n",
    "# In other words the threshold for each neuron to be considered reliable\n",
    "# e.g. if neuron 0 has a null distribution of SRVs across 10 shuffles \n",
    "# [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], the threshold would be 0.9\n",
    "top_99th_percentile_null = np.percentile(null_srv_all_neurons, 99, axis=0)\n",
    "print(top_99th_percentile_null) # [0.03651716 0.03126347 0.03325775 ... 0.02738261 0.03546677 0.0333109 ]\n",
    "\n",
    "# Get indices of reliable neurons\n",
    "reliable_neuron_indices = np.where(real_srv_all_neurons >= top_99th_percentile_null)[0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of reliable neurons: {len(reliable_neuron_indices)}\") # 5654\n",
    "print(f\"Indices of reliable neurons: {reliable_neuron_indices}\") # [   14    29    48 ... 15357 15358 15360]\n",
    "\n",
    "plt.hist(real_srv_all_neurons, bins=100, color='red', alpha=0.7)\n",
    "plt.hist(real_srv_all_neurons[reliable_neuron_indices], bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Shuffles\")\n",
    "plt.title(\"All Neurons: SRV all vs. SRV reliable\")\n",
    "plt.show()\n",
    "\n",
    "# Gather the neural responses for the reliable neurons\n",
    "# we take the average across repeats for each neuron\n",
    "neural_responses = imresps[:, :, reliable_neuron_indices] # Shape: (1573, 2, 5654)\n",
    "neural_responses_mean = neural_responses.mean(axis=1) # Shape: (1573, 5654) -> 1573 images, 5654 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1. Load and preprocess images\n",
    "\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import Normalize, Compose, Resize, CenterCrop\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import utils as torch_utils\n",
    " \n",
    "PATH_TO_DATA = '../../data/selection1866'\n",
    "\n",
    "file_list = sorted(f for f in os.listdir(PATH_TO_DATA) if f.endswith('.mat'))\n",
    "stim_ids = stimids.astype(int)\n",
    "\n",
    "print(stim_ids)\n",
    "print(stimids)\n",
    "\n",
    "# TODO: run tile 1 and 2 through model separately + concat feature reps (no crop, pad)\n",
    "transform = Compose([\n",
    "    Resize(96), # Resize shortest edge to 96 (cut off the rightmost part of the image)\n",
    "    CenterCrop((96, 96)), # Crop to (96, 96)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # !! Normalize expects input is already in the range [0, 1]\n",
    "])\n",
    "\n",
    "img_tensors, labels = [], []\n",
    "\n",
    "print('List:', file_list)\n",
    "\n",
    "# we have 1866 images here, but the neural response data only uses 1573 of them\n",
    "# because some ~300 images didn't have two repeats, so were disposed\n",
    "# therefore we filter the full set here so that we only use the relevant 1573\n",
    "for stim_id in stim_ids:\n",
    "    filename = 'img' + str(stim_id) + '.mat'\n",
    "    data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "\n",
    "    img = data['img'][:, :500] # Take leftmost part of the image\n",
    "    rgb_img = np.stack([img] * 3, axis=-1) # Convert grayscale to RGB for SimCLR\n",
    "    tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "    \n",
    "    # Min-max scale the tensor to [0, 1]\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Clamp to [0, 1] to ensure no outliers due to numerical precision\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)\n",
    "\n",
    "    transformed_tensor = transform(tensor) # Normalize and resize for SimCLR\n",
    "    img_tensors.append(transformed_tensor)\n",
    "    labels.append(stim_id)\n",
    "\n",
    "image_dataset = TensorDataset(torch.stack(img_tensors), torch.tensor(labels))\n",
    "\n",
    "images, labels = image_dataset.tensors\n",
    "print(\"Labels:\", labels[:10])\n",
    "print(\"Processed dataset shape:\", images.shape) # (N, C, 96, 96)\n",
    "print(f\"Min pixel value (processed): {torch.min(images)}\")\n",
    "print(f\"Max pixel value (processed): {torch.max(images)}\")\n",
    "\n",
    "# Show a sample of processed images\n",
    "img_grid = torch_utils.make_grid(images[:12], nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Processed images: sample')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2. Run images through a pretrained SimCLR model and extract features\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Base ResNet18 backbone (pretrained=False, because we load custom weights later, from the SimCLR checkpoint file)\n",
    "        self.convnet = torchvision.models.resnet18(pretrained=False)\n",
    "        \n",
    "        # This is the projection head, only needed during training. For downstream tasks it is disposed of\n",
    "        # and the final linear layer output is used (Chen et al., 2020) \n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.activations = {}\n",
    "        self.num_workers = os.cpu_count()\n",
    "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        \"\"\"\n",
    "        Register forward hooks to capture activations of convolutional layers.\n",
    "        \"\"\"\n",
    "        def hook_fn(layer_name):\n",
    "            def hook(module, input, output):\n",
    "                if layer_name not in self.activations:\n",
    "                    self.activations[layer_name] = []\n",
    "                self.activations[layer_name].append(output.detach().cpu())\n",
    "            return hook\n",
    "\n",
    "        # Register hooks for all convolutional layers\n",
    "        for name, layer in self.convnet.named_modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                layer.register_forward_hook(hook_fn(name))\n",
    "\n",
    "    def load_pretrained(self):\n",
    "        \"\"\"\n",
    "        Load pretrained SimCLR weights\n",
    "        \"\"\"\n",
    "        base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial17/\"\n",
    "        models_dir = \"../../models\"\n",
    "        pretrained_simclr_filename = \"SimCLR.ckpt\"\n",
    "        pretrained_simclr_path = os.path.join(models_dir, pretrained_simclr_filename)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Check whether the pretrained model file already exists locally. If not, try downloading it\n",
    "        file_url = base_url + pretrained_simclr_filename\n",
    "        if not os.path.isfile(pretrained_simclr_path):\n",
    "            print(f\"Downloading pretrained SimCLR model {file_url}...\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(file_url, pretrained_simclr_path)\n",
    "            except HTTPError as e:\n",
    "                print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
    "\n",
    "        print(f\"Already downloaded pretrained model: {file_url}\")\n",
    "\n",
    "        # Load pretrained model\n",
    "        checkpoint = torch.load(pretrained_simclr_path, map_location=self.device)\n",
    "        self.load_state_dict(checkpoint['state_dict'])\n",
    "        self.to(self.device)\n",
    "        self.eval()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \"\"\"\n",
    "        return self.convnet(x)\n",
    "\n",
    "sim_clr = SimCLR()\n",
    "sim_clr.load_pretrained()\n",
    "\n",
    "data_loader = DataLoader(image_dataset, batch_size=64, shuffle=False, drop_last=False, num_workers=sim_clr.num_workers)\n",
    "sim_clr.activations = {}\n",
    "\n",
    "# Process images in batches\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in data_loader:\n",
    "        imgs = imgs.to(sim_clr.device)\n",
    "        sim_clr(imgs) # Forward pass (hooks will collect activations)\n",
    "\n",
    "# Convert list of activations to tensors\n",
    "for layer_name in sim_clr.activations:\n",
    "    sim_clr.activations[layer_name] = torch.cat(sim_clr.activations[layer_name], dim=0)\n",
    "\n",
    "# Print stored activations\n",
    "for layer_name, activation in sim_clr.activations.items():\n",
    "    print(f\"Layer: {layer_name}, Activation Shape: {activation.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Our original images are grayscale, but SimCLR expects 3-channel RGB input.\n",
    "# To meet this requirement, we duplicated the grayscale values across all three RGB channels.\n",
    "# However, for PCA, we only need a single channel, so we extract just the first channel (Red).\n",
    "raw_pixels_flat = images[:, 0, :, :].view(images.shape[0], -1)\n",
    "\n",
    "print(images.shape) # torch.Size([1573, 3, 96, 96])\n",
    "print(raw_pixels_flat.shape) # torch.Size([1573, 9216]) - (1573, 1*96*96)\n",
    "\n",
    "num_pcs = 100 \n",
    "\n",
    "pca = PCA(n_components=num_pcs)\n",
    "pca.fit(raw_pixels_flat)\n",
    "raw_pixels_ev = pca.explained_variance_ratio_\n",
    "raw_pixels_cev = np.cumsum(raw_pixels_ev)\n",
    "raw_pixels_features = pca.transform(raw_pixels_flat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# dict_keys(['conv1', 'layer1.0.conv1', 'layer1.0.conv2', 'layer1.1.conv1', 'layer1.1.conv2', 'layer2.0.conv1', 'layer2.0.conv2', 'layer2.0.downsample.0', 'layer2.1.conv1', 'layer2.1.conv2', 'layer3.0.conv1', 'layer3.0.conv2', 'layer3.0.downsample.0', 'layer3.1.conv1', 'layer3.1.conv2', 'layer4.0.conv1', 'layer4.0.conv2', 'layer4.0.downsample.0', 'layer4.1.conv1', 'layer4.1.conv2'])\n",
    "print(sim_clr.activations.keys())\n",
    "\n",
    "# layer4.1.conv2\n",
    "l4conv2_activations = sim_clr.activations['layer4.1.conv2']\n",
    "N, C, H, W = l4conv2_activations.shape\n",
    "l4conv2_activations_flat = l4conv2_activations.view(N, -1).numpy() # Shape: (N, C*H*W)\n",
    "\n",
    "print(l4conv2_activations.shape) # torch.Size([1573, 512, 3, 3])\n",
    "print(l4conv2_activations_flat.shape) # (1573, 4608)\n",
    "\n",
    "pca_l4conv2 = PCA(n_components=num_pcs)\n",
    "pca_l4conv2.fit(l4conv2_activations_flat)\n",
    "l4conv2_ev = pca_l4conv2.explained_variance_ratio_\n",
    "l4conv2_cev = np.cumsum(l4conv2_ev)\n",
    "l4conv2_features = pca_l4conv2.transform(l4conv2_activations_flat)\n",
    "\n",
    "# layer1.1.conv1\n",
    "l1conv1_activations = sim_clr.activations['layer1.1.conv1']\n",
    "N, C, H, W = l1conv1_activations.shape\n",
    "l1conv1_activations_flat = l1conv1_activations.view(N, -1).numpy() # Shape: (N, C*H*W)\n",
    "\n",
    "print(l1conv1_activations.shape)\n",
    "print(l1conv1_activations_flat.shape)\n",
    "\n",
    "pca_l1conv1 = PCA(n_components=num_pcs)\n",
    "pca_l1conv1.fit(l1conv1_activations_flat)\n",
    "l1conv1_ev = pca_l1conv1.explained_variance_ratio_\n",
    "l1conv1_cev = np.cumsum(l1conv1_ev)\n",
    "l1conv1_features = pca_l1conv1.transform(l1conv1_activations_flat)\n",
    "\n",
    "# layer3.1.conv1\n",
    "l3conv1_activations = sim_clr.activations['layer3.1.conv1']\n",
    "N, C, H, W = l3conv1_activations.shape\n",
    "l3conv1_activations_flat = l3conv1_activations.view(N, -1).numpy() # Shape: (N, C*H*W)\n",
    "\n",
    "pca_l3conv1 = PCA(n_components=num_pcs)\n",
    "pca_l3conv1.fit(l3conv1_activations_flat)\n",
    "l3conv1_ev = pca_l3conv1.explained_variance_ratio_\n",
    "l3conv1_cev = np.cumsum(l3conv1_ev)\n",
    "l3conv1_features = pca_l3conv1.transform(l3conv1_activations_flat)\n",
    "\n",
    "# Plot PCA for comparison\n",
    "plot_components = range(1, num_pcs + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(plot_components, l4conv2_cev, label=\"layer4.1.conv2\", marker='x')\n",
    "plt.plot(plot_components, l3conv1_cev, label=\"layer3.1.conv2\", marker='x')\n",
    "plt.plot(plot_components, l1conv1_cev, label=\"layer1.1.conv1\", marker='x')\n",
    "plt.plot(plot_components, raw_pixels_cev, label=\"Raw Pixels\", marker='x')\n",
    "plt.xlabel(\"# Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance vs. Number of Components\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run regression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(l1conv1_features, neural_responses_mean, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train Ridge regression (per neuron)\n",
    "# reg = Ridge(alpha=1.0) # Adjust alpha for regularization\n",
    "# reg.fit(X_train, Y_train)\n",
    "\n",
    "# # Predict\n",
    "# Y_pred = reg.predict(X_test)\n",
    "\n",
    "# # Evaluate\n",
    "# r2_scores = r2_score(Y_test, Y_pred, multioutput='raw_values')\n",
    "# print(r2_scores)\n",
    "\n",
    "# for alpha in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "#     ridge_reg = Ridge(alpha=alpha)\n",
    "#     ridge_reg.fit(X_train, Y_train)\n",
    "#     print(f\"Alpha {alpha}: RÂ² = {ridge_reg.score(X_test, Y_test):.4f}\")\n",
    "\n",
    "# corr_matrix = np.corrcoef(X_train.T, Y_train.T)[:X_train.shape[1], X_train.shape[1]:]\n",
    "# print(f\"Mean absolute correlation: {np.abs(corr_matrix).mean()}\")\n",
    "\n",
    "# earlier layers\n",
    "# [\"conv1\", \"layer1.0.conv1\", \"layer1.0.conv2\"]\n",
    "layer_activations = sim_clr.activations['layer4.1.conv2']\n",
    "N, C, H, W = layer_activations.shape\n",
    "layer_activations_flat = layer_activations.view(N, -1).numpy() # Shape: (N, C*H*W)\n",
    "pca = PCA(n_components=50)  # Try 10, 20, 50, 100 PCs\n",
    "X_pca = pca.fit_transform(layer_activations_flat)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, neural_responses_mean, test_size=0.2, random_state=42)\n",
    "reg = Ridge(alpha=1.0)\n",
    "reg.fit(X_train, Y_train)\n",
    "corr_matrix = np.corrcoef(X_train.T, Y_train.T)[:X_train.shape[1], X_train.shape[1]:]\n",
    "print(f\"Mean absolute correlation: {np.abs(corr_matrix).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(128, 64), activation='relu', max_iter=500)\n",
    "mlp.fit(X_train, Y_train)\n",
    "Y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "r2_scores_mlp = r2_score(Y_test, Y_pred_mlp, multioutput='raw_values')\n",
    "print(\"RÂ² using MLP:\", r2_scores_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3conv1_activations = sim_clr.activations['layer3.1.conv1']\n",
    "N, C, H, W = l3conv1_activations.shape\n",
    "l3conv1_activations_flat = l3conv1_activations.view(N, -1).numpy() # Shape: (N, C*H*W)\n",
    "\n",
    "num_components = 2  # Try 50, 100, 200\n",
    "pca = PCA(n_components=num_components)\n",
    "layer4_pca = pca.fit_transform(l3conv1_activations_flat)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(layer4_pca, neural_responses_mean, test_size=0.2, random_state=42)\n",
    "\n",
    "reg.fit(X_train, Y_train)\n",
    "r2_scores_pca = r2_score(Y_test, reg.predict(X_test), multioutput='raw_values')\n",
    "\n",
    "print(f\"RÂ² using {num_components} PCs:\", r2_scores_pca)\n",
    "best_neurons = np.argsort(r2_scores_pca)[-10:]  # Top 10 neurons by RÂ²\n",
    "print(f\"Best neurons: {best_neurons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pcs_to_plot = [0, 1, 2, 3, 4]\n",
    "neuron_indices_to_plot = [0, 10, 100, 1000]\n",
    "\n",
    "for neuron_index in neuron_indices_to_plot:\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    \n",
    "    for i, pc in enumerate(pcs_to_plot):\n",
    "        ax = plt.subplot(1, len(pcs_to_plot), i + 1)\n",
    "        \n",
    "        # Scatter plot: x = PC values, y = neural response for this neuron\n",
    "        ax.scatter(X_pca[:, pc], neural_responses_mean[:, neuron_index], alpha=0.6)\n",
    "        \n",
    "        # Compute Pearson correlation coefficient for this PC and neuron response\n",
    "        corr_coef = np.corrcoef(X_pca[:, pc], neural_responses_mean[:, neuron_index])[0, 1]\n",
    "        \n",
    "        ax.set_title(f\"PC {pc+1}\\nCorr: {corr_coef:.3f}\")\n",
    "        ax.set_xlabel(f\"PC {pc+1}\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f\"Neuron {neuron_index} Resp.\")\n",
    "    \n",
    "    plt.suptitle(f\"Scatter plots for Neuron {neuron_index}\")\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
