{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Collection and Preprocessing\n",
    "\n",
    "# Neural responses were recorded from 166 neurons in macaque V1 using a 32-channel array.\n",
    "# Stimuli were natural images and synthesized textures, presented at 60 ms per image with no blank screens.\n",
    "# Spike counts were extracted from 40-100 ms after stimulus onset.\n",
    "# [TODO] Only neurons with at least 15% of their variance attributable to the stimulus were included.\n",
    "\n",
    "# 2. Feature Extraction from VGG-19\n",
    "\n",
    "# VGG-19, pre-trained on ImageNet, was used to extract features.\n",
    "# Conv3_1 was chosen as the best feature layer for V1 response prediction.\n",
    "# [TODO] Feature maps from VGG-19 were passed through batch normalization before being used for fitting.\n",
    "\n",
    "# 3. Generalized Linear Model (GLM) Readout\n",
    "\n",
    "# A GLM with a Poisson loss function was used to map VGG-19 features to neural responses.\n",
    "# Three regularization terms were applied:\n",
    "# L1 sparsity: Encourages feature selection.\n",
    "# Spatial smoothness: Ensures receptive field locality.\n",
    "# Group sparsity: Encourages pooling from a subset of feature maps.\n",
    "\n",
    "# 4. Performance Evaluation Using FEV\n",
    "\n",
    "# Fraction of Explainable Variance Explained (FEV) was computed as:\n",
    "\n",
    "# 5. Results \n",
    "# VGG-19 (Conv3_1) achieved ~51.6% FEV, outperforming LNP (16.3%) and GFB (45.6%).\n",
    "# The data-driven CNN performed similarly to VGG-19 but required more training data.\n",
    "# VGG-19's advantage: Achieved high performance with only 20% of the dataset, while the CNN needed the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions\n",
    "\n",
    "# My analysis gives very poor (- or close to zero) r-squared scores, so:\n",
    "\n",
    "# 1. Would using 1400 images instead of 7250 explain this poor score?\n",
    "# 2. Would using a pretrained VGG-19 mode, without Cadena's weights, explain this poor score?\n",
    "# 3. Would not copying Cadena's image preprocessing (downsampling and cropping) explain this poor score?\n",
    "# 4. Would not using their regularisation techniques explain this poor score (I don't think so, because they show that it lifts score from ~30% to ~50%)?\n",
    "\n",
    "# - What size images do they pass into the model? [224x224]\n",
    "# - They mention that only with at least 15% of their variance attributable to the stimulus were included - is this filtering already applied? [Yes, 166 neurons in data[\"responses\"]]\n",
    "# - They mention feature maps from VGG-19 were passed through batch normalization before being used for fitting - do I need to do this manually? [I think I need to do this manually]\n",
    "# - They use feature pooling - do I need to do this?\n",
    "# - They use feature maps directly, apply spatial pooling on all, then flatten. I was flattening features in batches and not applying spatial pooling. This may turn out to be important for the SimCLR analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/callummessiter/workspace/msc-neuro/research-project/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/callummessiter/workspace/msc-neuro/research-project/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images...\n",
      "Processed 400 images...\n",
      "Processed 800 images...\n",
      "Final extracted features shape: torch.Size([1000, 64, 224, 224])\n",
      "Features shape: (1000, 12544)\n",
      "Responses shape: (1000, 166)\n",
      "R-squared scores: [0.89060277 0.88540191 0.8963989  0.90245949 0.89787389 0.89113379\n",
      " 0.89793539 0.90631399 0.90327997 0.89530422 0.91103059 0.89458142\n",
      " 0.93634465 0.89253405 0.91119525 0.87338582 0.89048103 0.86483947\n",
      " 0.90539144 0.90791706 0.9125686  0.91423498 0.89813786 0.90189862\n",
      " 0.90640894 0.89071036 0.88935033 0.90857295 0.91338812 0.8940498\n",
      " 0.89509193 0.900233   0.91826891 0.90020132 0.89508512 0.91037615\n",
      " 0.90831182 0.90916178 0.90680847 0.88749172 0.89667413 0.90326387\n",
      " 0.90183524 0.90063514 0.88742066 0.87221846 0.92751308 0.91913752\n",
      " 0.90384837 0.91061083 0.92732379 0.86964886 0.91588777 0.90747767\n",
      " 0.89022524 0.90182161 0.91420553 0.90633035 0.89273091 0.91211604\n",
      " 0.9246586  0.9255471  0.89255537 0.89299036 0.89984535 0.91049699\n",
      " 0.85459994 0.910977   0.89242959 0.91369392 0.90713897 0.90256101\n",
      " 0.90316379 0.91282365 0.91953002 0.89451644 0.89968484 0.88773517\n",
      " 0.88608194 0.91990995 0.89856737 0.91194744 0.90069304 0.90086706\n",
      " 0.90220969 0.91115618 0.88749305 0.86941769 0.90370372 0.89836442\n",
      " 0.91067166 0.92187489 0.8915876  0.89599822 0.897353   0.90699518\n",
      " 0.89427162 0.87381808 0.8992654  0.90002033 0.89582875 0.9071471\n",
      " 0.91515237 0.90708772 0.90005646 0.92327752 0.92813569 0.89297013\n",
      " 0.88609508 0.94145891 0.87885571 0.91330258 0.90432226 0.91047271\n",
      " 0.90170102 0.90160698 0.91849613 0.91754916 0.90421936 0.92330908\n",
      " 0.90655966 0.91494383 0.94044455 0.90894465 0.88237162 0.9059815\n",
      " 0.89141451 0.93677325 0.91078868 0.89528766 0.89614284 0.90046512\n",
      " 0.91997573 0.91726289 0.92663468 0.9225214  0.94268337 0.92632877\n",
      " 0.91866668 0.89219031 0.90713852 0.93294846 0.92207941 0.88900594\n",
      " 0.91991055 0.90307932 0.92214637 0.9168844  0.94417289 0.91054038\n",
      " 0.90613585 0.90369705 0.90736419 0.91299589 0.92332496 0.91553169\n",
      " 0.92875018 0.91804258 0.92380073 0.92350881 0.88714025 0.91504521\n",
      " 0.89186959 0.92645046 0.92271476 0.9148526 ]\n",
      "Mean R-squared score: 0.9055388799686455\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "DATA_PATH = '../../data/cadena_plosCB19/'\n",
    "FILE = 'cadena_ploscb_data.pkl'\n",
    "\n",
    "# ===================================\n",
    "# Load Cadena's daata\n",
    "# ===================================\n",
    "def load_neural_data():\n",
    "    file_path = os.path.join(DATA_PATH, FILE)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "data_dict = load_neural_data()\n",
    "\n",
    "# ===================================\n",
    "# Clean neural data https://github.com/sacadena/Cadena2019PlosCB/blob/master/cnn_sys_ident/data.py\n",
    "# ===================================\n",
    "responses = data_dict['responses'].copy() \n",
    "responses[np.isnan(responses)] = 0\n",
    "data_dict['responses'] = responses\n",
    "\n",
    "# ===================================\n",
    "# Extract features from VGG-19\n",
    "# ===================================\n",
    "\n",
    "# Load VGG-19 and extract features only up to conv1_1\n",
    "vgg = models.vgg19(pretrained=True).features[:2].eval()\n",
    "\n",
    "# Move model to CPU (or GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg.to(device)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  # Resize for VGG-19\n",
    "            transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n",
    "            transforms.ToTensor(),  # Convert to PyTorch tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "\n",
    "        # Ensure img is in float32 format before converting to PIL\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = img.astype(np.float32)  # Explicitly cast to float32\n",
    "            img = Image.fromarray(img)  # Convert NumPy to PIL\n",
    "\n",
    "        return self.transform(img)  # Apply transforms\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "dataset = ImageDataset(data_dict[\"images\"][:1000])\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# Extract features in batches (to avoid RAM overload)\n",
    "all_features = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        features = vgg(batch)  # Extract Conv1_1 features\n",
    "\n",
    "        all_features.append(features.cpu())  # Keep shape (batch, C, H, W)\n",
    "\n",
    "        # Print progress every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i * len(batch)} images...\")\n",
    "\n",
    "# Concatenate all extracted features\n",
    "vgg_features = torch.cat(all_features, dim=0)\n",
    "print(\"Final extracted features shape:\", vgg_features.shape)  # Expected: (n_images, feature_dim)\n",
    "\n",
    "# ===================================\n",
    "# Apply spatial pooling to features, then flatten\n",
    "# NOTE: Cadena et al. use learned spatial pooling, not fixed pooling like adaptive_avg_pool2d.\n",
    "# NOTE: Their pooling is encouraged through trainable pooling weights and regularization​.\n",
    "# TODO: to replicate, might want to use a trainable depthwise convolution, instead of adaptive_avg_pool2d\n",
    "# TODO: they also apply batch normalisation BEFORE pooling https://github.com/sacadena/Cadena2019PlosCB/blob/master/cnn_sys_ident/vggsysid.py \n",
    "# vgg_feats_bn = tf.layers.batch_normalization(vgg_features, training = self.is_training, momentum = 0.9, epsilon = 1e-4, name='vgg_bn', fused =True)\n",
    "# ===================================\n",
    "\n",
    "# Apply spatial pooling before flattening\n",
    "if vgg_features.dim() == 2:  # Likely (n_images, flattened_dim)\n",
    "    raise ValueError(\"vgg_features is already flattened! Ensure it retains spatial dimensions.\")\n",
    "pooled_features = F.adaptive_avg_pool2d(vgg_features, (14, 14)) # Reduce spatial size\n",
    "X_pooled = pooled_features.view(pooled_features.size(0), -1).cpu().numpy() # Flatten pooled maps\n",
    "\n",
    "X = X_pooled\n",
    "Y = data_dict[\"responses\"].mean(axis=0)[:1000]\n",
    "\n",
    "# Features shape: (1000, 12544)\n",
    "# Responses shape: (1000, 166)\n",
    "print(\"Features shape:\", X_pooled.shape)\n",
    "print(\"Responses shape:\", Y.shape)\n",
    "\n",
    "# ===================================\n",
    "# Regression\n",
    "# ===================================\n",
    "\n",
    "ridge = Ridge(alpha=1.0) # Adjust alpha for regularization strength\n",
    "ridge.fit(X, Y) # Train on all neurons at once\n",
    "Y_pred = ridge.predict(X)\n",
    "r2 = r2_score(Y, Y_pred, multioutput='raw_values')\n",
    "\n",
    "print(\"R-squared scores:\", r2)\n",
    "print(\"Mean R-squared score:\", r2.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
