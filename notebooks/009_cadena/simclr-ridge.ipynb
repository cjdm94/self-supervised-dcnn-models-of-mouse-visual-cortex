{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "# - Ridge regression: 1000 first images, layer1, mean r2 = ~0.1, fev = ~0.25\n",
    "# - Ridge regression: 1000 first images, layer2, mean r2 = ~0.1, fev = ~0.25\n",
    "# - Ridge regression: 1000 first images, layer3, mean r2 = ~0.1, fev = ~0.25\n",
    "# - Ridge regression: 1000 first images, layer4, mean r2 = ~0.1, fev = ~0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run images through a pretrained SimCLR model and extract features\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict\n",
    "from torch.utils.data import Dataset\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Base ResNet18 backbone (pretrained=False, because we load custom weights later, from the SimCLR checkpoint file)\n",
    "        self.convnet = torchvision.models.resnet18(pretrained=False)\n",
    "        \n",
    "        # This is the projection head, only needed during training. For downstream tasks it is disposed of\n",
    "        # and the final linear layer output is used (Chen et al., 2020) \n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.intermediate_layers_to_capture =[]\n",
    "        self.intermediate_layer_features = {}\n",
    "        self.num_workers = os.cpu_count()\n",
    "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    def load_pretrained(self):\n",
    "        \"\"\"\n",
    "        Load pretrained SimCLR weights\n",
    "        \"\"\"\n",
    "        base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial17/\"\n",
    "        models_dir = \"../../models\"\n",
    "        pretrained_simclr_filename = \"SimCLR.ckpt\"\n",
    "        pretrained_simclr_path = os.path.join(models_dir, pretrained_simclr_filename)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Check whether the pretrained model file already exists locally. If not, try downloading it\n",
    "        file_url = base_url + pretrained_simclr_filename\n",
    "        if not os.path.isfile(pretrained_simclr_path):\n",
    "            print(f\"Downloading pretrained SimCLR model {file_url}...\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(file_url, pretrained_simclr_path)\n",
    "            except HTTPError as e:\n",
    "                print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
    "\n",
    "        print(f\"Already downloaded pretrained model: {file_url}\")\n",
    "\n",
    "        # Load pretrained model\n",
    "        checkpoint = torch.load(pretrained_simclr_path, map_location=self.device)\n",
    "        self.load_state_dict(checkpoint['state_dict'])\n",
    "        self.to(self.device)\n",
    "        self.eval()\n",
    "    \n",
    "    def set_intermediate_layers_to_capture(self, layers):\n",
    "        \"\"\"\n",
    "        Register hooks to capture features from intermediate layers\n",
    "        \"\"\"\n",
    "        # Just check the layers specified are actually in the convnet\n",
    "        top_level_block_layers = [name for name, _ in self.convnet.named_children()]\n",
    "        if not all(layer in top_level_block_layers for layer in layers):\n",
    "            print('You have specified convnet layers that are not top-level blocks - make sure your layer names are valid')\n",
    "        \n",
    "        self.intermediate_layers_to_capture = layers\n",
    "        intermediate_layer_features = {}\n",
    "\n",
    "        def get_hook(layer_name):\n",
    "            def hook(module, input, output):\n",
    "                intermediate_layer_features[layer_name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        for layer_name in layers:\n",
    "            layer = dict([*self.convnet.named_modules()])[layer_name]\n",
    "            layer.register_forward_hook(get_hook(layer_name))\n",
    "\n",
    "        self.intermediate_layer_features = intermediate_layer_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, dataset: Dataset) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run the pretrained SimCLR model on the image data, and capture features from final layer and intermediate layers.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): A PyTorch Dataset containing input images and labels. The image data should have shape (N, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: A dictionary containing:\n",
    "                - Intermediate layer features as tensors.\n",
    "                - Final layer features under 'final_layer'.\n",
    "                - Labels under 'labels'.\n",
    "            Features from a given layer has shape (N, F) where N is num images, F is number of features - flattened version of (C, H, W).\n",
    "        \"\"\"\n",
    "        self.convnet.fc = nn.Identity()  # Removing projection head g(.)\n",
    "        self.eval()\n",
    "        self.to(self.device)\n",
    "        \n",
    "        # Encode all images\n",
    "        data_loader = DataLoader(dataset, batch_size=64, num_workers=0, shuffle=False, drop_last=False)\n",
    "        feats, intermediate_features = [], {layer: [] for layer in self.intermediate_layers_to_capture}\n",
    "\n",
    "        for batch_idx, batch_imgs in enumerate(tqdm(data_loader)):\n",
    "            batch_imgs = batch_imgs.to(self.device)\n",
    "            batch_feats = self.convnet(batch_imgs)\n",
    "            \n",
    "            feats.append(batch_feats.detach().cpu())\n",
    "\n",
    "            # Collect intermediate layer outputs\n",
    "            for layer in self.intermediate_layers_to_capture:\n",
    "                # Final linear layer outputs a 2d tensor; but intermediate layers don't, so we flatten them (ready for PCA etc.)\n",
    "                # layer_output_flattened = self.intermediate_layer_features[layer].view(self.intermediate_layer_features[layer].size(0), -1)\n",
    "                # # Apply spatial pooling before flattening\n",
    "                pooled_features = F.adaptive_avg_pool2d(self.intermediate_layer_features[layer], (14, 14)) # Reduce spatial size\n",
    "                layer_output_flattened = pooled_features.view(pooled_features.size(0), -1) # Flatten after pooling \n",
    "                intermediate_features[layer].append(layer_output_flattened.cpu())\n",
    "        \n",
    "        # Concatenate results for each layer\n",
    "        feats = torch.cat(feats, dim=0)\n",
    "        intermediate_features = {layer: torch.cat(intermediate_features[layer], dim=0) for layer in self.intermediate_layers_to_capture}\n",
    "\n",
    "        # Debugging log after concatenation\n",
    "        print(\"âœ… Feature extraction complete. Final feature shapes:\")\n",
    "        print(f\"Final layer: {feats.shape}\")\n",
    "        for layer, feature in intermediate_features.items():\n",
    "            print(f\"{layer}: {feature.shape}\")  # Check final stored shape\n",
    "\n",
    "        return {**intermediate_features, 'final_layer': feats}\n",
    "\n",
    "intermediate_layers = ['layer1', 'layer2', 'layer3', 'layer4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "DATA_PATH = '../../data/cadena_plosCB19/'\n",
    "FILE = 'cadena_ploscb_data.pkl'\n",
    "\n",
    "# ===================================\n",
    "# Load Cadena's daata\n",
    "# ===================================\n",
    "def load_neural_data():\n",
    "    file_path = os.path.join(DATA_PATH, FILE)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "data_dict = load_neural_data()\n",
    "\n",
    "# ===================================\n",
    "# Clean neural data https://github.com/sacadena/Cadena2019PlosCB/blob/master/cnn_sys_ident/data.py\n",
    "# ===================================\n",
    "responses = data_dict['responses'].copy() \n",
    "responses[np.isnan(responses)] = 0\n",
    "data_dict['responses'] = responses\n",
    "\n",
    "# ===================================\n",
    "# Get sample images and responses\n",
    "# ===================================\n",
    "# np.random.seed(42)\n",
    "# indices = np.random.choice(7250, 1000, replace=False)\n",
    "sample_images = data_dict[\"images\"][:1000]\n",
    "sample_responses = data_dict[\"responses\"][:, :1000, :]\n",
    "\n",
    "# ===================================\n",
    "# Extract features from VGG-19\n",
    "# ===================================\n",
    "\n",
    "# Load VGG-19 and extract features only up to conv1_1\n",
    "vgg = models.vgg19(pretrained=True).features[:10].eval()\n",
    "\n",
    "# Move model to CPU (or GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg.to(device)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        self.transform = transforms.Compose([\n",
    "            # transforms.CenterCrop(80),\n",
    "            transforms.Resize((224, 224)),  # Resize for VGG-19\n",
    "            transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n",
    "            transforms.ToTensor(),  # Convert to PyTorch tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "\n",
    "        # Ensure img is in float32 format before converting to PIL\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = img.astype(np.float32)  # Explicitly cast to float32\n",
    "            img = Image.fromarray(img)  # Convert NumPy to PIL\n",
    "\n",
    "        return self.transform(img)  # Apply transforms\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "dataset = ImageDataset(sample_images)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "# ===================================\n",
    "# Extract SIMCLR features\n",
    "# ===================================\n",
    "sim_clr = SimCLR()\n",
    "sim_clr.load_pretrained()\n",
    "sim_clr.set_intermediate_layers_to_capture(intermediate_layers)\n",
    "feats = sim_clr.extract_features(dataset)\n",
    "\n",
    "for layer in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    if layer in feats:\n",
    "        variance = np.var(feats[layer].numpy())\n",
    "        print(f\"{layer} variance: {variance:.6f}\")\n",
    "\n",
    "layer1_feats = feats['layer1'] # Shape: torch.Size([1573, 200704]) (n_images, n_features)\n",
    "layer2_feats = feats['layer2']\n",
    "layer3_feats = feats['layer3']\n",
    "layer4_feats = feats['layer4']\n",
    "final_layer_feats = feats['final_layer'] # Shape: torch.Size([1573, 512])\n",
    "\n",
    "print('layer1 shape', layer1_feats.shape)\n",
    "print('final layer shape', final_layer_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Regression\n",
    "# ===================================\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Y = sample_responses.mean(axis=0)\n",
    "\n",
    "# Data for regression\n",
    "for x in [layer1_feats, layer2_feats, layer3_feats, layer4_feats, final_layer_feats]:\n",
    "    X = x\n",
    "\n",
    "    print(\"Features shape:\", X.shape)\n",
    "    print(\"Responses shape:\", Y.shape)\n",
    "\n",
    "    ridge = Ridge(alpha=1000) # Adjust alpha for regularization strength\n",
    "    ridge.fit(X, Y) # Train on all neurons at once\n",
    "    Y_pred = ridge.predict(X)\n",
    "    r2 = r2_score(Y, Y_pred, multioutput='raw_values')\n",
    "\n",
    "    print(\"Mean R-squared score:\", r2.mean())\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    ridge.fit(X_train, Y_train)  # Train on 80%\n",
    "    Y_pred_test = ridge.predict(X_test)  # Predict on 20%\n",
    "\n",
    "    r2_test = r2_score(Y_test, Y_pred_test, multioutput='raw_values')\n",
    "    print(f\"Mean RÂ² score on test set: {np.mean(r2_test):.4f}\")  # Should be lower than training RÂ²\n",
    "\n",
    "    # (FEV) NOTE: we use only first 1000 images and responses for regression, so we adjust variance calculations accordingly\n",
    "    Y_subset = Y[:1000]\n",
    "    total_var = np.var(Y_subset, axis=0)\n",
    "    trial_var = np.var(data_dict[\"responses\"][:, :1000, :], axis=0)  # Variance across trials\n",
    "    noise_var = np.mean(trial_var, axis=0)  # Average across images\n",
    "    mse = mean_squared_error(Y_test, Y_pred_test, multioutput='raw_values')\n",
    "    explainable_var = total_var - noise_var\n",
    "    fev = 1 - (mse - noise_var) / explainable_var\n",
    "    fev = np.clip(fev, 0, 1)\n",
    "    print(f\"Mean FEV across neurons (subset of 1000 images): {np.mean(fev):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
