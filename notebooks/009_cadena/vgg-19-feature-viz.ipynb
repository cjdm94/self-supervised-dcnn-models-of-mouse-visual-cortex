{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.nn as nn\n",
    "\n",
    "DATA_PATH = '../../data/cadena_plosCB19/'\n",
    "FILE = 'cadena_ploscb_data.pkl'\n",
    "\n",
    "# ===================================\n",
    "# Load Cadena's daata\n",
    "# ===================================\n",
    "def load_neural_data():\n",
    "    file_path = os.path.join(DATA_PATH, FILE)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "data_dict = load_neural_data()\n",
    "\n",
    "# ===================================\n",
    "# Clean neural data https://github.com/sacadena/Cadena2019PlosCB/blob/master/cnn_sys_ident/data.py\n",
    "# ===================================\n",
    "responses = data_dict['responses'].copy() \n",
    "responses[np.isnan(responses)] = 0\n",
    "data_dict['responses'] = responses\n",
    "\n",
    "# ===================================\n",
    "# Get sample images and responses\n",
    "# ===================================\n",
    "# np.random.seed(42)\n",
    "# indices = np.random.choice(7250, 1000, replace=False)\n",
    "sample_images = data_dict[\"images\"][:1000]\n",
    "sample_responses = data_dict[\"responses\"][:, :1000, :]\n",
    "\n",
    "# ===================================\n",
    "# Extract features from VGG-19\n",
    "# ===================================\n",
    "\n",
    "# Load VGG-19 and extract features only up to conv1_1\n",
    "vgg = models.vgg19(pretrained=True).features[:10].eval()\n",
    "\n",
    "# Move model to CPU (or GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg.to(device)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.CenterCrop(80),\n",
    "            transforms.Resize((224, 224)),  # Resize for VGG-19\n",
    "            transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n",
    "            transforms.ToTensor(),  # Convert to PyTorch tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "\n",
    "        # Ensure img is in float32 format before converting to PIL\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = img.astype(np.float32)  # Explicitly cast to float32\n",
    "            img = Image.fromarray(img)  # Convert NumPy to PIL\n",
    "\n",
    "        return self.transform(img)  # Apply transforms\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "dataset = ImageDataset(sample_images)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "# Extract features in batches (to avoid RAM overload)\n",
    "all_features = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        features = vgg(batch)  # Extract Conv1_1 features\n",
    "\n",
    "        all_features.append(features.cpu())  # Keep shape (batch, C, H, W)\n",
    "\n",
    "        # Print progress every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i * len(batch)} images...\")\n",
    "\n",
    "# Concatenate all extracted features\n",
    "vgg_features = torch.cat(all_features, dim=0)\n",
    "print(\"Final extracted features shape:\", vgg_features.shape)  # Expected: (n_images, feature_dim)\n",
    "\n",
    "# ===================================\n",
    "# Apply spatial pooling to features, then flatten\n",
    "# NOTE: Cadena et al. use learned spatial pooling, not fixed pooling like adaptive_avg_pool2d.\n",
    "# NOTE: Their pooling is encouraged through trainable pooling weights and regularization​.\n",
    "# TODO: to replicate, might want to use a trainable depthwise convolution, instead of adaptive_avg_pool2d\n",
    "# ===================================\n",
    "\n",
    "# Apply batch normalisation before pooling\n",
    "bn_layer = nn.BatchNorm2d(num_features=vgg_features.shape[1], momentum=0.9).to(device)\n",
    "bn_layer.eval()  \n",
    "vgg_features_bn = bn_layer(vgg_features)\n",
    "\n",
    "# Apply spatial pooling before flattening\n",
    "if vgg_features_bn.dim() == 2:  # Likely (n_images, flattened_dim)\n",
    "    raise ValueError(\"vgg_features is already flattened! Ensure it retains spatial dimensions.\")\n",
    "pooled_features = F.adaptive_avg_pool2d(vgg_features_bn, (14, 14)) # Reduce spatial size\n",
    "X_pooled = pooled_features.view(pooled_features.size(0), -1).detach().cpu().numpy() # Flatten pooled maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Regression\n",
    "# ===================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# NOTE: we use only first 1000 images and responses for regression, so we adjust test-train-split, and variance calculations accordingly\n",
    "\n",
    "X = X_pooled\n",
    "Y = sample_responses.mean(axis=0)\n",
    "\n",
    "print(\"Features shape:\", X_pooled.shape)\n",
    "print(\"Responses shape:\", Y.shape)\n",
    "\n",
    "ridge = Ridge(alpha=10000) # Adjust alpha for regularization strength\n",
    "ridge.fit(X, Y) # Train on all neurons at once\n",
    "Y_pred = ridge.predict(X)\n",
    "r2 = r2_score(Y, Y_pred, multioutput='raw_values')\n",
    "\n",
    "print(\"Mean R-squared score:\", r2.mean())\n",
    "\n",
    "# Generate indices for the entire dataset\n",
    "num_samples = X.shape[0]\n",
    "all_indices = np.arange(num_samples)\n",
    "\n",
    "# Split into train/test while keeping the indices\n",
    "train_indices, test_indices = train_test_split(\n",
    "    all_indices, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(train_indices[:10])\n",
    "print(test_indices[:10])\n",
    "\n",
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "Y_train, Y_test = Y[train_indices], Y[test_indices]\n",
    "\n",
    "ridge.fit(X_train, Y_train)  # Train on 80%\n",
    "Y_pred_test = ridge.predict(X_test)  # Predict on 20%\n",
    "\n",
    "r2_test = r2_score(Y_test, Y_pred_test, multioutput='raw_values')\n",
    "print(f\"Mean R² score on test set: {np.mean(r2_test):.4f}\")  # Should be lower than training R²\n",
    "\n",
    "# (FEV)\n",
    "total_var_test = np.var(Y_test, axis=0)\n",
    "trial_var_test = np.var(sample_responses[:, test_indices, :], axis=0)  # Variance across trials\n",
    "noise_var_test = np.mean(trial_var_test, axis=0)  # Average across images\n",
    "mse_test = mean_squared_error(Y_test, Y_pred_test, multioutput='raw_values')\n",
    "explainable_var_test = total_var_test - noise_var_test\n",
    "fev = 1 - (mse_test - noise_var_test) / explainable_var_test\n",
    "fev = np.clip(fev, 0, 1)\n",
    "print(f\"Mean FEV across neurons (subset of 1000 images): {np.mean(fev):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature visualisation - conv layers\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load pre-trained VGG-19 model (first convolutional layer)\n",
    "vgg = models.vgg19(pretrained=True).features[:33].eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg.to(device)\n",
    "\n",
    "# Function to generate a synthetic image that maximizes a given filter\n",
    "def generate_maximally_activating_image(filter_idx, steps=100, lr=0.1):\n",
    "    # Start with random noise image\n",
    "    img = torch.randn(1, 3, 224, 224, device=device, requires_grad=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([img], lr=lr)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through VGG-19\n",
    "        features = vgg(img)\n",
    "        \n",
    "        # Target specific filter's activation\n",
    "        activation = features[0, filter_idx].mean()  # Mean activation of filter\n",
    "        \n",
    "        # Gradient ascent\n",
    "        loss = -activation  # Negate to maximize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Normalize image to keep it visually stable\n",
    "        with torch.no_grad():\n",
    "            img.clamp_(-1.5, 1.5)  # Prevent extreme values\n",
    "    \n",
    "    return img.detach()\n",
    "\n",
    "# Function to visualize the optimized image\n",
    "def visualize_feature_map(img):\n",
    "    \"\"\"\n",
    "    Displays the feature visualization as a grayscale image.\n",
    "    \"\"\"\n",
    "    img = img.squeeze().cpu().detach().numpy()  # Remove singleton dimensions\n",
    "\n",
    "    if img.ndim == 3:  # (1, H, W) -> (H, W)\n",
    "        img = img[0]\n",
    "\n",
    "    img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0,1]\n",
    "\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Generate an image that maximally activates filter 10\n",
    "optimized_img = generate_maximally_activating_image(filter_idx=10)\n",
    "\n",
    "# Convert to grayscale and visualize\n",
    "grayscale_img = optimized_img.mean(dim=1, keepdim=True)  # Average RGB channels\n",
    "visualize_feature_map(grayscale_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature visualisation - fully-connected layers\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load VGG-19\n",
    "vgg = models.vgg19(pretrained=True).eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to generate an image that maximally activates an `fc6` neuron\n",
    "def generate_fc6_maximization(neuron_idx, steps=200, lr=0.1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Start with random noise image\n",
    "    img = torch.randn(1, 3, 224, 224, device=device, requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([img], lr=lr)\n",
    "\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through convolutional layers and pool5\n",
    "        features = vgg.features(img)  # Get conv5_4 output\n",
    "        features = vgg.avgpool(features)  # Pooling before FC layers\n",
    "        features = torch.flatten(features, start_dim=1)  # Flatten for FC layer\n",
    "\n",
    "        # Pass through FC layers up to fc6\n",
    "        fc6_output = vgg.classifier[:1](features)  # Only extracting fc6\n",
    "\n",
    "        # Maximize activation of selected neuron in `fc6`\n",
    "        activation = fc6_output[0, neuron_idx]  # Select specific neuron\n",
    "        loss = -activation  # Gradient ascent\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Regularization: Normalize image to avoid extreme values\n",
    "        with torch.no_grad():\n",
    "            img.clamp_(-1.5, 1.5)\n",
    "\n",
    "    return img.detach()\n",
    "\n",
    "# Function to preprocess and visualize the image\n",
    "def visualize_optimized_image(img):\n",
    "    img = img.squeeze().cpu().detach().numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))  # Convert to (H, W, C)\n",
    "    img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0,1]\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Choose a neuron in `fc6` to visualize (e.g., 1000th neuron out of 4096)\n",
    "neuron_idx = 500\n",
    "optimized_img = generate_fc6_maximization(neuron_idx)\n",
    "\n",
    "# Show the maximally activating image for `fc6`\n",
    "visualize_optimized_image(optimized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature visualisation - class-conditioned\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained VGG-19\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = models.vgg19(pretrained=True).eval().to(device)\n",
    "\n",
    "def total_variation_loss(img):\n",
    "    \"\"\"\n",
    "    Computes Total Variation (TV) loss to encourage smoothness in the generated image.\n",
    "    \"\"\"\n",
    "    loss = torch.sum(torch.abs(img[:, :, :-1] - img[:, :, 1:])) + torch.sum(torch.abs(img[:, :-1, :] - img[:, 1:, :]))\n",
    "    return loss\n",
    "\n",
    "def generate_class_visualization(target_class, steps=300, lr=0.1, tv_weight=1e-6):\n",
    "    \"\"\"\n",
    "    Generates an image that maximally activates a specific class in VGG-19 while applying\n",
    "    Total Variation (TV) regularization for smoother results.\n",
    "    \"\"\"\n",
    "    img = torch.randn(1, 3, 224, 224, device=device, requires_grad=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([img], lr=lr)\n",
    "\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        features = vgg.features(img)\n",
    "        features = vgg.avgpool(features)\n",
    "        features = torch.flatten(features, start_dim=1)\n",
    "        logits = vgg.classifier(features)  # Get class scores\n",
    "\n",
    "        # Maximize target class activation\n",
    "        loss = -logits[0, target_class]  \n",
    "\n",
    "        # Add Total Variation (TV) loss for smoothness\n",
    "        loss += tv_weight * total_variation_loss(img)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Regularization: Clip pixel values\n",
    "        with torch.no_grad():\n",
    "            img.clamp_(-1.5, 1.5)\n",
    "\n",
    "    return img.detach()\n",
    "\n",
    "def preprocess_and_visualize(img):\n",
    "    \"\"\"\n",
    "    Preprocesses and displays the optimized image.\n",
    "    \"\"\"\n",
    "    img = img.squeeze().cpu().detach().numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))  # Convert to HWC\n",
    "    img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0,1]\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Target Class: Golden Retriever (ImageNet ID: 207)\n",
    "target_class = 207  \n",
    "optimized_img = generate_class_visualization(target_class, tv_weight=1e-5)\n",
    "\n",
    "# Show improved visualization\n",
    "preprocess_and_visualize(optimized_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
