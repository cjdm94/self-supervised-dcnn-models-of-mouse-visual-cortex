{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06041de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581895e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "\n",
    "# imresps.npy is of shape (1573, 2, 15363), where 1573 is number of images, 2 repeats each, and 15363 neurons recorded\n",
    "#Â stimids.npy has the image id (matching the image dataset ~selection1866~) for each stimulus number, \n",
    "# so of you want to see what image was presented on imresps[502] you would check stim_ids[502]\n",
    "\n",
    "PATH_TO_DATA = '../../data/neural'\n",
    "\n",
    "imresps = np.load(path.join(PATH_TO_DATA, 'imresps.npy'))\n",
    "stimids = np.load(path.join(PATH_TO_DATA, 'stimids.npy'))\n",
    "\n",
    "print(imresps.shape) # (1573, 2, 15363)\n",
    "print(stimids.shape) # (1573,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_signal_related_variance(resp_a, resp_b, mean_center=True):\n",
    "    \"\"\"\n",
    "    compute the fraction of signal-related variance for each neuron,\n",
    "    as per Stringer et al Nature 2019. Cross-validated by splitting\n",
    "    responses into two halves. Note, this only is \"correct\" if resp_a\n",
    "    and resp_b are *not* averages of many trials.\n",
    "\n",
    "    Args:\n",
    "        resp_a (ndarray): n_stimuli, n_cells\n",
    "        resp_b (ndarray): n_stimuli, n_cells\n",
    "\n",
    "    Returns:\n",
    "        fraction_of_stimulus_variance: 0-1, 0 is non-stimulus-caring, 1 is only-stimulus-caring neurons\n",
    "        stim_to_noise_ratio: ratio of the stim-related variance to all other variance\n",
    "    \"\"\"\n",
    "    if len(resp_a.shape) > 2:\n",
    "        # if the stimulus is multi-dimensional, flatten across all stimuli\n",
    "        resp_a = resp_a.reshape(-1, resp_a.shape[-1])\n",
    "        resp_b = resp_b.reshape(-1, resp_b.shape[-1])\n",
    "    ns, nc = resp_a.shape\n",
    "    if mean_center:\n",
    "        # mean-center the activity of each cell\n",
    "        resp_a = resp_a - resp_a.mean(axis=0)\n",
    "        resp_b = resp_b - resp_b.mean(axis=0)\n",
    "    \n",
    "    # compute the cross-trial stimulus covariance of each cell\n",
    "    # dot-product each cell's (n_stim, ) vector from one half\n",
    "    # with its own (n_stim, ) vector on the other half\n",
    "\n",
    "    covariance = (resp_a * resp_b).sum(axis=0) / ns\n",
    "\n",
    "    # compute the variance of each cell across both halves\n",
    "    resp_a_variance = (resp_a**2).sum(axis=0) / ns\n",
    "    resp_b_variance = (resp_b**2).sum(axis=0) / ns\n",
    "    total_variance = (resp_a_variance + resp_b_variance) / 2\n",
    "\n",
    "    if np.any(total_variance < 1e-12):\n",
    "        print(f\"Warning: Near-zero total variance for neurons: {np.where(total_variance < 1e-12)[0]}\")\n",
    "\n",
    "    # compute the fraction of the total variance that is\n",
    "    # captured in the covariance\n",
    "    fraction_of_stimulus_variance = covariance / total_variance\n",
    "\n",
    "    # if you want, you can compute SNR as well:\n",
    "    stim_to_noise_ratio = fraction_of_stimulus_variance / (\n",
    "        1 - fraction_of_stimulus_variance\n",
    "    )\n",
    "\n",
    "    return fraction_of_stimulus_variance, stim_to_noise_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the null distribution of SRV values for all neurons\n",
    "\n",
    "# imresps shape = (1573, 2, 15363)\n",
    "# responses in imresps shape = (2, 15363)\n",
    "num_stimuli = imresps.shape[0] # 1573\n",
    "num_repeats = imresps.shape[1] # 2\n",
    "num_neurons = imresps.shape[2] # 15363\n",
    "n_shuffles = 100\n",
    "\n",
    "null_srv_all_neurons = [] # shape (n_shuffles, num_neurons)\n",
    "\n",
    "for _ in range(n_shuffles):\n",
    "    # Shuffle stimulus indices *twice* to create two independent splits!\n",
    "    shuffled_indices_A = np.random.permutation(num_stimuli)\n",
    "    shuffled_indices_B = np.random.permutation(num_stimuli)\n",
    "\n",
    "    # Now for the splits, we can just use fixed repeat indices, \n",
    "    # because for each split, at index N the responses correspond to different stimuli\n",
    "    # e.g. split_A = [ stim_100_repeat_1, stim_2_repeat_1, stim_19_repeat_1, ... ]\n",
    "    # e.g. split_B = [ stim_543_repeat_2, stim_345_repeat_2, stim_3_repeat_2, ... ]\n",
    "    split_A = imresps[shuffled_indices_A, 0, :]\n",
    "    split_B = imresps[shuffled_indices_B, 1, :]\n",
    "\n",
    "    # Compute SRV for the shuffled data\n",
    "    fraction_of_stimulus_variance, _ = compute_signal_related_variance(split_A, split_B)\n",
    "    null_srv_all_neurons.append(fraction_of_stimulus_variance)\n",
    "\n",
    "null_srv_all_neurons = np.array(null_srv_all_neurons)\n",
    "null_srv_all_neurons.shape # (100, 15363)\n",
    "\n",
    "print(null_srv_all_neurons[0])\n",
    "print(null_srv_all_neurons[33])\n",
    "\n",
    "# e.g. if neuron_index = 0, it will plot the SRV value for neuron 0 across all shuffles\n",
    "neuron_index = 0\n",
    "plt.hist([srv[neuron_index] for srv in null_srv_all_neurons], bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Shuffles\")\n",
    "plt.title(f\"Null Distribution of SRV for Neuron {neuron_index}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252632db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the real SRV for each neuron\n",
    "\n",
    "# split_A_real = imresps[:, 0, :] # First repeat for each stimulus\n",
    "# split_B_real = imresps[:, 1, :] # Second repeat for each stimulus\n",
    "\n",
    "split_A, split_B = [], []\n",
    "for responses in imresps: # responses shape: (2, n_neurons)\n",
    "    indices = np.random.permutation(2) # Randomly shuffle [0, 1]\n",
    "    split_A.append(responses[indices[0]]) # Assign one repeat to split_A\n",
    "    split_B.append(responses[indices[1]]) # Assign the other to split_B\n",
    "\n",
    "split_A = np.array(split_A)  # Shape: (n_stimuli, n_neurons)\n",
    "split_B = np.array(split_B)  # Shape: (n_stimuli, n_neurons)\n",
    "\n",
    "# Compute SRV for real data\n",
    "real_srv_all_neurons, stim_to_noise_ratio = compute_signal_related_variance(split_A, split_B)\n",
    "\n",
    "print(real_srv_all_neurons)\n",
    "print(stim_to_noise_ratio)\n",
    "\n",
    "print(\"Real SRV shape:\", real_srv_all_neurons.shape) # Should be (15363,)\n",
    "\n",
    "plt.hist(real_srv_all_neurons, bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Shuffles\")\n",
    "plt.title(f\"Null Distribution of SRV for Neuron {neuron_index}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a080d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter neurons whose real SRV is in the top 90th percentile of its null distribution\n",
    "\n",
    "# This gives the 90th-percentile SRV value of the null distribution for each neuron\n",
    "# In other words the threshold for each neuron to be considered reliable\n",
    "# e.g. if neuron 0 has a null distribution of SRVs across 10 shuffles \n",
    "# [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], the threshold would be 0.9\n",
    "top_99th_percentile_null = np.percentile(null_srv_all_neurons, 99, axis=0)\n",
    "print(top_99th_percentile_null) # [0.03651716 0.03126347 0.03325775 ... 0.02738261 0.03546677 0.0333109 ]\n",
    "\n",
    "# Get indices of reliable neurons\n",
    "reliable_neuron_indices = np.where(real_srv_all_neurons >= top_99th_percentile_null)[0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of reliable neurons: {len(reliable_neuron_indices)}\") # 5654\n",
    "print(f\"Indices of reliable neurons: {reliable_neuron_indices}\") # [   14    29    48 ... 15357 15358 15360]\n",
    "\n",
    "plt.hist(real_srv_all_neurons, bins=100, color='red', alpha=0.7)\n",
    "plt.hist(real_srv_all_neurons[reliable_neuron_indices], bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Shuffles\")\n",
    "plt.title(\"All Neurons: SRV all vs. SRV reliable\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(real_srv_all_neurons[reliable_neuron_indices], bins=100, color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Fraction of Stimulus-Related Variance (SRV)\")\n",
    "plt.ylabel(\"Number of Neurons\")\n",
    "plt.title(\"SRV Distribution for Reliable Neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter only the top Y neurons (SRV)\n",
    "\n",
    "num_neurons = 500\n",
    "\n",
    "reliable_srv_scores = real_srv_all_neurons[reliable_neuron_indices]\n",
    "sorted_indices = np.argsort(reliable_srv_scores)[::-1]\n",
    "most_reliable_neurons = reliable_neuron_indices[sorted_indices[:num_neurons]]\n",
    "highest_srv_scores = real_srv_all_neurons[most_reliable_neurons]\n",
    "neural_responses = imresps[:, :, most_reliable_neurons]\n",
    "neural_responses_mean = neural_responses.mean(axis=1)\n",
    "\n",
    "assert most_reliable_neurons.shape[0] == num_neurons, \"Mismatch in neuron selection!\"\n",
    "print(\"Dimensionality of neural responses:\", neural_responses_mean.shape)\n",
    "print(\"Top 500 reliable neuron indices:\", most_reliable_neurons[:10])\n",
    "print(\"Corresponding SRV scores:\", highest_srv_scores[:10])\n",
    "print(\"Top 500 neural responses shape:\", neural_responses.shape) # (1573, 2, 500)\n",
    "print(\"Averaged top 500 neural responses shape:\", neural_responses_mean.shape) # (1573, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef994538",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get first PC of neural data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(100)\n",
    "neural_data_pcs = pca.fit_transform(neural_responses_mean)\n",
    "pc1_neural_data = neural_data_pcs[:, 0]\n",
    "pc2_neural_data = neural_data_pcs[:, 1]\n",
    "pc3_neural_data = neural_data_pcs[:, 2]\n",
    "pc4_neural_data = neural_data_pcs[:, 3]\n",
    "\n",
    "print(pc1_neural_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and preprocess images\n",
    "\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import Normalize, Compose, Resize, CenterCrop\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import utils as torch_utils\n",
    " \n",
    "PATH_TO_DATA = '../../data/selection1866'\n",
    "\n",
    "file_list = sorted(f for f in os.listdir(PATH_TO_DATA) if f.endswith('.mat'))\n",
    "stim_ids = stimids.astype(int)\n",
    "\n",
    "print(stim_ids)\n",
    "print(stimids)\n",
    "\n",
    "transform = Compose([\n",
    "    Resize(96), # Resize shortest edge to 96 (cut off the rightmost part of the image)\n",
    "    CenterCrop((96, 96)), # Crop to (96, 96)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # !! Normalize expects input is already in the range [0, 1]\n",
    "])\n",
    "\n",
    "img_tensors, labels = [], []\n",
    "\n",
    "print('List:', file_list)\n",
    "\n",
    "# we have 1866 images here, but the neural response data only uses 1573 of them\n",
    "# because some ~300 images didn't have two repeats, so were disposed\n",
    "# therefore we filter the full set here so that we only use the relevant 1573\n",
    "for stim_id in stim_ids:\n",
    "    filename = 'img' + str(stim_id) + '.mat'\n",
    "    data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "\n",
    "    img = data['img'][:, :500] # Take leftmost part of the image\n",
    "    rgb_img = np.stack([img] * 3, axis=-1) # Convert grayscale to RGB for SimCLR\n",
    "    tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "    \n",
    "    # Min-max scale the tensor to [0, 1]\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Clamp to [0, 1] to ensure no outliers due to numerical precision\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)\n",
    "\n",
    "    transformed_tensor = transform(tensor) # Normalize and resize for SimCLR\n",
    "    img_tensors.append(transformed_tensor)\n",
    "    labels.append(stim_id)\n",
    "\n",
    "image_dataset = TensorDataset(torch.stack(img_tensors), torch.tensor(labels))\n",
    "\n",
    "images, labels = image_dataset.tensors\n",
    "print(\"Processed image labels (stim id):\", labels[:30])\n",
    "print(\"Stim IDs from neural data:\", stim_ids[:30])\n",
    "print(\"Processed dataset shape:\", images.shape) # (N, C, 96, 96)\n",
    "print(f\"Min pixel value (processed): {torch.min(images)}\")\n",
    "print(f\"Max pixel value (processed): {torch.max(images)}\")\n",
    "\n",
    "# Show a sample of processed images\n",
    "img_grid = torch_utils.make_grid(images[:12], nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Processed images: sample')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "filename = 'img20.mat'\n",
    "data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "img = data['img'][:, :500]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img, cmap='gray')  # Adjust cmap as needed ('viridis', 'jet', etc.)\n",
    "plt.colorbar(label=\"Pixel Intensity\")\n",
    "plt.title(\"Rendered Image\")\n",
    "plt.axis(\"off\")  # Hide axis for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run images through a pretrained SimCLR model and extract features\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict\n",
    "from torch.utils.data import Dataset\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "from collections import defaultdict\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Base ResNet18 backbone (pretrained=False, because we load custom weights later, from the SimCLR checkpoint file)\n",
    "        self.convnet = torchvision.models.resnet18(pretrained=False)\n",
    "        \n",
    "        # This is the projection head, only needed during training. For downstream tasks it is disposed of\n",
    "        # and the final linear layer output is used (Chen et al., 2020) \n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.intermediate_layers_to_capture =[]\n",
    "        self.intermediate_layer_features = {}\n",
    "        self.num_workers = os.cpu_count()\n",
    "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    def load_pretrained(self):\n",
    "        \"\"\"\n",
    "        Load pretrained SimCLR weights\n",
    "        \"\"\"\n",
    "        base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial17/\"\n",
    "        models_dir = \"../../models\"\n",
    "        pretrained_simclr_filename = \"SimCLR.ckpt\"\n",
    "        pretrained_simclr_path = os.path.join(models_dir, pretrained_simclr_filename)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Check whether the pretrained model file already exists locally. If not, try downloading it\n",
    "        file_url = base_url + pretrained_simclr_filename\n",
    "        if not os.path.isfile(pretrained_simclr_path):\n",
    "            print(f\"Downloading pretrained SimCLR model {file_url}...\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(file_url, pretrained_simclr_path)\n",
    "            except HTTPError as e:\n",
    "                print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
    "\n",
    "        print(f\"Already downloaded pretrained model: {file_url}\")\n",
    "\n",
    "        # Load pretrained model\n",
    "        checkpoint = torch.load(pretrained_simclr_path, map_location=self.device)\n",
    "        self.load_state_dict(checkpoint['state_dict'])\n",
    "        self.to(self.device)\n",
    "        self.eval()\n",
    "    \n",
    "    def set_intermediate_layers_to_capture(self, layers):\n",
    "        \"\"\"\n",
    "        Register hooks to capture features from intermediate layers\n",
    "        \"\"\"\n",
    "        # Just check the layers specified are actually in the convnet\n",
    "        top_level_block_layers = [name for name, _ in self.convnet.named_children()]\n",
    "        if not all(layer in top_level_block_layers for layer in layers):\n",
    "            print('You have specified convnet layers that are not top-level blocks - make sure your layer names are valid')\n",
    "        \n",
    "        self.intermediate_layers_to_capture = layers\n",
    "        intermediate_layer_features = {}\n",
    "\n",
    "        def get_hook(layer_name):\n",
    "            def hook(module, input, output):\n",
    "                intermediate_layer_features[layer_name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        for layer_name in layers:\n",
    "            layer = dict([*self.convnet.named_modules()])[layer_name]\n",
    "            layer.register_forward_hook(get_hook(layer_name))\n",
    "\n",
    "        self.intermediate_layer_features = intermediate_layer_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, dataset: Dataset) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run the pretrained SimCLR model on the image data, and capture features from final layer and intermediate layers.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): A PyTorch Dataset containing input images and labels. The image data should have shape (N, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: A dictionary containing:\n",
    "                - Intermediate layer features as tensors.\n",
    "                - Final layer features under 'final_layer'.\n",
    "                - Labels under 'labels'.\n",
    "            Features from a given layer has shape (N, F) where N is num images, F is number of features - flattened version of (C, H, W).\n",
    "        \"\"\"\n",
    "        self.convnet.fc = nn.Identity()  # Removing projection head g(.)\n",
    "        self.eval()\n",
    "        self.to(self.device)\n",
    "        \n",
    "        # Encode all images\n",
    "        data_loader = DataLoader(dataset, batch_size=64, num_workers=self.num_workers, shuffle=False, drop_last=False)\n",
    "        feats, labels, intermediate_features = [], [], {layer: [] for layer in self.intermediate_layers_to_capture}\n",
    "\n",
    "        for batch_idx, (batch_imgs, batch_labels) in enumerate(tqdm(data_loader)):\n",
    "            batch_imgs = batch_imgs.to(self.device)\n",
    "            batch_feats = self.convnet(batch_imgs)\n",
    "            \n",
    "            feats.append(batch_feats.detach().cpu())\n",
    "            labels.append(batch_labels)\n",
    "\n",
    "            # Collect intermediate layer outputs\n",
    "            for layer in self.intermediate_layers_to_capture:\n",
    "                # Final linear layer outputs a 2d tensor; but intermediate layers don't, so we flatten them (ready for PCA etc.)\n",
    "                # layer_output_flattened = self.intermediate_layer_features[layer].view(self.intermediate_layer_features[layer].size(0), -1) \n",
    "                # intermediate_features[layer].append(layer_output_flattened.cpu())\n",
    "\n",
    "                # DON'T FLATTEN - IT CAUSES PROBLEMS WHEN VISUALISING FEATURES LATER\n",
    "                intermediate_features[layer].append(self.intermediate_layer_features[layer].cpu())\n",
    "\n",
    "        \n",
    "        # Concatenate results for each layer\n",
    "        feats = torch.cat(feats, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        intermediate_features = {layer: torch.cat(intermediate_features[layer], dim=0) for layer in self.intermediate_layers_to_capture}\n",
    "\n",
    "        # Debugging log after concatenation\n",
    "        print(\"â Feature extraction complete. Final feature shapes:\")\n",
    "        print(f\"Final layer: {feats.shape}\")\n",
    "        for layer, feature in intermediate_features.items():\n",
    "            print(f\"{layer}: {feature.shape}\")  # Check final stored shape\n",
    "\n",
    "        return {**intermediate_features, 'final_layer': feats, 'labels': labels}\n",
    "\n",
    "intermediate_layers = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "\n",
    "sim_clr = SimCLR()\n",
    "sim_clr.load_pretrained()\n",
    "sim_clr.set_intermediate_layers_to_capture(intermediate_layers)\n",
    "feats = sim_clr.extract_features(image_dataset)\n",
    "\n",
    "for layer in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    if layer in feats:\n",
    "        variance = np.var(feats[layer].numpy())\n",
    "        print(f\"{layer} variance: {variance:.6f}\")\n",
    "\n",
    "# Our original images are grayscale, but SimCLR expects 3-channel RGB input.\n",
    "# To meet this requirement, we duplicated the grayscale values across all three RGB channels.\n",
    "# However, for PCA, we only need a single channel, so we extract just the first channel (Red).\n",
    "flattened_images = images[:, 0, :, :].view(images.shape[0], -1) # shape: [1573, 50176] (1573 images, 224x224 pixels)\n",
    "\n",
    "layer1_feats = feats['layer1'] # Shape: torch.Size([1573, 200704]) (n_images, n_features)\n",
    "layer2_feats = feats['layer2']\n",
    "layer3_feats = feats['layer3']\n",
    "layer4_feats = feats['layer4']\n",
    "final_layer_feats = feats['final_layer'] # Shape: torch.Size([1573, 512])\n",
    "\n",
    "print('flattened_images shape', flattened_images.shape)\n",
    "print('layer1 shape', layer1_feats.shape)\n",
    "print('final layer shape', final_layer_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea94f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regression from layer feats to PCs of neural data\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "def regressor(X, Y):\n",
    "    alphas = np.logspace(1, 7, 20)\n",
    "    ridge = RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "    ridge.fit(X, Y)\n",
    "\n",
    "    Y_pred = ridge.predict(X)\n",
    "\n",
    "    print(\"Best alpha:\", ridge.alpha_)\n",
    "    print(\"Ridge regression coefficients:\", ridge.coef_.mean(axis=0))\n",
    "    print(\"Ridge regression pred:\", Y_pred)\n",
    "    print(\"Ridge regression score:\", ridge.score(X, Y))\n",
    "\n",
    "    return ridge\n",
    "\n",
    "def l2_penalty(img, lam=0.01):\n",
    "    l2_penalty = lam * torch.sum(img ** 2)\n",
    "    return l2_penalty\n",
    "\n",
    "def generate_synthetic_img(layer_name, ridge, iterations=200, regularise=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sim_clr = SimCLR()\n",
    "    sim_clr.load_pretrained()\n",
    "    sim_clr.eval().to(device)\n",
    "\n",
    "    # Hook layer2\n",
    "    intermediate_features = {}\n",
    "    def hook_fn(module, input, output):\n",
    "        intermediate_features[layer_name] = output\n",
    "\n",
    "    layer = dict([*sim_clr.convnet.named_modules()])[layer_name]\n",
    "    hook_handle = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Convert ridge regressor weights to torch\n",
    "    ridge_weights = torch.tensor(ridge.coef_, dtype=torch.float32, device=device).unsqueeze(0) # (1, D)\n",
    "\n",
    "    synthetic_image = torch.randn(1, 1, 96, 96, device=device, requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([synthetic_image], lr=0.05, weight_decay=1e-6)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        input_img = synthetic_image.repeat(1, 3, 1, 1)\n",
    "        _ = sim_clr.convnet(input_img)\n",
    "\n",
    "        feats = intermediate_features[layer_name].view(1, -1)\n",
    "        score = torch.matmul(feats, ridge_weights.t()).squeeze()\n",
    "        loss = -score + (l2_penalty(synthetic_image) if regularise else 0) \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        synthetic_image.data.clamp_(-1, 1)\n",
    "\n",
    "    img_np = synthetic_image.detach().cpu().squeeze().numpy()\n",
    "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "    hook_handle.remove()\n",
    "\n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For layers 1 and 2, generate synthetic image for all PCs from 1-100\n",
    "\n",
    "# every 10th PC\n",
    "for pc_index in range(0, 101, 10):\n",
    "    pc_neural_data = neural_data_pcs[:, pc_index]\n",
    "\n",
    "    ridge_layer1 = regressor(layer1_feats.view(layer1_feats.size(0), -1), pc_neural_data)\n",
    "    img_layer1 = generate_synthetic_img('layer1', ridge_layer1)\n",
    "\n",
    "    ridge_layer2 = regressor(layer2_feats.view(layer2_feats.size(0), -1), pc_neural_data)\n",
    "    img_layer2 = generate_synthetic_img('layer2', ridge_layer2)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(img_layer1, cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(\"Layer 1\")\n",
    "\n",
    "    axs[1].imshow(img_layer2, cmap='gray')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(\"Layer 2\")\n",
    "\n",
    "    plt.suptitle(f\"Synthetic Image Maximising PC{pc_index+1}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Layer1: Compute cosine similarity between images to reveal variance\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "synthetic_images = []\n",
    "\n",
    "for pc_index in range(0, 100):\n",
    "    pc_neural_data = neural_data_pcs[:, pc_index]\n",
    "    ridge = regressor(layer1_feats.view(layer1_feats.size(0), -1), pc_neural_data)\n",
    "    img_np = generate_synthetic_img('layer1', ridge)\n",
    "    synthetic_images.append(img_np.flatten()) # Flatten each image to 1D\n",
    "\n",
    "# Stack them into a matrix (100, 96*96)\n",
    "synthetic_images = np.stack(synthetic_images)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(synthetic_images)\n",
    "\n",
    "# Calculate average cosine similarity (excluding diagonal)\n",
    "num_images = len(synthetic_images)\n",
    "avg_cosine_similarity = (np.sum(cosine_sim_matrix) - np.trace(cosine_sim_matrix)) / (num_images * (num_images - 1))\n",
    "\n",
    "print(f\"Average Cosine Similarity Layer 1 (L2 Regularized): {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "plt.imshow(cosine_sim_matrix, cmap='viridis')\n",
    "plt.title(f\"Cosine Similarity Matrix Layer 1 (L2 Regularized, Avg: {avg_cosine_similarity:.4f})\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Layer2: Compute cosine similarity between images to reveal variance\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "synthetic_images = []\n",
    "\n",
    "for pc_index in range(0, 100):\n",
    "    pc_neural_data = neural_data_pcs[:, pc_index]\n",
    "    ridge = regressor(layer2_feats.view(layer2_feats.size(0), -1), pc_neural_data)\n",
    "    img_np = generate_synthetic_img('layer2', ridge)\n",
    "    synthetic_images.append(img_np.flatten()) # Flatten each image to 1D\n",
    "\n",
    "# Stack them into a matrix (100, 96*96)\n",
    "synthetic_images = np.stack(synthetic_images)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(synthetic_images)\n",
    "\n",
    "# Calculate average cosine similarity (excluding diagonal)\n",
    "num_images = len(synthetic_images)\n",
    "avg_cosine_similarity = (np.sum(cosine_sim_matrix) - np.trace(cosine_sim_matrix)) / (num_images * (num_images - 1))\n",
    "\n",
    "print(f\"Average Cosine Similarity Layer 2 (L2 Regularized): {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "plt.imshow(cosine_sim_matrix, cmap='viridis')\n",
    "plt.title(f\"Cosine Similarity Matrix Layer 2 (L2 Regularized, Avg: {avg_cosine_similarity:.4f})\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
