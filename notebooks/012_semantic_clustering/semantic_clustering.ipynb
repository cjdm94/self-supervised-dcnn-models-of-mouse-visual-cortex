{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "\n",
    "# imresps.npy is of shape (1573, 2, 15363), where 1573 is number of images, 2 repeats each, and 15363 neurons recorded\n",
    "# stimids.npy has the image id (matching the image dataset ~selection1866~) for each stimulus number, \n",
    "# so of you want to see what image was presented on imresps[502] you would check stim_ids[502]\n",
    "\n",
    "PATH_TO_DATA = '../../data/neural'\n",
    "\n",
    "imresps = np.load(path.join(PATH_TO_DATA, 'imresps.npy'))\n",
    "stimids = np.load(path.join(PATH_TO_DATA, 'stimids.npy'))\n",
    "\n",
    "print(imresps.shape) # (1573, 2, 15363)\n",
    "print(stimids.shape) # (1573,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20549686",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_labels = [\n",
    "    {'stim_id': 1, 'category': 'bird'},\n",
    "    {'stim_id': 2, 'category': 'fungus'},\n",
    "    {'stim_id': 4, 'category': 'fungus'},\n",
    "    {'stim_id': 5, 'category': 'fungus'},\n",
    "    {'stim_id': 7, 'category': 'fungus'},\n",
    "    {'stim_id': 9, 'category': 'bird'},\n",
    "    {'stim_id': 10, 'category': 'fungus'},\n",
    "    {'stim_id': 11, 'category': 'fungus'},\n",
    "    {'stim_id': 12, 'category': 'fungus'},\n",
    "    {'stim_id': 13, 'category': 'fungus'},\n",
    "    {'stim_id': 16, 'category': 'fungus'},\n",
    "    {'stim_id': 17, 'category': 'fungus'},\n",
    "    {'stim_id': 18, 'category': 'fungus'},\n",
    "    {'stim_id': 25, 'category': 'fungus'},\n",
    "    {'stim_id': 28, 'category': 'fungus'},\n",
    "    {'stim_id': 33, 'category': 'fungus'},\n",
    "    {'stim_id': 35, 'category': 'fungus'},\n",
    "    {'stim_id': 40, 'category': 'fungus'},\n",
    "    {'stim_id': 41, 'category': 'fungus'},\n",
    "    {'stim_id': 44, 'category': 'fungus'},\n",
    "    {'stim_id': 45, 'category': 'fungus'},\n",
    "    {'stim_id': 45, 'category': 'fungus'},\n",
    "    {'stim_id': 48, 'category': 'fungus'},\n",
    "    {'stim_id': 49, 'category': 'fungus'},\n",
    "    {'stim_id': 50, 'category': 'fungus'},\n",
    "    {'stim_id': 51, 'category': 'fungus'},\n",
    "    {'stim_id': 55, 'category': 'fungus'},\n",
    "    {'stim_id': 56, 'category': 'fungus'},\n",
    "    {'stim_id': 100, 'category': 'snake'},\n",
    "    {'stim_id': 101, 'category': 'snake'},\n",
    "    {'stim_id': 102, 'category': 'snake'},\n",
    "    {'stim_id': 103, 'category': 'snake'},\n",
    "    {'stim_id': 108, 'category': 'snake'},\n",
    "    {'stim_id': 109, 'category': 'snake'},\n",
    "    {'stim_id': 110, 'category': 'snake'},\n",
    "    {'stim_id': 111, 'category': 'snake'},\n",
    "    {'stim_id': 112, 'category': 'snake'},\n",
    "    {'stim_id': 114, 'category': 'snake'},\n",
    "    {'stim_id': 115, 'category': 'snake'},\n",
    "    {'stim_id': 116, 'category': 'snake'},\n",
    "    {'stim_id': 116, 'category': 'snake'},\n",
    "    {'stim_id': 118, 'category': 'snake'},\n",
    "    {'stim_id': 121, 'category': 'snake'},\n",
    "    {'stim_id': 122, 'category': 'snake'},\n",
    "    {'stim_id': 123, 'category': 'snake'},\n",
    "    {'stim_id': 124, 'category': 'snake'},\n",
    "    {'stim_id': 125, 'category': 'snake'},\n",
    "    {'stim_id': 126, 'category': 'snake'},\n",
    "    {'stim_id': 128, 'category': 'snake'},\n",
    "    {'stim_id': 129, 'category': 'snake'},\n",
    "    {'stim_id': 619, 'category': 'snake'},\n",
    "    {'stim_id': 620, 'category': 'snake'},\n",
    "    {'stim_id': 621, 'category': 'snake'},\n",
    "    {'stim_id': 622, 'category': 'snake'},\n",
    "    {'stim_id': 623, 'category': 'snake'},\n",
    "    {'stim_id': 624, 'category': 'snake'},\n",
    "    {'stim_id': 625, 'category': 'snake'},\n",
    "    {'stim_id': 626, 'category': 'snake'},\n",
    "    {'stim_id': 628, 'category': 'snake'},\n",
    "    {'stim_id': 629, 'category': 'snake'},\n",
    "    {'stim_id': 630, 'category': 'snake'},\n",
    "    {'stim_id': 632, 'category': 'snake'},\n",
    "    {'stim_id': 633, 'category': 'snake'},\n",
    "    {'stim_id': 634, 'category': 'snake'},\n",
    "    {'stim_id': 635, 'category': 'snake'},\n",
    "    {'stim_id': 637, 'category': 'snake'},\n",
    "    {'stim_id': 639, 'category': 'snake'},\n",
    "    {'stim_id': 640, 'category': 'snake'},\n",
    "    {'stim_id': 641, 'category': 'snake'},\n",
    "    {'stim_id': 152, 'category': 'bird'},\n",
    "    {'stim_id': 153, 'category': 'bird'},\n",
    "    {'stim_id': 155, 'category': 'bird'},\n",
    "    {'stim_id': 156, 'category': 'bird'},\n",
    "    {'stim_id': 157, 'category': 'bird'},\n",
    "    {'stim_id': 158, 'category': 'bird'},\n",
    "    {'stim_id': 159, 'category': 'bird'},\n",
    "    {'stim_id': 160, 'category': 'bird'},\n",
    "    {'stim_id': 161, 'category': 'bird'},\n",
    "    {'stim_id': 162, 'category': 'bird'},\n",
    "    {'stim_id': 163, 'category': 'bird'},\n",
    "    {'stim_id': 164, 'category': 'bird'},\n",
    "    {'stim_id': 166, 'category': 'bird'},\n",
    "    {'stim_id': 170, 'category': 'bird'},\n",
    "    {'stim_id': 172, 'category': 'bird'},\n",
    "    {'stim_id': 173, 'category': 'bird'},\n",
    "    {'stim_id': 174, 'category': 'bird'},\n",
    "    {'stim_id': 175, 'category': 'bird'},\n",
    "    {'stim_id': 176, 'category': 'bird'},\n",
    "    {'stim_id': 177, 'category': 'bird'},\n",
    "    {'stim_id': 176, 'category': 'bird'},\n",
    "    {'stim_id': 182, 'category': 'bird'},\n",
    "    {'stim_id': 184, 'category': 'bird'},\n",
    "    {'stim_id': 185, 'category': 'bird'},\n",
    "    {'stim_id': 186, 'category': 'bird'},\n",
    "    {'stim_id': 187, 'category': 'bird'},\n",
    "    {'stim_id': 188, 'category': 'bird'},\n",
    "    {'stim_id': 189, 'category': 'bird'},\n",
    "    {'stim_id': 190, 'category': 'bird'},\n",
    "    {'stim_id': 191, 'category': 'bird'},\n",
    "    {'stim_id': 192, 'category': 'bird'},\n",
    "    {'stim_id': 131, 'category': 'bird'},\n",
    "    {'stim_id': 203, 'category': 'bird'},\n",
    "    {'stim_id': 206, 'category': 'bird'},\n",
    "    {'stim_id': 674, 'category': 'bird'},\n",
    "    {'stim_id': 1200, 'category': 'bird'}, \n",
    "    {'stim_id': 1207, 'category': 'bird'}, \n",
    "    {'stim_id': 1240, 'category': 'bird'}, \n",
    "    {'stim_id': 1790, 'category': 'bird'},\n",
    "    {'stim_id': 133, 'category': 'cat'}, \n",
    "    {'stim_id': 134, 'category': 'cat'}, \n",
    "    {'stim_id': 135, 'category': 'cat'}, \n",
    "    {'stim_id': 136, 'category': 'cat'}, \n",
    "    {'stim_id': 137, 'category': 'cat'}, \n",
    "    {'stim_id': 138, 'category': 'cat'}, \n",
    "    {'stim_id': 139, 'category': 'cat'}, \n",
    "    {'stim_id': 140, 'category': 'cat'}, \n",
    "    {'stim_id': 141, 'category': 'cat'}, \n",
    "    {'stim_id': 143, 'category': 'cat'}, \n",
    "    {'stim_id': 144, 'category': 'cat'}, \n",
    "    {'stim_id': 145, 'category': 'cat'}, \n",
    "    {'stim_id': 146, 'category': 'cat'}, \n",
    "    {'stim_id': 147, 'category': 'cat'}, \n",
    "    {'stim_id': 148, 'category': 'cat'}, \n",
    "    {'stim_id': 149, 'category': 'cat'}, \n",
    "    {'stim_id': 150, 'category': 'cat'}, \n",
    "    {'stim_id': 219, 'category': 'cat'}, \n",
    "    {'stim_id': 221, 'category': 'cat'}, \n",
    "    {'stim_id': 222, 'category': 'cat'}, \n",
    "    {'stim_id': 223, 'category': 'cat'}, \n",
    "    {'stim_id': 224, 'category': 'cat'}, \n",
    "    {'stim_id': 225, 'category': 'cat'}, \n",
    "    {'stim_id': 226, 'category': 'cat'}, \n",
    "    {'stim_id': 227, 'category': 'cat'}, \n",
    "    {'stim_id': 228, 'category': 'cat'}, \n",
    "    {'stim_id': 229, 'category': 'cat'}, \n",
    "    {'stim_id': 230, 'category': 'cat'}, \n",
    "    {'stim_id': 232, 'category': 'cat'}, \n",
    "    {'stim_id': 233, 'category': 'cat'}, \n",
    "    {'stim_id': 235, 'category': 'cat'}, \n",
    "    {'stim_id': 237, 'category': 'cat'}, \n",
    "    {'stim_id': 238, 'category': 'cat'}, \n",
    "    {'stim_id': 239, 'category': 'cat'}, \n",
    "    {'stim_id': 241, 'category': 'cat'}, \n",
    "    {'stim_id': 243, 'category': 'cat'}, \n",
    "    {'stim_id': 244, 'category': 'cat'}, \n",
    "    {'stim_id': 245, 'category': 'cat'}, \n",
    "    {'stim_id': 246, 'category': 'cat'}, \n",
    "    {'stim_id': 247, 'category': 'cat'}, \n",
    "    {'stim_id': 248, 'category': 'cat'}, \n",
    "    {'stim_id': 249, 'category': 'cat'}, \n",
    "    {'stim_id': 250, 'category': 'cat'}, \n",
    "    {'stim_id': 252, 'category': 'cat'}, \n",
    "    {'stim_id': 254, 'category': 'cat'}, \n",
    "    {'stim_id': 255, 'category': 'cat'}, \n",
    "    {'stim_id': 256, 'category': 'cat'}, \n",
    "    {'stim_id': 257, 'category': 'cat'}, \n",
    "    {'stim_id': 258, 'category': 'cat'},\n",
    "    {'stim_id': 291, 'category': 'rodent'}, \n",
    "    {'stim_id': 293, 'category': 'rodent'}, \n",
    "    {'stim_id': 294, 'category': 'rodent'}, \n",
    "    {'stim_id': 295, 'category': 'rodent'}, \n",
    "    {'stim_id': 295, 'category': 'rodent'}, \n",
    "    {'stim_id': 296, 'category': 'rodent'}, \n",
    "    {'stim_id': 297, 'category': 'rodent'}, \n",
    "    {'stim_id': 299, 'category': 'rodent'}, \n",
    "    {'stim_id': 300, 'category': 'rodent'}, \n",
    "    {'stim_id': 302, 'category': 'rodent'}, \n",
    "    {'stim_id': 304, 'category': 'rodent'}, \n",
    "    {'stim_id': 306, 'category': 'rodent'}, \n",
    "    {'stim_id': 307, 'category': 'rodent'}, \n",
    "    {'stim_id': 309, 'category': 'rodent'}, \n",
    "    {'stim_id': 310, 'category': 'rodent'}, \n",
    "    {'stim_id': 311, 'category': 'rodent'}, \n",
    "    {'stim_id': 315, 'category': 'rodent'}, \n",
    "    {'stim_id': 318, 'category': 'rodent'}, \n",
    "    {'stim_id': 320, 'category': 'rodent'}, \n",
    "    {'stim_id': 323, 'category': 'rodent'}, \n",
    "    {'stim_id': 324, 'category': 'rodent'}, \n",
    "    {'stim_id': 325, 'category': 'rodent'}, \n",
    "    {'stim_id': 326, 'category': 'rodent'},\n",
    "    {'stim_id': 330, 'category': 'rodent'},\n",
    "    {'stim_id': 331, 'category': 'rodent'},\n",
    "    {'stim_id': 332, 'category': 'rodent'},\n",
    "    {'stim_id': 333, 'category': 'rodent'},\n",
    "    {'stim_id': 334, 'category': 'rodent'},\n",
    "    {'stim_id': 335, 'category': 'rodent'},\n",
    "    {'stim_id': 336, 'category': 'rodent'},\n",
    "    {'stim_id': 337, 'category': 'rodent'},\n",
    "    {'stim_id': 338, 'category': 'rodent'},\n",
    "    {'stim_id': 339, 'category': 'rodent'},\n",
    "    {'stim_id': 343, 'category': 'rodent'},\n",
    "    {'stim_id': 344, 'category': 'rodent'},\n",
    "    {'stim_id': 810, 'category': 'rodent'},\n",
    "    {'stim_id': 812, 'category': 'rodent'},\n",
    "    {'stim_id': 813, 'category': 'rodent'},\n",
    "    {'stim_id': 815, 'category': 'rodent'},\n",
    "    {'stim_id': 817, 'category': 'rodent'},\n",
    "    {'stim_id': 818, 'category': 'rodent'},\n",
    "    {'stim_id': 822, 'category': 'rodent'},\n",
    "    {'stim_id': 823, 'category': 'rodent'},\n",
    "    {'stim_id': 824, 'category': 'rodent'},\n",
    "    {'stim_id': 826, 'category': 'rodent'},\n",
    "    {'stim_id': 828, 'category': 'rodent'},\n",
    "    {'stim_id': 830, 'category': 'rodent'},\n",
    "    {'stim_id': 832, 'category': 'rodent'},\n",
    "    {'stim_id': 833, 'category': 'rodent'},\n",
    "    {'stim_id': 834, 'category': 'rodent'},\n",
    "    {'stim_id': 836, 'category': 'rodent'},\n",
    "    {'stim_id': 838, 'category': 'rodent'},\n",
    "    {'stim_id': 841, 'category': 'rodent'},\n",
    "    {'stim_id': 843, 'category': 'rodent'},\n",
    "    {'stim_id': 844, 'category': 'rodent'},\n",
    "    {'stim_id': 845, 'category': 'rodent'},\n",
    "    {'stim_id': 856, 'category': 'rodent'},\n",
    "    {'stim_id': 858, 'category': 'rodent'},\n",
    "    {'stim_id': 859, 'category': 'rodent'},\n",
    "    {'stim_id': 860, 'category': 'rodent'},\n",
    "    {'stim_id': 861, 'category': 'rodent'},\n",
    "    {'stim_id': 862, 'category': 'rodent'},\n",
    "    {'stim_id': 863, 'category': 'rodent'},\n",
    "    {'stim_id': 1739, 'category': 'insect'},\n",
    "    {'stim_id': 1740, 'category': 'insect'},\n",
    "    {'stim_id': 1741, 'category': 'insect'},\n",
    "    {'stim_id': 1746, 'category': 'insect'},\n",
    "    {'stim_id': 1748, 'category': 'insect'},\n",
    "    {'stim_id': 1749, 'category': 'insect'},\n",
    "    {'stim_id': 1750, 'category': 'insect'},\n",
    "    {'stim_id': 1751, 'category': 'insect'},\n",
    "    {'stim_id': 1752, 'category': 'insect'},\n",
    "    {'stim_id': 1755, 'category': 'insect'},\n",
    "    {'stim_id': 1760, 'category': 'insect'},\n",
    "    {'stim_id': 1761, 'category': 'insect'},\n",
    "    {'stim_id': 1762, 'category': 'insect'},\n",
    "    {'stim_id': 1763, 'category': 'insect'},\n",
    "    {'stim_id': 1765, 'category': 'insect'},\n",
    "    {'stim_id': 1773, 'category': 'insect'},\n",
    "    {'stim_id': 1775, 'category': 'insect'},\n",
    "    {'stim_id': 1776, 'category': 'insect'},\n",
    "    {'stim_id': 1782, 'category': 'insect'},\n",
    "    {'stim_id': 1783, 'category': 'insect'},\n",
    "    {'stim_id': 1422, 'category': 'insect'},\n",
    "    {'stim_id': 1424, 'category': 'insect'},\n",
    "    {'stim_id': 1425, 'category': 'insect'},\n",
    "    {'stim_id': 1429, 'category': 'insect'},\n",
    "    {'stim_id': 1431, 'category': 'insect'},\n",
    "    {'stim_id': 1435, 'category': 'insect'},\n",
    "    {'stim_id': 1436, 'category': 'insect'},\n",
    "    {'stim_id': 1439, 'category': 'insect'},\n",
    "    {'stim_id': 1457, 'category': 'insect'},\n",
    "    {'stim_id': 1458, 'category': 'insect'},\n",
    "    {'stim_id': 1459, 'category': 'insect'},\n",
    "    {'stim_id': 1460, 'category': 'insect'},\n",
    "    {'stim_id': 1462, 'category': 'insect'},\n",
    "    {'stim_id': 1465, 'category': 'insect'},\n",
    "    {'stim_id': 1471, 'category': 'insect'},\n",
    "    {'stim_id': 1472, 'category': 'insect'},\n",
    "    {'stim_id': 3, 'category': 'texture'},\n",
    "    {'stim_id': 8, 'category': 'texture'},\n",
    "    {'stim_id': 14, 'category': 'texture'},\n",
    "    {'stim_id': 15, 'category': 'texture'},\n",
    "    {'stim_id': 79, 'category': 'texture'},\n",
    "    {'stim_id': 82, 'category': 'texture'},\n",
    "    {'stim_id': 88, 'category': 'texture'},\n",
    "    {'stim_id': 98, 'category': 'texture'},\n",
    "    {'stim_id': 270, 'category': 'texture'},\n",
    "    {'stim_id': 285, 'category': 'texture'},\n",
    "    {'stim_id': 366, 'category': 'texture'},\n",
    "    {'stim_id': 379, 'category': 'texture'},\n",
    "    {'stim_id': 388, 'category': 'texture'},\n",
    "    {'stim_id': 406, 'category': 'texture'},\n",
    "    {'stim_id': 517, 'category': 'texture'},\n",
    "    {'stim_id': 518, 'category': 'texture'},\n",
    "    {'stim_id': 529, 'category': 'texture'},\n",
    "    {'stim_id': 537, 'category': 'texture'},\n",
    "    {'stim_id': 555, 'category': 'texture'},\n",
    "    {'stim_id': 566, 'category': 'texture'},\n",
    "    {'stim_id': 578, 'category': 'texture'},\n",
    "    {'stim_id': 569, 'category': 'texture'},\n",
    "    {'stim_id': 580, 'category': 'texture'},\n",
    "    {'stim_id': 581, 'category': 'texture'},\n",
    "    {'stim_id': 584, 'category': 'texture'},\n",
    "    {'stim_id': 586, 'category': 'texture'},\n",
    "    {'stim_id': 600, 'category': 'texture'},\n",
    "    {'stim_id': 601, 'category': 'texture'},\n",
    "    {'stim_id': 607, 'category': 'texture'},\n",
    "    {'stim_id': 612, 'category': 'texture'},\n",
    "    {'stim_id': 672, 'category': 'texture'},\n",
    "    {'stim_id': 680, 'category': 'texture'},\n",
    "    {'stim_id': 676, 'category': 'texture'},\n",
    "    {'stim_id': 673, 'category': 'texture'},\n",
    "    {'stim_id': 686, 'category': 'texture'},\n",
    "    {'stim_id': 729, 'category': 'texture'},\n",
    "    {'stim_id': 761, 'category': 'texture'},\n",
    "    {'stim_id': 765, 'category': 'texture'},\n",
    "    {'stim_id': 774, 'category': 'texture'},\n",
    "    {'stim_id': 787, 'category': 'texture'},\n",
    "    {'stim_id': 788, 'category': 'texture'},\n",
    "    {'stim_id': 784, 'category': 'texture'},\n",
    "    {'stim_id': 789, 'category': 'texture'},\n",
    "    {'stim_id': 803, 'category': 'texture'},\n",
    "    {'stim_id': 801, 'category': 'texture'},\n",
    "    {'stim_id': 799, 'category': 'texture'},\n",
    "    {'stim_id': 809, 'category': 'texture'},\n",
    "    {'stim_id': 829, 'category': 'texture'},\n",
    "    {'stim_id': 842, 'category': 'texture'},\n",
    "    {'stim_id': 847, 'category': 'texture'},\n",
    "    {'stim_id': 850, 'category': 'texture'},\n",
    "    {'stim_id': 866, 'category': 'texture'},\n",
    "    {'stim_id': 873, 'category': 'texture'},\n",
    "    {'stim_id': 871, 'category': 'texture'},\n",
    "    {'stim_id': 870, 'category': 'texture'},\n",
    "    {'stim_id': 869, 'category': 'texture'},\n",
    "    {'stim_id': 868, 'category': 'texture'},\n",
    "    {'stim_id': 878, 'category': 'texture'},\n",
    "    {'stim_id': 880, 'category': 'texture'},\n",
    "    {'stim_id': 886, 'category': 'texture'},\n",
    "    {'stim_id': 894, 'category': 'texture'},\n",
    "    {'stim_id': 897, 'category': 'texture'},\n",
    "    {'stim_id': 1051, 'category': 'texture'}\n",
    "]\n",
    "\n",
    "num_labeled = len(manual_labels)\n",
    "num_insect = len([label for label in manual_labels if label['category'] == 'insect'])\n",
    "num_rodent = len([label for label in manual_labels if label['category'] == 'rodent'])\n",
    "num_snake = len([label for label in manual_labels if label['category'] == 'snake'])\n",
    "num_bird = len([label for label in manual_labels if label['category'] == 'bird'])\n",
    "num_fungus = len([label for label in manual_labels if label['category'] == 'fungus'])\n",
    "num_cat = len([label for label in manual_labels if label['category'] == 'cat'])\n",
    "num_texture = len([label for label in manual_labels if label['category'] == 'texture'])\n",
    "\n",
    "print(f\"Number of labeled images: {num_labeled}\")\n",
    "print(f\"Number of insect images: {num_insect}\")\n",
    "print(f\"Number of rodent images: {num_rodent}\")\n",
    "print(f\"Number of snake images: {num_snake}\")\n",
    "print(f\"Number of bird images: {num_bird}\")\n",
    "print(f\"Number of fungus images: {num_fungus}\")\n",
    "print(f\"Number of cat images: {num_cat}\")\n",
    "print(f\"Number of textre images: {num_texture}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and preprocess images\n",
    "\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import Normalize, Compose, Resize, CenterCrop\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import utils as torch_utils\n",
    " \n",
    "PATH_TO_DATA = '../../data/selection1866'\n",
    "\n",
    "file_list = sorted(f for f in os.listdir(PATH_TO_DATA) if f.endswith('.mat'))\n",
    "stim_ids = stimids.astype(int)\n",
    "\n",
    "print(stim_ids)\n",
    "print(stimids)\n",
    "\n",
    "transform = Compose([\n",
    "    Resize(96), # Resize shortest edge to 96 (cut off the rightmost part of the image)\n",
    "    CenterCrop((96, 96)), # Crop to (96, 96)\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # !! Normalize expects input is already in the range [0, 1]\n",
    "])\n",
    "\n",
    "img_tensors, labels, category_labels = [], [], []\n",
    "\n",
    "print('List:', file_list)\n",
    "\n",
    "# we have 1866 images here, but the neural response data only uses 1573 of them\n",
    "# because some ~300 images didn't have two repeats, so were disposed\n",
    "# therefore we filter the full set here so that we only use the relevant 1573\n",
    "for stim_id in stim_ids:\n",
    "    filename = 'img' + str(stim_id) + '.mat'\n",
    "    data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "\n",
    "    img = data['img'][:, :500] # Take leftmost part of the image\n",
    "    rgb_img = np.stack([img] * 3, axis=-1) # Convert grayscale to RGB for SimCLR\n",
    "    tensor = torch.tensor(rgb_img, dtype=torch.float32).permute(2, 0, 1) # Shape (C, H, W)\n",
    "    \n",
    "    # Min-max scale the tensor to [0, 1]\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "\n",
    "    # Clamp to [0, 1] to ensure no outliers due to numerical precision\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)\n",
    "\n",
    "    transformed_tensor = transform(tensor) # Normalize and resize for SimCLR\n",
    "    img_tensors.append(transformed_tensor)\n",
    "\n",
    "    # Append category label for sample that we have manually assigned categories to\n",
    "    labels.append(stim_id)\n",
    "    entry = next((item for item in manual_labels if item['stim_id'] == stim_id), None)\n",
    "    if entry is not None:\n",
    "        category_labels.append(entry['category'])\n",
    "    else:\n",
    "        category_labels.append('undetermined')\n",
    "    \n",
    "print(labels)\n",
    "\n",
    "image_dataset = TensorDataset(torch.stack(img_tensors), torch.tensor(labels))\n",
    "\n",
    "images, labels = image_dataset.tensors\n",
    "print(\"Processed image labels (stim id):\", labels[:30])\n",
    "print(\"Stim IDs from neural data:\", stim_ids[:30])\n",
    "print(\"Processed dataset shape:\", images.shape) # (N, C, 96, 96)\n",
    "print(f\"Min pixel value (processed): {torch.min(images)}\")\n",
    "print(f\"Max pixel value (processed): {torch.max(images)}\")\n",
    "\n",
    "# Show a sample of processed images\n",
    "img_grid = torch_utils.make_grid(images[:12], nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Processed images: sample')\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "filename = 'img20.mat'\n",
    "data = loadmat(os.path.join(PATH_TO_DATA, filename))\n",
    "img = data['img'][:, :500]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img, cmap='gray')  # Adjust cmap as needed ('viridis', 'jet', etc.)\n",
    "plt.colorbar(label=\"Pixel Intensity\")\n",
    "plt.title(\"Rendered Image\")\n",
    "plt.axis(\"off\")  # Hide axis for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Show images with stimulus IDs, and manually assign semantic labels\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# ncols = 8\n",
    "# manual_labels = {}\n",
    "\n",
    "# for i in range(0, len(images), BATCH_SIZE):\n",
    "#     imgs = images[i:i+BATCH_SIZE]\n",
    "#     ids = labels[i:i+BATCH_SIZE]\n",
    "\n",
    "#     n = len(imgs)\n",
    "#     nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "#     fig, axs = plt.subplots(nrows, ncols, figsize=(2 * ncols, 2 * nrows))\n",
    "#     axs = axs.flatten()\n",
    "\n",
    "#     for j, (img, stim_id) in enumerate(zip(imgs, ids)):\n",
    "#         img_np = img.permute(1, 2, 0).numpy()\n",
    "#         img_np = (img_np + 1) / 2  # Normalize from [-1, 1] to [0, 1]\n",
    "#         img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "#         axs[j].imshow(img_np)\n",
    "#         axs[j].set_title(f'{stim_id}', fontsize=9)\n",
    "#         axs[j].axis('off')\n",
    "\n",
    "#     # Turn off any extra axes\n",
    "#     for k in range(j + 1, len(axs)):\n",
    "#         axs[k].axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run images through a pretrained SimCLR model and extract features\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict\n",
    "from torch.utils.data import Dataset\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Base ResNet18 backbone (pretrained=False, because we load custom weights later, from the SimCLR checkpoint file)\n",
    "        self.convnet = torchvision.models.resnet18(pretrained=False)\n",
    "        \n",
    "        # This is the projection head, only needed during training. For downstream tasks it is disposed of\n",
    "        # and the final linear layer output is used (Chen et al., 2020) \n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.intermediate_layers_to_capture =[]\n",
    "        self.intermediate_layer_features = {}\n",
    "        self.num_workers = os.cpu_count()\n",
    "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    def load_pretrained(self):\n",
    "        \"\"\"\n",
    "        Load pretrained SimCLR weights\n",
    "        \"\"\"\n",
    "        base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial17/\"\n",
    "        models_dir = \"../../models\"\n",
    "        pretrained_simclr_filename = \"SimCLR.ckpt\"\n",
    "        pretrained_simclr_path = os.path.join(models_dir, pretrained_simclr_filename)\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "        # Check whether the pretrained model file already exists locally. If not, try downloading it\n",
    "        file_url = base_url + pretrained_simclr_filename\n",
    "        if not os.path.isfile(pretrained_simclr_path):\n",
    "            print(f\"Downloading pretrained SimCLR model {file_url}...\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(file_url, pretrained_simclr_path)\n",
    "            except HTTPError as e:\n",
    "                print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
    "\n",
    "        print(f\"Already downloaded pretrained model: {file_url}\")\n",
    "\n",
    "        # Load pretrained model\n",
    "        checkpoint = torch.load(pretrained_simclr_path, map_location=self.device)\n",
    "        self.load_state_dict(checkpoint['state_dict'])\n",
    "        self.to(self.device)\n",
    "        self.eval()\n",
    "    \n",
    "    def set_intermediate_layers_to_capture(self, layers):\n",
    "        \"\"\"\n",
    "        Register hooks to capture features from intermediate layers\n",
    "        \"\"\"\n",
    "        # Just check the layers specified are actually in the convnet\n",
    "        top_level_block_layers = [name for name, _ in self.convnet.named_children()]\n",
    "        if not all(layer in top_level_block_layers for layer in layers):\n",
    "            print('You have specified convnet layers that are not top-level blocks - make sure your layer names are valid')\n",
    "        \n",
    "        self.intermediate_layers_to_capture = layers\n",
    "        intermediate_layer_features = {}\n",
    "\n",
    "        def get_hook(layer_name):\n",
    "            def hook(module, input, output):\n",
    "                intermediate_layer_features[layer_name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        for layer_name in layers:\n",
    "            layer = dict([*self.convnet.named_modules()])[layer_name]\n",
    "            layer.register_forward_hook(get_hook(layer_name))\n",
    "\n",
    "        self.intermediate_layer_features = intermediate_layer_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_features(self, dataset: Dataset) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run the pretrained SimCLR model on the image data, and capture features from final layer and intermediate layers.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): A PyTorch Dataset containing input images and labels. The image data should have shape (N, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: A dictionary containing:\n",
    "                - Intermediate layer features as tensors.\n",
    "                - Final layer features under 'final_layer'.\n",
    "                - Labels under 'labels'.\n",
    "            Features from a given layer has shape (N, F) where N is num images, F is number of features - flattened version of (C, H, W).\n",
    "        \"\"\"\n",
    "        self.convnet.fc = nn.Identity()  # Removing projection head g(.)\n",
    "        self.eval()\n",
    "        self.to(self.device)\n",
    "        \n",
    "        # Encode all images\n",
    "        data_loader = DataLoader(dataset, batch_size=64, num_workers=self.num_workers, shuffle=False, drop_last=False)\n",
    "        feats, labels, intermediate_features = [], [], {layer: [] for layer in self.intermediate_layers_to_capture}\n",
    "\n",
    "        for batch_idx, (batch_imgs, batch_labels) in enumerate(tqdm(data_loader)):\n",
    "            batch_imgs = batch_imgs.to(self.device)\n",
    "            batch_feats = self.convnet(batch_imgs)\n",
    "            \n",
    "            feats.append(batch_feats.detach().cpu())\n",
    "            labels.append(batch_labels)\n",
    "\n",
    "            # Collect intermediate layer outputs\n",
    "            for layer in self.intermediate_layers_to_capture:\n",
    "                # Final linear layer outputs a 2d tensor; but intermediate layers don't, so we flatten them (ready for PCA etc.)\n",
    "                # layer_output_flattened = self.intermediate_layer_features[layer].view(self.intermediate_layer_features[layer].size(0), -1) \n",
    "                # intermediate_features[layer].append(layer_output_flattened.cpu())\n",
    "\n",
    "                # DON'T FLATTEN - IT CAUSES PROBLEMS WHEN VISUALISING FEATURES LATER\n",
    "                intermediate_features[layer].append(self.intermediate_layer_features[layer].cpu())\n",
    "\n",
    "        \n",
    "        # Concatenate results for each layer\n",
    "        feats = torch.cat(feats, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        intermediate_features = {layer: torch.cat(intermediate_features[layer], dim=0) for layer in self.intermediate_layers_to_capture}\n",
    "\n",
    "        # Debugging log after concatenation\n",
    "        print(\"✅ Feature extraction complete. Final feature shapes:\")\n",
    "        print(f\"Final layer: {feats.shape}\")\n",
    "        for layer, feature in intermediate_features.items():\n",
    "            print(f\"{layer}: {feature.shape}\")  # Check final stored shape\n",
    "\n",
    "        return {**intermediate_features, 'final_layer': feats, 'labels': labels}\n",
    "\n",
    "intermediate_layers = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "\n",
    "sim_clr = SimCLR()\n",
    "sim_clr.load_pretrained()\n",
    "sim_clr.set_intermediate_layers_to_capture(intermediate_layers)\n",
    "feats = sim_clr.extract_features(image_dataset)\n",
    "\n",
    "for layer in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "    if layer in feats:\n",
    "        variance = np.var(feats[layer].numpy())\n",
    "        print(f\"{layer} variance: {variance:.6f}\")\n",
    "\n",
    "# Our original images are grayscale, but SimCLR expects 3-channel RGB input.\n",
    "# To meet this requirement, we duplicated the grayscale values across all three RGB channels.\n",
    "# However, for PCA, we only need a single channel, so we extract just the first channel (Red).\n",
    "flattened_images = images[:, 0, :, :].view(images.shape[0], -1) # shape: [1573, 50176] (1573 images, 224x224 pixels)\n",
    "\n",
    "layer1_feats = feats['layer1'] # Shape: torch.Size([1573, 200704]) (n_images, n_features)\n",
    "layer2_feats = feats['layer2']\n",
    "layer3_feats = feats['layer3']\n",
    "layer4_feats = feats['layer4']\n",
    "final_layer_feats = feats['final_layer'] # Shape: torch.Size([1573, 512])\n",
    "\n",
    "print('flattened_images shape', flattened_images.shape)\n",
    "print('layer1 shape', layer1_feats.shape)\n",
    "print('final layer shape', final_layer_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf226641",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For each layer, reduce features with PCA (optional) then t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "feats = layer2_feats.view(layer2_feats.size(0), -1) \n",
    "\n",
    "# Make mask for all determined labels\n",
    "feats_np = feats.cpu().numpy()\n",
    "valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "filtered_feats = feats_np[valid_mask]\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "\n",
    "feats_pca = PCA(n_components=100).fit_transform(filtered_feats)\n",
    "tsne_feats = TSNE(n_components=2, perplexity=30).fit_transform(feats_pca)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=tsne_feats[:, 0], y=tsne_feats[:, 1], hue=filtered_labels, palette='tab10', s=30)\n",
    "plt.title(\"t-SNE on SimCLR layer2 features (filtered)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(tsne_feats, filtered_labels)\n",
    "print(f\"Silhouette score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a01f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For each layer, reduce features with PCA (optional) then t-SNE\n",
    "feats = final_layer_feats\n",
    "\n",
    "# Make mask for all determined labels\n",
    "feats_np = feats.cpu().numpy()\n",
    "valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "filtered_feats = feats_np[valid_mask]\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "\n",
    "feats_pca = PCA(n_components=100).fit_transform(filtered_feats)\n",
    "tsne_feats = TSNE(n_components=2, perplexity=30).fit_transform(feats_pca)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=tsne_feats[:, 0], y=tsne_feats[:, 1], hue=filtered_labels, palette='tab10', s=30)\n",
    "plt.title(\"t-SNE on SimCLR fc features (filtered)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(tsne_feats, filtered_labels)\n",
    "print(f\"Silhouette score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f03c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For each layer, reduce features with PCA (optional) then t-SNE\n",
    "feats = layer4_feats.view(layer1_feats.size(0), -1)\n",
    "\n",
    "# Make mask for all determined labels\n",
    "feats_np = feats.cpu().numpy()\n",
    "valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "filtered_feats = feats_np[valid_mask]\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "\n",
    "feats_pca = PCA(n_components=100).fit_transform(filtered_feats)\n",
    "tsne_feats = TSNE(n_components=2, perplexity=30).fit_transform(feats_pca)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=tsne_feats[:, 0], y=tsne_feats[:, 1], hue=filtered_labels, palette='tab10', s=30)\n",
    "plt.title(\"t-SNE on SimCLR layer4 features (filtered)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(tsne_feats, filtered_labels)\n",
    "print(f\"Silhouette score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For each layer, reduce features with PCA (optional) then t-SNE\n",
    "feats = layer3_feats.view(layer3_feats.size(0), -1)\n",
    "\n",
    "# Make mask for all determined labels\n",
    "feats_np = feats.cpu().numpy()\n",
    "valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "filtered_feats = feats_np[valid_mask]\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "\n",
    "feats_pca = PCA(n_components=100).fit_transform(filtered_feats)\n",
    "tsne_feats = TSNE(n_components=2, perplexity=30).fit_transform(feats_pca)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=tsne_feats[:, 0], y=tsne_feats[:, 1], hue=filtered_labels, palette='tab10', s=30)\n",
    "plt.title(\"t-SNE on SimCLR layer3 features (filtered)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(tsne_feats, filtered_labels)\n",
    "print(f\"Silhouette score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For each layer, reduce features with PCA (optional) then t-SNE\n",
    "feats = layer1_feats.view(layer1_feats.size(0), -1)\n",
    "\n",
    "# Make mask for all determined labels\n",
    "feats_np = feats.cpu().numpy()\n",
    "valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "filtered_feats = feats_np[valid_mask]\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "\n",
    "feats_pca = PCA(n_components=100).fit_transform(filtered_feats)\n",
    "tsne_feats = TSNE(n_components=2, perplexity=30).fit_transform(feats_pca)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=tsne_feats[:, 0], y=tsne_feats[:, 1], hue=filtered_labels, palette='tab10', s=30)\n",
    "plt.title(\"t-SNE on SimCLR layer1 features (filtered)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(tsne_feats, filtered_labels)\n",
    "print(f\"Silhouette score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db32297",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recompute silhouette score for raw features\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "raw_feats = layer2_feats.view(layer2_feats.size(0), -1)\n",
    "\n",
    "feats_np = raw_feats.cpu().numpy()\n",
    "valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "filtered_feats = feats_np[valid_mask]\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "\n",
    "tsne_feats = TSNE(n_components=2, perplexity=30).fit_transform(filtered_feats)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=tsne_feats[:, 0], y=tsne_feats[:, 1], hue=filtered_labels, palette='tab10', s=30)\n",
    "plt.title(\"t-SNE on SimCLR final layer features (filtered)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "score = silhouette_score(tsne_feats, filtered_labels)\n",
    "print(f\"Silhouette score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164589f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### t-SNE silhouette scores\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "layer_feats = {\n",
    "    'layer1': layer1_feats.view(layer1_feats.size(0), -1),\n",
    "    'layer2': layer2_feats.view(layer2_feats.size(0), -1),\n",
    "    'layer3': layer3_feats.view(layer3_feats.size(0), -1),\n",
    "    'layer4': layer4_feats.view(layer4_feats.size(0), -1),\n",
    "    'fc': final_layer_feats,\n",
    "}\n",
    "\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "\n",
    "for layer_name, feats in layer_feats.items():\n",
    "    feats_np = feats.cpu().numpy()\n",
    "    valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "    filtered_feats = feats_np[valid_mask]\n",
    "\n",
    "    # feats_pca = PCA(n_components=100).fit_transform(filtered_feats)\n",
    "    tsne_feats = TSNE(n_components=2, perplexity=30).fit_transform(filtered_feats)\n",
    "    score = silhouette_score(tsne_feats, filtered_labels)\n",
    "    print(f\"Silhouette score for {layer_name}: {score:.3f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=tsne_feats[:, 0], y=tsne_feats[:, 1], hue=filtered_labels, palette='tab10', s=30)\n",
    "    plt.title(f\"t-SNE on SimCLR {layer_name} features (filtered)\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Silhouette score for layer1: -0.077\n",
    "# Silhouette score for layer2: -0.046\n",
    "# Silhouette score for layer3: -0.023\n",
    "# Silhouette score for layer4: -0.037\n",
    "# Silhouette score for fc: 0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94698285",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try K-means and purity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def compute_purity(y_true, y_pred):\n",
    "    # For each cluster, find the most frequent true label\n",
    "    total = 0\n",
    "    for cluster in np.unique(y_pred):\n",
    "        indices = np.where(y_pred == cluster)[0]\n",
    "        true_labels = y_true[indices]\n",
    "        most_common = Counter(true_labels).most_common(1)[0][1]\n",
    "        total += most_common\n",
    "    return total / len(y_true)\n",
    "\n",
    "layer_feats = {\n",
    "    'layer1': layer1_feats.view(layer1_feats.size(0), -1),\n",
    "    'layer2': layer2_feats.view(layer2_feats.size(0), -1),\n",
    "    'layer3': layer3_feats.view(layer3_feats.size(0), -1),\n",
    "    'layer4': layer4_feats.view(layer4_feats.size(0), -1),\n",
    "    'fc': final_layer_feats,\n",
    "}\n",
    "\n",
    "def compute_random_purity_baseline(y_true, n_clusters=None, n_trials=20, seed=None):\n",
    "    # Compute the average cluster purity of random cluster assignments\n",
    "    y_true = np.array(y_true)\n",
    "    if n_clusters is None:\n",
    "        n_clusters = len(np.unique(y_true))\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    purities = []\n",
    "    for _ in range(n_trials):\n",
    "        y_random = rng.integers(0, n_clusters, size=len(y_true))\n",
    "        purity = compute_purity(y_true, y_random)\n",
    "        purities.append(purity)\n",
    "    \n",
    "    return np.mean(purities)\n",
    "\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(filtered_labels)  # e.g., 'cat' → 0, 'bird' → 1, etc.\n",
    "n_clusters = len(np.unique(encoded_labels))\n",
    "random_baseline = compute_random_purity_baseline(encoded_labels, n_clusters=6, n_trials=50, seed=42)\n",
    "print(f\"Random cluster purity baseline: {random_baseline:.3f}\")\n",
    "\n",
    "for layer_name, layer_feats in layer_feats.items():\n",
    "    raw_feats = layer_feats\n",
    "    feats_np = raw_feats.cpu().numpy()\n",
    "    valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "    filtered_feats = feats_np[valid_mask]\n",
    "\n",
    "    # Use only labeled samples\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=0)\n",
    "    cluster_assignments = kmeans.fit_predict(filtered_feats)\n",
    "\n",
    "    purity = compute_purity(encoded_labels, cluster_assignments)\n",
    "    print(f\"{layer_name} Cluster Purity: {purity:.3f}\")\n",
    "\n",
    "# without texture category\n",
    "# Random cluster purity baseline: 0.272\n",
    "# layer1 Cluster Purity: 0.333\n",
    "# layer2 Cluster Purity: 0.357\n",
    "# layer3 Cluster Purity: 0.409\n",
    "# layer4 Cluster Purity: 0.456\n",
    "# fc Cluster Purity: 0.468\n",
    "\n",
    "# with texture category\n",
    "# Random cluster purity baseline: 0.241\n",
    "# layer1 Cluster Purity: 0.327\n",
    "# layer2 Cluster Purity: 0.295\n",
    "# layer3 Cluster Purity: 0.400\n",
    "# layer4 Cluster Purity: 0.438\n",
    "# fc Cluster Purity: 0.438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute NMI (Normalized Mutual Information)\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "def compute_random_nmi_baseline(y_true, n_clusters=None, n_trials=20, seed=None):\n",
    "    # Compute the expected NMI from random cluster assignments\n",
    "    y_true = np.array(y_true)\n",
    "    if n_clusters is None:\n",
    "        n_clusters = len(np.unique(y_true))\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    nmi_scores = []\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        random_labels = rng.integers(0, n_clusters, size=len(y_true))\n",
    "        nmi = normalized_mutual_info_score(y_true, random_labels)\n",
    "        nmi_scores.append(nmi)\n",
    "    \n",
    "    return np.mean(nmi_scores)\n",
    "\n",
    "layer_feats = {\n",
    "    'layer1': layer1_feats.view(layer1_feats.size(0), -1),\n",
    "    'layer2': layer2_feats.view(layer2_feats.size(0), -1),\n",
    "    'layer3': layer3_feats.view(layer3_feats.size(0), -1),\n",
    "    'layer4': layer4_feats.view(layer4_feats.size(0), -1),\n",
    "    'fc': final_layer_feats,\n",
    "}\n",
    "\n",
    "# Run KMeans clustering\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(filtered_labels)  # e.g., 'cat' → 0, 'bird' → 1, etc.\n",
    "n_clusters = len(np.unique(encoded_labels))\n",
    "\n",
    "random_nmi = compute_random_nmi_baseline(encoded_labels, n_clusters=6, n_trials=50, seed=42)\n",
    "print(f\"Random NMI baseline: {random_nmi:.3f}\")\n",
    "\n",
    "for layer_name, layer_feats in layer_feats.items():\n",
    "    raw_feats = layer_feats\n",
    "    feats_np = raw_feats.cpu().numpy()\n",
    "    valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "    filtered_feats = feats_np[valid_mask]\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=0)\n",
    "    cluster_assignments = kmeans.fit_predict(filtered_feats)\n",
    "\n",
    "    # Compute NMI\n",
    "    nmi = normalized_mutual_info_score(encoded_labels, cluster_assignments)\n",
    "    print(f\"NMI {layer_name}: {nmi:.3f}\")\n",
    "\n",
    "# without texture category\n",
    "# Random NMI baseline: 0.031\n",
    "# NMI: 0.104\n",
    "# NMI: 0.100\n",
    "# NMI: 0.193\n",
    "# NMI: 0.260\n",
    "# NMI: 0.281\n",
    "\n",
    "# with texture category\n",
    "# Random NMI baseline: 0.030\n",
    "# NMI layer1: 0.103\n",
    "# NMI layer2: 0.089\n",
    "# NMI layer3: 0.176\n",
    "# NMI layer4: 0.233\n",
    "# NMI fc: 0.244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb051a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UMAP\n",
    "\n",
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def compute_cluster_purity(pred_labels, true_labels):\n",
    "    cluster_to_labels = {}\n",
    "    for pred, true in zip(pred_labels, true_labels):\n",
    "        cluster_to_labels.setdefault(pred, []).append(true)\n",
    "\n",
    "    correct = 0\n",
    "    for members in cluster_to_labels.values():\n",
    "        most_common = Counter(members).most_common(1)[0][1]\n",
    "        correct += most_common\n",
    "\n",
    "    return correct / len(true_labels)\n",
    "\n",
    "layer_feats = {\n",
    "    'layer1': layer1_feats.view(layer1_feats.size(0), -1),\n",
    "    'layer2': layer2_feats.view(layer2_feats.size(0), -1),\n",
    "    'layer3': layer3_feats.view(layer3_feats.size(0), -1),\n",
    "    'layer4': layer4_feats.view(layer4_feats.size(0), -1),\n",
    "    'fc': final_layer_feats  # already flat\n",
    "}\n",
    "\n",
    "# Filter out undetermined labels and get mask\n",
    "valid_mask = np.array([label != 'undetermined' for label in category_labels])\n",
    "filtered_labels = [label for label in category_labels if label != 'undetermined']\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(filtered_labels)\n",
    "n_clusters = len(np.unique(encoded_labels))\n",
    "\n",
    "# UMAP + plot for each layer\n",
    "for layer_name, feats_tensor in layer_feats.items():\n",
    "    feats_np = feats_tensor.cpu().numpy()\n",
    "    filtered_feats = feats_np[valid_mask]  # same filtering for all layers\n",
    "\n",
    "    reducer = UMAP(n_components=2, random_state=42)\n",
    "    umap_feats = reducer.fit_transform(filtered_feats)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=umap_feats[:, 0], y=umap_feats[:, 1], hue=filtered_labels, palette='tab10', s=30)\n",
    "    plt.title(f\"UMAP on SimCLR {layer_name} features (filtered)\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Cluster metrics\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    pred_labels = kmeans.fit_predict(umap_feats)\n",
    "\n",
    "    purity = compute_cluster_purity(pred_labels, encoded_labels)\n",
    "    nmi = normalized_mutual_info_score(encoded_labels, pred_labels)\n",
    "    sil = silhouette_score(umap_feats, encoded_labels)\n",
    "\n",
    "    print(f\"Layer {layer_name}:\")\n",
    "    print(f\"  ➤ Cluster Purity:     {purity:.3f}\")\n",
    "    print(f\"  ➤ Normalized MI:      {nmi:.3f}\")\n",
    "    print(f\"  ➤ Silhouette Score:   {sil:.3f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nearest neighbors - layer 2\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stim_ids_ambiguous_imgs = [1719, 1470, 1375, 1237, 1153, 928]\n",
    "for stim_id in stim_ids_ambiguous_imgs:\n",
    "    # Find the index of the image with stim_id\n",
    "    query_idx = (labels == stim_id).nonzero(as_tuple=True)[0].item()\n",
    "    print(f\"Stim ID {stim_id} found at index {idx}\")\n",
    "\n",
    "    layer2_feats = layer2_feats.view(layer2_feats.size(0), -1)\n",
    "    layer2_feats_np = layer2_feats.cpu().numpy()\n",
    "\n",
    "    dists = cosine_distances([layer2_feats_np[query_idx]], layer2_feats_np)[0]\n",
    "    nearest_idxs = np.argsort(dists)[1:6]  # skip self\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i, idx in enumerate([query_idx] + list(nearest_idxs)):\n",
    "        img_np = images[idx].permute(1, 2, 0).numpy()\n",
    "        img_np = (img_np + 1) / 2\n",
    "        plt.subplot(1, 6, i + 1)\n",
    "        plt.imshow(np.clip(img_np, 0, 1))\n",
    "        plt.title(\"Query Layer2\" if i == 0 else f\"NN {i}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fc_features = final_layer_feats\n",
    "    fc_feats_np = final_layer_feats.cpu().numpy()\n",
    "\n",
    "    dists = cosine_distances([fc_feats_np[query_idx]], fc_feats_np)[0]\n",
    "    nearest_idxs = np.argsort(dists)[1:6]  # skip self\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i, idx in enumerate([query_idx] + list(nearest_idxs)):\n",
    "        img_np = images[idx].permute(1, 2, 0).numpy()\n",
    "        img_np = (img_np + 1) / 2\n",
    "        plt.subplot(1, 6, i + 1)\n",
    "        plt.imshow(np.clip(img_np, 0, 1))\n",
    "        plt.title(\"Query LayerFC\" if i == 0 else f\"NN {i}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
